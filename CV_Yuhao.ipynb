{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0UA6WFgcYYMI"},"outputs":[],"source":["# import packages here\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","import random \n","import time\n","import gc\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7T72O-1ks-a","executionInfo":{"status":"ok","timestamp":1667803035815,"user_tz":300,"elapsed":17786,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"1770617b-1f7b-4c5f-8790-7540ab9aa3d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Mount google drive \n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xIT3WIuykrg0","executionInfo":{"status":"ok","timestamp":1667803160231,"user_tz":300,"elapsed":737,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"9779e3b0-14cb-4157-9dda-729abc476d97"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/CV_Project\n"]}],"source":["# Set working directory\n","%cd /content/gdrive/My Drive/CV_Project"]},{"cell_type":"code","source":["!pip install git+https://github.com/ncullen93/torchsample"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56Xl8fmxjcfC","executionInfo":{"status":"ok","timestamp":1650857720213,"user_tz":240,"elapsed":7676,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"ed233d22-13d8-4d79-c4d2-426bdd41227a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/ncullen93/torchsample\n","  Cloning https://github.com/ncullen93/torchsample to /tmp/pip-req-build-rcdigybj\n","  Running command git clone -q https://github.com/ncullen93/torchsample /tmp/pip-req-build-rcdigybj\n","Building wheels for collected packages: torchsample\n","  Building wheel for torchsample (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchsample: filename=torchsample-0.1.3-py3-none-any.whl size=43432 sha256=4c2940fc67ed3ac5ba7de9fbb79450e63871a0e0e4eafef8a98c3cdd6bc77337\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-kwsezxew/wheels/88/d1/14/b6fc8c104020db989a0941e99c350d5db17bdd9366245c8c2e\n","Successfully built torchsample\n","Installing collected packages: torchsample\n","  Attempting uninstall: torchsample\n","    Found existing installation: torchsample 0.1.3\n","    Uninstalling torchsample-0.1.3:\n","      Successfully uninstalled torchsample-0.1.3\n","Successfully installed torchsample-0.1.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"HnkJ-LmJyBod"},"source":["## Loading and Preprocessing data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NxnBcUqk_9B","outputId":"3153df87-1230-42ec-ff66-421df4512757","executionInfo":{"status":"ok","timestamp":1650857869693,"user_tz":240,"elapsed":280,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["class_names: {0: 'Mountain', 1: 'TallBuilding', 2: 'OpenCountry', 3: 'LivingRoom', 4: 'Industrial', 5: 'Street', 6: 'Kitchen', 7: 'Store', 8: 'InsideCity', 9: 'Suburb', 10: 'Bedroom', 11: 'Flower', 12: 'Coast', 13: 'Office', 14: 'Highway', 15: 'Forest'} \n"]}],"source":["#--------------------------------------------------\n","#    Load Training Data and Testing Data\n","#--------------------------------------------------\n","\n","def set_random_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","set_random_seed(0)\n","\n","class_names = [name[13:] for name in glob.glob('./data/train/*')]\n","class_names = dict(zip(range(len(class_names)), class_names))\n","print(\"class_names: %s \" % class_names)\n","\n","def load_dataset(path, img_size, num_per_class=-1, batch_size=16, shuffle=False,\n","           augment=False, is_color=False, zero_centered=False): \n","    set_random_seed(0)   \n","    data = []\n","    labels = []    \n","    channel_num = 3 if is_color else 1\n","        \n","    # read images and resizing\n","    for id, class_name in class_names.items():\n","        print(\"Loading images from class: %s\" % id)\n","        img_path_class = glob.glob(path + class_name + '/*.jpg')\n","        if num_per_class > 0:\n","            img_path_class = img_path_class[:num_per_class]\n","        labels.extend([id]*len(img_path_class))\n","        for filename in img_path_class:\n","            if is_color:\n","                img = cv2.imread(filename)\n","            else:\n","                img = cv2.imread(filename, 0)\n","            \n","            # resize the image\n","            img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n","            \n","            if is_color:\n","                img = np.transpose(img, [2, 0, 1])\n","\n","            # norm pixel values to [-1, 1]\n","            data.append(img.astype(np.float)/255*2-1)\n","            \n","    \n","    # Data Augmentation \n","    if augment: \n","        pass\n","        data += [np.flip(img,1) for img in data]\n","        labels += labels\n"," \n","\n","    # Data Normalization\n","    if zero_centered:\n","        pass\n","        data -= np.mean(data, 0)\n","\n","         \n","    # randomly permute (this step is important for training)\n","    if shuffle:\n","        bundle = list(zip(data, labels))\n","        random.shuffle(bundle)\n","        data, labels = zip(*bundle)\n","    \n","    # divide data into minibatches of TorchTensors\n","    if batch_size > 1:\n","        batch_data = []\n","        batch_labels = []\n","         \n","        for i in range(int(len(data) / batch_size)):\n","            minibatch_d = data[i*batch_size: (i+1)*batch_size]\n","            minibatch_d = np.reshape(minibatch_d, (batch_size, channel_num, img_size[0], img_size[1]))\n","            batch_data.append(torch.from_numpy(minibatch_d))\n","\n","            minibatch_l = labels[i*batch_size: (i+1)*batch_size]\n","            batch_labels.append(torch.LongTensor(minibatch_l))\n","        data, labels = batch_data, batch_labels \n","    \n","    return zip(batch_data, batch_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHI2VYbDloje","executionInfo":{"status":"ok","timestamp":1650857907899,"user_tz":240,"elapsed":15318,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"bea10f0c-811b-4a69-ad95-bf8a466c582a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading images from class: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 75 minibatches (batch_size=32) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 12 minibatches (batch_size=32) of testing samples.\n"]}],"source":["# load data into size (64, 64)\n","img_size = (64, 64)\n","batch_size = 32 # training sample number per batch\n","\n","# load training dataset\n","trainloader_small = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True))\n","train_num = len(trainloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n","\n","# load testing dataset\n","testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size))\n","test_num = len(testloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"krCXjDOzlq0f","outputId":"4e1baf07-de65-4f39-92f2-54b6baa5bb5f","executionInfo":{"status":"ok","timestamp":1650852054633,"user_tz":240,"elapsed":1336,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Forest\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# show some images\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    if len(npimg.shape) > 2:\n","        npimg = np.transpose(img, [1, 2, 0])\n","    plt.figure\n","    plt.imshow(npimg, 'gray')\n","    plt.show()\n","img, label = trainloader_small[0][0][11][0], trainloader_small[0][1][11]\n","label = int(np.array(label))\n","print(class_names[label])\n","imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"JGA-K6QzYYMR"},"source":["Training a Network From Scratch\n"]},{"cell_type":"markdown","metadata":{"id":"s6OPghPL1ZGt"},"source":["First, let's define a simpe network architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI9qG3x-zbe-"},"outputs":[],"source":["#--------------------------------------------------\n","#       Define Network Architecture\n","#--------------------------------------------------\n","class TNet(nn.Module):\n","    def __init__(self):\n","      super(TNet,self).__init__()  \n","           \n","      self.features = torch.nn.Sequential(\n","        nn.Conv2d(1, 16, 3),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4), \n","      )\n","      \n","      self.classifier = nn.Sequential(\n","         nn.Linear(3600, 16), \n","      )\n","      \n","    def forward(self, x):\n","      x = self.features(x)  \n","      x = torch.flatten(x, 1)\n","      x = self.classifier(x)\n","      return x  \n","  "]},{"cell_type":"markdown","metadata":{"id":"-PBerwk-1leX"},"source":["Then, let's define model training and evaluation functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNJTDG8xoJwH"},"outputs":[],"source":["#--------------------------------------------------\n","#       Model Training Function\n","#--------------------------------------------------\n","import torch.optim as optim\n","import time\n","  \n","def trainModel(net, trainloader, train_option, testloader=None):  \n","  loss_func = nn.CrossEntropyLoss()\n","  lr = train_option['lr']\n","  epoch = train_option['epoch']\n","  device = train_option['device'] if 'device' in train_option.keys() else 'cpu'\n","  log_iter = train_option['log_iter'] if 'log_iter' in train_option.keys() else 20\n","  eval_epoch = 1\n","  \n","  if 'optimizer' in train_option.keys():\n","    optimizer = train_option['optimizer']\n","  else:\n","    optimizer = optim.Adam(net.parameters(), lr=lr)\n","\n","  start_time = time.time()\n","  if device == 'gpu':\n","    net = net.cuda()\n","    \n","  iters = 0\n","  running_loss = 0.0\n","  for ep in range(epoch):\n","    net.train()        \n","    for iter, (x, y) in enumerate(trainloader):\n","      iters += 1\n","      batch_x = Variable(x).float()\n","      batch_y = Variable(y).long()\n","      if device == 'gpu':\n","        batch_x = batch_x.cuda()\n","        batch_y = batch_y.cuda()\n","\n","      outputs = net(batch_x)\n","      loss = loss_func(outputs, batch_y)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      running_loss += loss.item()\n","      \n","      time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n","      if iter % log_iter == 0:\n","        print('Epoch:{:2d} | Iter:{:5d} | Time: {} | Train Loss: {:.4f} | Average Loss: {:.4f} '.format(ep+1, iter, time_lapse, loss.item(), running_loss/iters))\n","   \n","    if testloader is not None and ep % eval_epoch == 0:\n","      evalModel(net, testloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5sadf8qoOW6"},"outputs":[],"source":["#--------------------------------------------------\n","#       Model Evaluating Function\n","#--------------------------------------------------\n","import time\n","  \n","def evalModel(net, testloader): \n","  acc = 0.0\n","  count = 0\n","  start_time = time.time()\n","  device = 'gpu' if next(net.parameters()).is_cuda else 'cpu'\n","  net.eval()\n","  \n","  for iter, (x, y) in enumerate(testloader):\n","        count += x.shape[0]\n","        batch_x = Variable(x).float()\n","        batch_y = Variable(y).long()\n","        if device == 'gpu':\n","          batch_x = batch_x.cuda()\n","          batch_y = batch_y.cuda()\n","        outputs = net(batch_x)\n","        acc += torch.sum(outputs.max(1)[1]==batch_y)\n","        \n","  time_lapse = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))        \n","  print('Accuracy: {:5f} | Time: {}'.format(acc/count,time_lapse))\n","  "]},{"cell_type":"markdown","metadata":{"id":"Mrm2SErt7wzZ"},"source":["Finally, let's start training and evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KDwrBoHVukOK","outputId":"7920b13e-d723-42dd-b3fc-9255edd6ecd4","executionInfo":{"status":"ok","timestamp":1650852084295,"user_tz":240,"elapsed":13988,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:09 | Train Loss: 2.7568 | Average Loss: 2.7568 \n","Epoch: 1 | Iter:   20 | Time: 00:00:09 | Train Loss: 2.2201 | Average Loss: 2.6270 \n","Accuracy: 0.369792 | Time: 00:00:00\n","Epoch: 2 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.7504 | Average Loss: 2.4415 \n","Epoch: 2 | Iter:   20 | Time: 00:00:09 | Train Loss: 1.6060 | Average Loss: 2.2417 \n","Accuracy: 0.424479 | Time: 00:00:00\n","Epoch: 3 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.3842 | Average Loss: 2.1154 \n","Epoch: 3 | Iter:   20 | Time: 00:00:09 | Train Loss: 1.2620 | Average Loss: 1.9864 \n","Accuracy: 0.450521 | Time: 00:00:00\n","Epoch: 4 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.0796 | Average Loss: 1.8854 \n","Epoch: 4 | Iter:   20 | Time: 00:00:10 | Train Loss: 1.0240 | Average Loss: 1.7849 \n","Accuracy: 0.468750 | Time: 00:00:00\n","Epoch: 5 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.8425 | Average Loss: 1.7001 \n","Epoch: 5 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.8346 | Average Loss: 1.6161 \n","Accuracy: 0.479167 | Time: 00:00:00\n","Epoch: 6 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.6739 | Average Loss: 1.5439 \n","Epoch: 6 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.6911 | Average Loss: 1.4721 \n","Accuracy: 0.476562 | Time: 00:00:00\n","Epoch: 7 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.5549 | Average Loss: 1.4103 \n","Epoch: 7 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.5856 | Average Loss: 1.3482 \n","Accuracy: 0.476562 | Time: 00:00:00\n","Epoch: 8 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.4594 | Average Loss: 1.2951 \n","Epoch: 8 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.4978 | Average Loss: 1.2410 \n","Accuracy: 0.484375 | Time: 00:00:00\n","Epoch: 9 | Iter:    0 | Time: 00:00:11 | Train Loss: 0.3721 | Average Loss: 1.1950 \n","Epoch: 9 | Iter:   20 | Time: 00:00:11 | Train Loss: 0.4228 | Average Loss: 1.1478 \n","Accuracy: 0.458333 | Time: 00:00:00\n","Epoch:10 | Iter:    0 | Time: 00:00:11 | Train Loss: 0.2969 | Average Loss: 1.1079 \n","Epoch:10 | Iter:   20 | Time: 00:00:11 | Train Loss: 0.3691 | Average Loss: 1.0664 \n","Accuracy: 0.453125 | Time: 00:00:00\n","Epoch:11 | Iter:    0 | Time: 00:00:11 | Train Loss: 0.2455 | Average Loss: 1.0321 \n","Epoch:11 | Iter:   20 | Time: 00:00:11 | Train Loss: 0.3111 | Average Loss: 0.9955 \n","Accuracy: 0.463542 | Time: 00:00:00\n","Epoch:12 | Iter:    0 | Time: 00:00:11 | Train Loss: 0.2072 | Average Loss: 0.9660 \n","Epoch:12 | Iter:   20 | Time: 00:00:11 | Train Loss: 0.2534 | Average Loss: 0.9350 \n","Accuracy: 0.466146 | Time: 00:00:00\n","Epoch:13 | Iter:    0 | Time: 00:00:11 | Train Loss: 0.3332 | Average Loss: 0.9093 \n","Epoch:13 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.2753 | Average Loss: 0.8828 \n","Accuracy: 0.484375 | Time: 00:00:00\n","Epoch:14 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.1919 | Average Loss: 0.8602 \n","Epoch:14 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.2763 | Average Loss: 0.8373 \n","Accuracy: 0.463542 | Time: 00:00:00\n","Epoch:15 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.1869 | Average Loss: 0.8170 \n","Epoch:15 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.2393 | Average Loss: 0.7959 \n","Accuracy: 0.468750 | Time: 00:00:00\n","Epoch:16 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.2647 | Average Loss: 0.7781 \n","Epoch:16 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.3028 | Average Loss: 0.7601 \n","Accuracy: 0.458333 | Time: 00:00:00\n","Epoch:17 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.2468 | Average Loss: 0.7454 \n","Epoch:17 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.3337 | Average Loss: 0.7301 \n","Accuracy: 0.440104 | Time: 00:00:00\n","Epoch:18 | Iter:    0 | Time: 00:00:13 | Train Loss: 0.2553 | Average Loss: 0.7184 \n","Epoch:18 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.3249 | Average Loss: 0.7062 \n","Accuracy: 0.434896 | Time: 00:00:00\n","Epoch:19 | Iter:    0 | Time: 00:00:13 | Train Loss: 0.4323 | Average Loss: 0.6977 \n","Epoch:19 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.1594 | Average Loss: 0.6860 \n","Accuracy: 0.468750 | Time: 00:00:00\n","Epoch:20 | Iter:    0 | Time: 00:00:13 | Train Loss: 0.2222 | Average Loss: 0.6752 \n","Epoch:20 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.1537 | Average Loss: 0.6623 \n","Accuracy: 0.468750 | Time: 00:00:00\n"]}],"source":["#--------------------------------------------------\n","#       Start Training & Evaluation\n","#--------------------------------------------------\n","net = TNet() \n","train_option = {}\n","train_option['lr'] = 0.002\n","train_option['epoch'] = 20\n","train_option['device'] = 'gpu'\n","trainModel(net, trainloader_small, train_option, testloader_small)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3S9dgoEWFCcI","executionInfo":{"status":"ok","timestamp":1650857257933,"user_tz":240,"elapsed":14170,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"e8986410-8b06-4c2f-9de6-4c21ef9c0f0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading images from class: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 150 minibatches (batch_size=32) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 12 minibatches (batch_size=32) of testing samples.\n"]}],"source":["# load data into size (64, 64)\n","img_size = (64, 64)\n","batch_size = 32 # training sample number per batch\n","\n","# load training dataset\n","trainloader_small = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True, augment=True, zero_centered=True))\n","train_num = len(trainloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n","\n","# load testing dataset\n","testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size, zero_centered=True))\n","test_num = len(testloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clXaGdwaoIL7"},"outputs":[],"source":["#--------------------------------------------------\n","#       Define Network Architecture\n","#--------------------------------------------------\n","class Improved_TNet(nn.Module):\n","  def __init__(self):\n","    super(Improved_TNet,self).__init__()\n","    self.features = torch.nn.Sequential(\n","        nn.Conv2d(1, 32, 3),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4),\n","        nn.Conv2d(32, 256, 3),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4),\n","        nn.Dropout(0.5)\n","    )\n","    self.classifier = nn.Sequential(\n","        nn.Linear(2304, 16),\n","    )\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = torch.flatten(x, 1)\n","    x = self.classifier(x)\n","    return x\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Tj73gVp8gN7","executionInfo":{"status":"ok","timestamp":1650852608500,"user_tz":240,"elapsed":111444,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"79daf291-e72b-472f-fe1f-7dd1657c5577"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.8126 | Average Loss: 2.8126 \n","Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.5247 | Average Loss: 2.7008 \n","Epoch: 1 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.3219 | Average Loss: 2.5594 \n","Epoch: 1 | Iter:   60 | Time: 00:00:01 | Train Loss: 2.0123 | Average Loss: 2.4304 \n","Accuracy: 0.380208 | Time: 00:00:00\n","Epoch: 2 | Iter:    0 | Time: 00:00:01 | Train Loss: 2.1373 | Average Loss: 2.3638 \n","Epoch: 2 | Iter:   20 | Time: 00:00:01 | Train Loss: 1.8962 | Average Loss: 2.2686 \n","Epoch: 2 | Iter:   40 | Time: 00:00:01 | Train Loss: 1.8060 | Average Loss: 2.1717 \n","Epoch: 2 | Iter:   60 | Time: 00:00:02 | Train Loss: 1.5232 | Average Loss: 2.1030 \n","Accuracy: 0.476562 | Time: 00:00:00\n","Epoch: 3 | Iter:    0 | Time: 00:00:02 | Train Loss: 1.6864 | Average Loss: 2.0582 \n","Epoch: 3 | Iter:   20 | Time: 00:00:02 | Train Loss: 1.5542 | Average Loss: 2.0010 \n","Epoch: 3 | Iter:   40 | Time: 00:00:02 | Train Loss: 1.5692 | Average Loss: 1.9439 \n","Epoch: 3 | Iter:   60 | Time: 00:00:03 | Train Loss: 1.2975 | Average Loss: 1.9019 \n","Accuracy: 0.536458 | Time: 00:00:00\n","Epoch: 4 | Iter:    0 | Time: 00:00:03 | Train Loss: 1.4994 | Average Loss: 1.8695 \n","Epoch: 4 | Iter:   20 | Time: 00:00:03 | Train Loss: 1.3726 | Average Loss: 1.8291 \n","Epoch: 4 | Iter:   40 | Time: 00:00:04 | Train Loss: 1.2313 | Average Loss: 1.7881 \n","Epoch: 4 | Iter:   60 | Time: 00:00:04 | Train Loss: 1.3505 | Average Loss: 1.7572 \n","Accuracy: 0.544271 | Time: 00:00:00\n","Epoch: 5 | Iter:    0 | Time: 00:00:04 | Train Loss: 1.3441 | Average Loss: 1.7339 \n","Epoch: 5 | Iter:   20 | Time: 00:00:04 | Train Loss: 1.1673 | Average Loss: 1.7033 \n","Epoch: 5 | Iter:   40 | Time: 00:00:05 | Train Loss: 1.1710 | Average Loss: 1.6732 \n","Epoch: 5 | Iter:   60 | Time: 00:00:05 | Train Loss: 1.2037 | Average Loss: 1.6477 \n","Accuracy: 0.580729 | Time: 00:00:00\n","Epoch: 6 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.2280 | Average Loss: 1.6298 \n","Epoch: 6 | Iter:   20 | Time: 00:00:06 | Train Loss: 1.1337 | Average Loss: 1.6049 \n","Epoch: 6 | Iter:   40 | Time: 00:00:06 | Train Loss: 1.1028 | Average Loss: 1.5802 \n","Epoch: 6 | Iter:   60 | Time: 00:00:06 | Train Loss: 1.1273 | Average Loss: 1.5599 \n","Accuracy: 0.609375 | Time: 00:00:00\n","Epoch: 7 | Iter:    0 | Time: 00:00:06 | Train Loss: 1.2309 | Average Loss: 1.5464 \n","Epoch: 7 | Iter:   20 | Time: 00:00:07 | Train Loss: 0.9342 | Average Loss: 1.5257 \n","Epoch: 7 | Iter:   40 | Time: 00:00:07 | Train Loss: 1.0050 | Average Loss: 1.5053 \n","Epoch: 7 | Iter:   60 | Time: 00:00:07 | Train Loss: 1.0012 | Average Loss: 1.4873 \n","Accuracy: 0.601562 | Time: 00:00:00\n","Epoch: 8 | Iter:    0 | Time: 00:00:07 | Train Loss: 1.2122 | Average Loss: 1.4752 \n","Epoch: 8 | Iter:   20 | Time: 00:00:08 | Train Loss: 0.9915 | Average Loss: 1.4585 \n","Epoch: 8 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.0985 | Average Loss: 1.4411 \n","Epoch: 8 | Iter:   60 | Time: 00:00:08 | Train Loss: 1.0260 | Average Loss: 1.4272 \n","Accuracy: 0.635417 | Time: 00:00:00\n","Epoch: 9 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.0501 | Average Loss: 1.4170 \n","Epoch: 9 | Iter:   20 | Time: 00:00:09 | Train Loss: 0.9729 | Average Loss: 1.4025 \n","Epoch: 9 | Iter:   40 | Time: 00:00:09 | Train Loss: 0.9910 | Average Loss: 1.3874 \n","Epoch: 9 | Iter:   60 | Time: 00:00:09 | Train Loss: 0.8638 | Average Loss: 1.3748 \n","Accuracy: 0.632812 | Time: 00:00:00\n","Epoch:10 | Iter:    0 | Time: 00:00:10 | Train Loss: 0.9148 | Average Loss: 1.3653 \n","Epoch:10 | Iter:   20 | Time: 00:00:10 | Train Loss: 0.7539 | Average Loss: 1.3523 \n","Epoch:10 | Iter:   40 | Time: 00:00:10 | Train Loss: 0.9305 | Average Loss: 1.3382 \n","Epoch:10 | Iter:   60 | Time: 00:00:11 | Train Loss: 0.9769 | Average Loss: 1.3267 \n","Accuracy: 0.632812 | Time: 00:00:00\n","Epoch:11 | Iter:    0 | Time: 00:00:11 | Train Loss: 1.0231 | Average Loss: 1.3186 \n","Epoch:11 | Iter:   20 | Time: 00:00:11 | Train Loss: 0.8922 | Average Loss: 1.3068 \n","Epoch:11 | Iter:   40 | Time: 00:00:11 | Train Loss: 0.8134 | Average Loss: 1.2950 \n","Epoch:11 | Iter:   60 | Time: 00:00:12 | Train Loss: 0.9227 | Average Loss: 1.2851 \n","Accuracy: 0.640625 | Time: 00:00:00\n","Epoch:12 | Iter:    0 | Time: 00:00:12 | Train Loss: 0.8709 | Average Loss: 1.2780 \n","Epoch:12 | Iter:   20 | Time: 00:00:12 | Train Loss: 0.9219 | Average Loss: 1.2680 \n","Epoch:12 | Iter:   40 | Time: 00:00:12 | Train Loss: 0.8485 | Average Loss: 1.2567 \n","Epoch:12 | Iter:   60 | Time: 00:00:13 | Train Loss: 0.8756 | Average Loss: 1.2477 \n","Accuracy: 0.638021 | Time: 00:00:00\n","Epoch:13 | Iter:    0 | Time: 00:00:13 | Train Loss: 1.0925 | Average Loss: 1.2408 \n","Epoch:13 | Iter:   20 | Time: 00:00:13 | Train Loss: 0.7106 | Average Loss: 1.2315 \n","Epoch:13 | Iter:   40 | Time: 00:00:14 | Train Loss: 0.7864 | Average Loss: 1.2212 \n","Epoch:13 | Iter:   60 | Time: 00:00:14 | Train Loss: 0.8230 | Average Loss: 1.2131 \n","Accuracy: 0.661458 | Time: 00:00:00\n","Epoch:14 | Iter:    0 | Time: 00:00:14 | Train Loss: 0.8541 | Average Loss: 1.2069 \n","Epoch:14 | Iter:   20 | Time: 00:00:14 | Train Loss: 0.7558 | Average Loss: 1.1979 \n","Epoch:14 | Iter:   40 | Time: 00:00:15 | Train Loss: 0.8375 | Average Loss: 1.1885 \n","Epoch:14 | Iter:   60 | Time: 00:00:15 | Train Loss: 0.6712 | Average Loss: 1.1802 \n","Accuracy: 0.661458 | Time: 00:00:00\n","Epoch:15 | Iter:    0 | Time: 00:00:15 | Train Loss: 0.9439 | Average Loss: 1.1743 \n","Epoch:15 | Iter:   20 | Time: 00:00:15 | Train Loss: 0.6908 | Average Loss: 1.1665 \n","Epoch:15 | Iter:   40 | Time: 00:00:16 | Train Loss: 0.6768 | Average Loss: 1.1579 \n","Epoch:15 | Iter:   60 | Time: 00:00:16 | Train Loss: 0.8062 | Average Loss: 1.1509 \n","Accuracy: 0.643229 | Time: 00:00:00\n","Epoch:16 | Iter:    0 | Time: 00:00:16 | Train Loss: 0.9251 | Average Loss: 1.1455 \n","Epoch:16 | Iter:   20 | Time: 00:00:17 | Train Loss: 0.6885 | Average Loss: 1.1378 \n","Epoch:16 | Iter:   40 | Time: 00:00:17 | Train Loss: 0.8157 | Average Loss: 1.1303 \n","Epoch:16 | Iter:   60 | Time: 00:00:17 | Train Loss: 0.6795 | Average Loss: 1.1234 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:17 | Iter:    0 | Time: 00:00:17 | Train Loss: 0.8665 | Average Loss: 1.1184 \n","Epoch:17 | Iter:   20 | Time: 00:00:18 | Train Loss: 0.7361 | Average Loss: 1.1109 \n","Epoch:17 | Iter:   40 | Time: 00:00:18 | Train Loss: 0.7213 | Average Loss: 1.1036 \n","Epoch:17 | Iter:   60 | Time: 00:00:18 | Train Loss: 0.6806 | Average Loss: 1.0971 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:18 | Iter:    0 | Time: 00:00:19 | Train Loss: 0.9210 | Average Loss: 1.0926 \n","Epoch:18 | Iter:   20 | Time: 00:00:19 | Train Loss: 0.7729 | Average Loss: 1.0860 \n","Epoch:18 | Iter:   40 | Time: 00:00:19 | Train Loss: 0.6520 | Average Loss: 1.0794 \n","Epoch:18 | Iter:   60 | Time: 00:00:19 | Train Loss: 0.6407 | Average Loss: 1.0731 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:19 | Iter:    0 | Time: 00:00:20 | Train Loss: 0.6412 | Average Loss: 1.0683 \n","Epoch:19 | Iter:   20 | Time: 00:00:20 | Train Loss: 0.5804 | Average Loss: 1.0622 \n","Epoch:19 | Iter:   40 | Time: 00:00:20 | Train Loss: 0.6451 | Average Loss: 1.0562 \n","Epoch:19 | Iter:   60 | Time: 00:00:21 | Train Loss: 0.5390 | Average Loss: 1.0507 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:20 | Iter:    0 | Time: 00:00:21 | Train Loss: 0.7681 | Average Loss: 1.0465 \n","Epoch:20 | Iter:   20 | Time: 00:00:21 | Train Loss: 0.6754 | Average Loss: 1.0407 \n","Epoch:20 | Iter:   40 | Time: 00:00:21 | Train Loss: 0.5108 | Average Loss: 1.0350 \n","Epoch:20 | Iter:   60 | Time: 00:00:22 | Train Loss: 0.5896 | Average Loss: 1.0297 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:21 | Iter:    0 | Time: 00:00:22 | Train Loss: 0.6449 | Average Loss: 1.0258 \n","Epoch:21 | Iter:   20 | Time: 00:00:22 | Train Loss: 0.4890 | Average Loss: 1.0202 \n","Epoch:21 | Iter:   40 | Time: 00:00:22 | Train Loss: 0.5018 | Average Loss: 1.0145 \n","Epoch:21 | Iter:   60 | Time: 00:00:23 | Train Loss: 0.5477 | Average Loss: 1.0091 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:22 | Iter:    0 | Time: 00:00:23 | Train Loss: 0.8677 | Average Loss: 1.0061 \n","Epoch:22 | Iter:   20 | Time: 00:00:23 | Train Loss: 0.6666 | Average Loss: 1.0009 \n","Epoch:22 | Iter:   40 | Time: 00:00:24 | Train Loss: 0.5645 | Average Loss: 0.9956 \n","Epoch:22 | Iter:   60 | Time: 00:00:24 | Train Loss: 0.5286 | Average Loss: 0.9910 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:23 | Iter:    0 | Time: 00:00:24 | Train Loss: 0.7568 | Average Loss: 0.9878 \n","Epoch:23 | Iter:   20 | Time: 00:00:24 | Train Loss: 0.6623 | Average Loss: 0.9829 \n","Epoch:23 | Iter:   40 | Time: 00:00:25 | Train Loss: 0.6196 | Average Loss: 0.9778 \n","Epoch:23 | Iter:   60 | Time: 00:00:25 | Train Loss: 0.5706 | Average Loss: 0.9730 \n","Accuracy: 0.651042 | Time: 00:00:00\n","Epoch:24 | Iter:    0 | Time: 00:00:25 | Train Loss: 0.5919 | Average Loss: 0.9695 \n","Epoch:24 | Iter:   20 | Time: 00:00:25 | Train Loss: 0.6139 | Average Loss: 0.9649 \n","Epoch:24 | Iter:   40 | Time: 00:00:26 | Train Loss: 0.5117 | Average Loss: 0.9602 \n","Epoch:24 | Iter:   60 | Time: 00:00:26 | Train Loss: 0.4787 | Average Loss: 0.9555 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:25 | Iter:    0 | Time: 00:00:26 | Train Loss: 0.5749 | Average Loss: 0.9522 \n","Epoch:25 | Iter:   20 | Time: 00:00:27 | Train Loss: 0.4757 | Average Loss: 0.9473 \n","Epoch:25 | Iter:   40 | Time: 00:00:27 | Train Loss: 0.6218 | Average Loss: 0.9427 \n","Epoch:25 | Iter:   60 | Time: 00:00:27 | Train Loss: 0.5718 | Average Loss: 0.9387 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:26 | Iter:    0 | Time: 00:00:27 | Train Loss: 0.6948 | Average Loss: 0.9359 \n","Epoch:26 | Iter:   20 | Time: 00:00:28 | Train Loss: 0.4728 | Average Loss: 0.9315 \n","Epoch:26 | Iter:   40 | Time: 00:00:28 | Train Loss: 0.4895 | Average Loss: 0.9269 \n","Epoch:26 | Iter:   60 | Time: 00:00:28 | Train Loss: 0.4013 | Average Loss: 0.9229 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:27 | Iter:    0 | Time: 00:00:29 | Train Loss: 0.7559 | Average Loss: 0.9200 \n","Epoch:27 | Iter:   20 | Time: 00:00:29 | Train Loss: 0.3859 | Average Loss: 0.9159 \n","Epoch:27 | Iter:   40 | Time: 00:00:29 | Train Loss: 0.7271 | Average Loss: 0.9118 \n","Epoch:27 | Iter:   60 | Time: 00:00:29 | Train Loss: 0.4310 | Average Loss: 0.9083 \n","Accuracy: 0.661458 | Time: 00:00:00\n","Epoch:28 | Iter:    0 | Time: 00:00:30 | Train Loss: 0.5908 | Average Loss: 0.9057 \n","Epoch:28 | Iter:   20 | Time: 00:00:30 | Train Loss: 0.4929 | Average Loss: 0.9020 \n","Epoch:28 | Iter:   40 | Time: 00:00:30 | Train Loss: 0.4550 | Average Loss: 0.8978 \n","Epoch:28 | Iter:   60 | Time: 00:00:30 | Train Loss: 0.4912 | Average Loss: 0.8941 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:29 | Iter:    0 | Time: 00:00:31 | Train Loss: 0.5010 | Average Loss: 0.8915 \n","Epoch:29 | Iter:   20 | Time: 00:00:31 | Train Loss: 0.4487 | Average Loss: 0.8877 \n","Epoch:29 | Iter:   40 | Time: 00:00:31 | Train Loss: 0.5149 | Average Loss: 0.8839 \n","Epoch:29 | Iter:   60 | Time: 00:00:32 | Train Loss: 0.5202 | Average Loss: 0.8803 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:30 | Iter:    0 | Time: 00:00:32 | Train Loss: 0.5595 | Average Loss: 0.8777 \n","Epoch:30 | Iter:   20 | Time: 00:00:32 | Train Loss: 0.5752 | Average Loss: 0.8738 \n","Epoch:30 | Iter:   40 | Time: 00:00:32 | Train Loss: 0.4294 | Average Loss: 0.8702 \n","Epoch:30 | Iter:   60 | Time: 00:00:33 | Train Loss: 0.3742 | Average Loss: 0.8668 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:31 | Iter:    0 | Time: 00:00:33 | Train Loss: 0.5215 | Average Loss: 0.8647 \n","Epoch:31 | Iter:   20 | Time: 00:00:33 | Train Loss: 0.5212 | Average Loss: 0.8614 \n","Epoch:31 | Iter:   40 | Time: 00:00:34 | Train Loss: 0.5072 | Average Loss: 0.8577 \n","Epoch:31 | Iter:   60 | Time: 00:00:34 | Train Loss: 0.3924 | Average Loss: 0.8544 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:32 | Iter:    0 | Time: 00:00:34 | Train Loss: 0.4024 | Average Loss: 0.8522 \n","Epoch:32 | Iter:   20 | Time: 00:00:34 | Train Loss: 0.4429 | Average Loss: 0.8490 \n","Epoch:32 | Iter:   40 | Time: 00:00:35 | Train Loss: 0.4391 | Average Loss: 0.8454 \n","Epoch:32 | Iter:   60 | Time: 00:00:35 | Train Loss: 0.4258 | Average Loss: 0.8422 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:33 | Iter:    0 | Time: 00:00:35 | Train Loss: 0.3726 | Average Loss: 0.8398 \n","Epoch:33 | Iter:   20 | Time: 00:00:35 | Train Loss: 0.4876 | Average Loss: 0.8365 \n","Epoch:33 | Iter:   40 | Time: 00:00:36 | Train Loss: 0.7032 | Average Loss: 0.8331 \n","Epoch:33 | Iter:   60 | Time: 00:00:36 | Train Loss: 0.3173 | Average Loss: 0.8299 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:34 | Iter:    0 | Time: 00:00:36 | Train Loss: 0.4165 | Average Loss: 0.8278 \n","Epoch:34 | Iter:   20 | Time: 00:00:37 | Train Loss: 0.3918 | Average Loss: 0.8245 \n","Epoch:34 | Iter:   40 | Time: 00:00:37 | Train Loss: 0.5346 | Average Loss: 0.8214 \n","Epoch:34 | Iter:   60 | Time: 00:00:37 | Train Loss: 0.5267 | Average Loss: 0.8184 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:35 | Iter:    0 | Time: 00:00:37 | Train Loss: 0.7696 | Average Loss: 0.8163 \n","Epoch:35 | Iter:   20 | Time: 00:00:38 | Train Loss: 0.5499 | Average Loss: 0.8133 \n","Epoch:35 | Iter:   40 | Time: 00:00:38 | Train Loss: 0.4193 | Average Loss: 0.8101 \n","Epoch:35 | Iter:   60 | Time: 00:00:38 | Train Loss: 0.4930 | Average Loss: 0.8072 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:36 | Iter:    0 | Time: 00:00:39 | Train Loss: 0.3859 | Average Loss: 0.8050 \n","Epoch:36 | Iter:   20 | Time: 00:00:39 | Train Loss: 0.4115 | Average Loss: 0.8019 \n","Epoch:36 | Iter:   40 | Time: 00:00:39 | Train Loss: 0.4636 | Average Loss: 0.7989 \n","Epoch:36 | Iter:   60 | Time: 00:00:39 | Train Loss: 0.2895 | Average Loss: 0.7960 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:37 | Iter:    0 | Time: 00:00:40 | Train Loss: 0.5594 | Average Loss: 0.7941 \n","Epoch:37 | Iter:   20 | Time: 00:00:40 | Train Loss: 0.4848 | Average Loss: 0.7912 \n","Epoch:37 | Iter:   40 | Time: 00:00:40 | Train Loss: 0.6140 | Average Loss: 0.7884 \n","Epoch:37 | Iter:   60 | Time: 00:00:41 | Train Loss: 0.3917 | Average Loss: 0.7856 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:38 | Iter:    0 | Time: 00:00:41 | Train Loss: 0.4344 | Average Loss: 0.7836 \n","Epoch:38 | Iter:   20 | Time: 00:00:41 | Train Loss: 0.5211 | Average Loss: 0.7811 \n","Epoch:38 | Iter:   40 | Time: 00:00:41 | Train Loss: 0.4666 | Average Loss: 0.7784 \n","Epoch:38 | Iter:   60 | Time: 00:00:42 | Train Loss: 0.3381 | Average Loss: 0.7758 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:39 | Iter:    0 | Time: 00:00:42 | Train Loss: 0.5750 | Average Loss: 0.7737 \n","Epoch:39 | Iter:   20 | Time: 00:00:42 | Train Loss: 0.2981 | Average Loss: 0.7710 \n","Epoch:39 | Iter:   40 | Time: 00:00:42 | Train Loss: 0.3722 | Average Loss: 0.7685 \n","Epoch:39 | Iter:   60 | Time: 00:00:43 | Train Loss: 0.3679 | Average Loss: 0.7659 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:40 | Iter:    0 | Time: 00:00:43 | Train Loss: 0.6982 | Average Loss: 0.7642 \n","Epoch:40 | Iter:   20 | Time: 00:00:43 | Train Loss: 0.4000 | Average Loss: 0.7617 \n","Epoch:40 | Iter:   40 | Time: 00:00:44 | Train Loss: 0.5024 | Average Loss: 0.7589 \n","Epoch:40 | Iter:   60 | Time: 00:00:44 | Train Loss: 0.3737 | Average Loss: 0.7566 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:41 | Iter:    0 | Time: 00:00:44 | Train Loss: 0.4407 | Average Loss: 0.7548 \n","Epoch:41 | Iter:   20 | Time: 00:00:44 | Train Loss: 0.4098 | Average Loss: 0.7521 \n","Epoch:41 | Iter:   40 | Time: 00:00:45 | Train Loss: 0.5193 | Average Loss: 0.7495 \n","Epoch:41 | Iter:   60 | Time: 00:00:45 | Train Loss: 0.4451 | Average Loss: 0.7474 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:42 | Iter:    0 | Time: 00:00:45 | Train Loss: 0.3950 | Average Loss: 0.7458 \n","Epoch:42 | Iter:   20 | Time: 00:00:45 | Train Loss: 0.3584 | Average Loss: 0.7436 \n","Epoch:42 | Iter:   40 | Time: 00:00:46 | Train Loss: 0.3016 | Average Loss: 0.7410 \n","Epoch:42 | Iter:   60 | Time: 00:00:46 | Train Loss: 0.3938 | Average Loss: 0.7387 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:43 | Iter:    0 | Time: 00:00:46 | Train Loss: 0.6093 | Average Loss: 0.7371 \n","Epoch:43 | Iter:   20 | Time: 00:00:47 | Train Loss: 0.3534 | Average Loss: 0.7348 \n","Epoch:43 | Iter:   40 | Time: 00:00:47 | Train Loss: 0.3412 | Average Loss: 0.7324 \n","Epoch:43 | Iter:   60 | Time: 00:00:47 | Train Loss: 0.2175 | Average Loss: 0.7301 \n","Accuracy: 0.645833 | Time: 00:00:00\n","Epoch:44 | Iter:    0 | Time: 00:00:47 | Train Loss: 0.4449 | Average Loss: 0.7285 \n","Epoch:44 | Iter:   20 | Time: 00:00:48 | Train Loss: 0.2622 | Average Loss: 0.7264 \n","Epoch:44 | Iter:   40 | Time: 00:00:48 | Train Loss: 0.3408 | Average Loss: 0.7243 \n","Epoch:44 | Iter:   60 | Time: 00:00:48 | Train Loss: 0.4910 | Average Loss: 0.7222 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:45 | Iter:    0 | Time: 00:00:49 | Train Loss: 0.3303 | Average Loss: 0.7204 \n","Epoch:45 | Iter:   20 | Time: 00:00:49 | Train Loss: 0.3254 | Average Loss: 0.7183 \n","Epoch:45 | Iter:   40 | Time: 00:00:49 | Train Loss: 0.3899 | Average Loss: 0.7160 \n","Epoch:45 | Iter:   60 | Time: 00:00:49 | Train Loss: 0.3403 | Average Loss: 0.7137 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:46 | Iter:    0 | Time: 00:00:50 | Train Loss: 0.5934 | Average Loss: 0.7120 \n","Epoch:46 | Iter:   20 | Time: 00:00:50 | Train Loss: 0.3152 | Average Loss: 0.7097 \n","Epoch:46 | Iter:   40 | Time: 00:00:50 | Train Loss: 0.3737 | Average Loss: 0.7076 \n","Epoch:46 | Iter:   60 | Time: 00:00:51 | Train Loss: 0.3234 | Average Loss: 0.7054 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:47 | Iter:    0 | Time: 00:00:51 | Train Loss: 0.2515 | Average Loss: 0.7039 \n","Epoch:47 | Iter:   20 | Time: 00:00:51 | Train Loss: 0.3599 | Average Loss: 0.7016 \n","Epoch:47 | Iter:   40 | Time: 00:00:51 | Train Loss: 0.3628 | Average Loss: 0.6994 \n","Epoch:47 | Iter:   60 | Time: 00:00:52 | Train Loss: 0.4192 | Average Loss: 0.6975 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:48 | Iter:    0 | Time: 00:00:52 | Train Loss: 0.4139 | Average Loss: 0.6959 \n","Epoch:48 | Iter:   20 | Time: 00:00:52 | Train Loss: 0.3450 | Average Loss: 0.6938 \n","Epoch:48 | Iter:   40 | Time: 00:00:52 | Train Loss: 0.4509 | Average Loss: 0.6916 \n","Epoch:48 | Iter:   60 | Time: 00:00:53 | Train Loss: 0.3221 | Average Loss: 0.6898 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:49 | Iter:    0 | Time: 00:00:53 | Train Loss: 0.5317 | Average Loss: 0.6885 \n","Epoch:49 | Iter:   20 | Time: 00:00:53 | Train Loss: 0.4223 | Average Loss: 0.6866 \n","Epoch:49 | Iter:   40 | Time: 00:00:54 | Train Loss: 0.3466 | Average Loss: 0.6846 \n","Epoch:49 | Iter:   60 | Time: 00:00:54 | Train Loss: 0.2046 | Average Loss: 0.6826 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:50 | Iter:    0 | Time: 00:00:54 | Train Loss: 0.4374 | Average Loss: 0.6814 \n","Epoch:50 | Iter:   20 | Time: 00:00:54 | Train Loss: 0.2740 | Average Loss: 0.6794 \n","Epoch:50 | Iter:   40 | Time: 00:00:55 | Train Loss: 0.4341 | Average Loss: 0.6773 \n","Epoch:50 | Iter:   60 | Time: 00:00:55 | Train Loss: 0.3544 | Average Loss: 0.6755 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:51 | Iter:    0 | Time: 00:00:55 | Train Loss: 0.4694 | Average Loss: 0.6742 \n","Epoch:51 | Iter:   20 | Time: 00:00:56 | Train Loss: 0.4900 | Average Loss: 0.6724 \n","Epoch:51 | Iter:   40 | Time: 00:00:56 | Train Loss: 0.3712 | Average Loss: 0.6705 \n","Epoch:51 | Iter:   60 | Time: 00:00:56 | Train Loss: 0.3198 | Average Loss: 0.6687 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:52 | Iter:    0 | Time: 00:00:56 | Train Loss: 0.3366 | Average Loss: 0.6671 \n","Epoch:52 | Iter:   20 | Time: 00:00:57 | Train Loss: 0.3576 | Average Loss: 0.6652 \n","Epoch:52 | Iter:   40 | Time: 00:00:57 | Train Loss: 0.3646 | Average Loss: 0.6633 \n","Epoch:52 | Iter:   60 | Time: 00:00:57 | Train Loss: 0.1104 | Average Loss: 0.6613 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:53 | Iter:    0 | Time: 00:00:57 | Train Loss: 0.2353 | Average Loss: 0.6598 \n","Epoch:53 | Iter:   20 | Time: 00:00:58 | Train Loss: 0.3726 | Average Loss: 0.6578 \n","Epoch:53 | Iter:   40 | Time: 00:00:58 | Train Loss: 0.2844 | Average Loss: 0.6558 \n","Epoch:53 | Iter:   60 | Time: 00:00:58 | Train Loss: 0.3355 | Average Loss: 0.6541 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:54 | Iter:    0 | Time: 00:00:59 | Train Loss: 0.2241 | Average Loss: 0.6527 \n","Epoch:54 | Iter:   20 | Time: 00:00:59 | Train Loss: 0.2993 | Average Loss: 0.6509 \n","Epoch:54 | Iter:   40 | Time: 00:00:59 | Train Loss: 0.3832 | Average Loss: 0.6490 \n","Epoch:54 | Iter:   60 | Time: 00:00:59 | Train Loss: 0.3727 | Average Loss: 0.6473 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:55 | Iter:    0 | Time: 00:01:00 | Train Loss: 0.2874 | Average Loss: 0.6461 \n","Epoch:55 | Iter:   20 | Time: 00:01:00 | Train Loss: 0.2647 | Average Loss: 0.6443 \n","Epoch:55 | Iter:   40 | Time: 00:01:00 | Train Loss: 0.3197 | Average Loss: 0.6425 \n","Epoch:55 | Iter:   60 | Time: 00:01:01 | Train Loss: 0.2508 | Average Loss: 0.6409 \n","Accuracy: 0.708333 | Time: 00:00:00\n","Epoch:56 | Iter:    0 | Time: 00:01:01 | Train Loss: 0.2851 | Average Loss: 0.6397 \n","Epoch:56 | Iter:   20 | Time: 00:01:01 | Train Loss: 0.2535 | Average Loss: 0.6381 \n","Epoch:56 | Iter:   40 | Time: 00:01:01 | Train Loss: 0.3373 | Average Loss: 0.6364 \n","Epoch:56 | Iter:   60 | Time: 00:01:02 | Train Loss: 0.2664 | Average Loss: 0.6346 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:57 | Iter:    0 | Time: 00:01:02 | Train Loss: 0.3607 | Average Loss: 0.6333 \n","Epoch:57 | Iter:   20 | Time: 00:01:02 | Train Loss: 0.3122 | Average Loss: 0.6315 \n","Epoch:57 | Iter:   40 | Time: 00:01:02 | Train Loss: 0.2015 | Average Loss: 0.6298 \n","Epoch:57 | Iter:   60 | Time: 00:01:03 | Train Loss: 0.2720 | Average Loss: 0.6281 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:58 | Iter:    0 | Time: 00:01:03 | Train Loss: 0.3510 | Average Loss: 0.6269 \n","Epoch:58 | Iter:   20 | Time: 00:01:03 | Train Loss: 0.2564 | Average Loss: 0.6254 \n","Epoch:58 | Iter:   40 | Time: 00:01:04 | Train Loss: 0.4396 | Average Loss: 0.6238 \n","Epoch:58 | Iter:   60 | Time: 00:01:04 | Train Loss: 0.2974 | Average Loss: 0.6221 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:59 | Iter:    0 | Time: 00:01:04 | Train Loss: 0.2539 | Average Loss: 0.6209 \n","Epoch:59 | Iter:   20 | Time: 00:01:04 | Train Loss: 0.3267 | Average Loss: 0.6192 \n","Epoch:59 | Iter:   40 | Time: 00:01:05 | Train Loss: 0.3411 | Average Loss: 0.6175 \n","Epoch:59 | Iter:   60 | Time: 00:01:05 | Train Loss: 0.2857 | Average Loss: 0.6160 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:60 | Iter:    0 | Time: 00:01:05 | Train Loss: 0.3644 | Average Loss: 0.6149 \n","Epoch:60 | Iter:   20 | Time: 00:01:06 | Train Loss: 0.2837 | Average Loss: 0.6134 \n","Epoch:60 | Iter:   40 | Time: 00:01:06 | Train Loss: 0.2007 | Average Loss: 0.6118 \n","Epoch:60 | Iter:   60 | Time: 00:01:06 | Train Loss: 0.4146 | Average Loss: 0.6104 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:61 | Iter:    0 | Time: 00:01:06 | Train Loss: 0.2911 | Average Loss: 0.6093 \n","Epoch:61 | Iter:   20 | Time: 00:01:07 | Train Loss: 0.3604 | Average Loss: 0.6078 \n","Epoch:61 | Iter:   40 | Time: 00:01:07 | Train Loss: 0.1652 | Average Loss: 0.6064 \n","Epoch:61 | Iter:   60 | Time: 00:01:07 | Train Loss: 0.2918 | Average Loss: 0.6049 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:62 | Iter:    0 | Time: 00:01:07 | Train Loss: 0.2027 | Average Loss: 0.6038 \n","Epoch:62 | Iter:   20 | Time: 00:01:08 | Train Loss: 0.3254 | Average Loss: 0.6025 \n","Epoch:62 | Iter:   40 | Time: 00:01:08 | Train Loss: 0.2780 | Average Loss: 0.6010 \n","Epoch:62 | Iter:   60 | Time: 00:01:08 | Train Loss: 0.3025 | Average Loss: 0.5997 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:63 | Iter:    0 | Time: 00:01:09 | Train Loss: 0.2632 | Average Loss: 0.5986 \n","Epoch:63 | Iter:   20 | Time: 00:01:09 | Train Loss: 0.2580 | Average Loss: 0.5972 \n","Epoch:63 | Iter:   40 | Time: 00:01:09 | Train Loss: 0.2402 | Average Loss: 0.5956 \n","Epoch:63 | Iter:   60 | Time: 00:01:09 | Train Loss: 0.2814 | Average Loss: 0.5942 \n","Accuracy: 0.705729 | Time: 00:00:00\n","Epoch:64 | Iter:    0 | Time: 00:01:10 | Train Loss: 0.3350 | Average Loss: 0.5932 \n","Epoch:64 | Iter:   20 | Time: 00:01:10 | Train Loss: 0.2336 | Average Loss: 0.5918 \n","Epoch:64 | Iter:   40 | Time: 00:01:10 | Train Loss: 0.4888 | Average Loss: 0.5904 \n","Epoch:64 | Iter:   60 | Time: 00:01:11 | Train Loss: 0.3101 | Average Loss: 0.5892 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:65 | Iter:    0 | Time: 00:01:11 | Train Loss: 0.3375 | Average Loss: 0.5883 \n","Epoch:65 | Iter:   20 | Time: 00:01:11 | Train Loss: 0.2709 | Average Loss: 0.5868 \n","Epoch:65 | Iter:   40 | Time: 00:01:11 | Train Loss: 0.1857 | Average Loss: 0.5853 \n","Epoch:65 | Iter:   60 | Time: 00:01:12 | Train Loss: 0.1570 | Average Loss: 0.5840 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:66 | Iter:    0 | Time: 00:01:12 | Train Loss: 0.2748 | Average Loss: 0.5829 \n","Epoch:66 | Iter:   20 | Time: 00:01:12 | Train Loss: 0.1876 | Average Loss: 0.5816 \n","Epoch:66 | Iter:   40 | Time: 00:01:12 | Train Loss: 0.2815 | Average Loss: 0.5802 \n","Epoch:66 | Iter:   60 | Time: 00:01:13 | Train Loss: 0.2257 | Average Loss: 0.5788 \n","Accuracy: 0.710938 | Time: 00:00:00\n","Epoch:67 | Iter:    0 | Time: 00:01:13 | Train Loss: 0.2500 | Average Loss: 0.5777 \n","Epoch:67 | Iter:   20 | Time: 00:01:13 | Train Loss: 0.3210 | Average Loss: 0.5764 \n","Epoch:67 | Iter:   40 | Time: 00:01:14 | Train Loss: 0.3450 | Average Loss: 0.5750 \n","Epoch:67 | Iter:   60 | Time: 00:01:14 | Train Loss: 0.2192 | Average Loss: 0.5738 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:68 | Iter:    0 | Time: 00:01:14 | Train Loss: 0.3243 | Average Loss: 0.5729 \n","Epoch:68 | Iter:   20 | Time: 00:01:14 | Train Loss: 0.2872 | Average Loss: 0.5716 \n","Epoch:68 | Iter:   40 | Time: 00:01:15 | Train Loss: 0.1771 | Average Loss: 0.5701 \n","Epoch:68 | Iter:   60 | Time: 00:01:15 | Train Loss: 0.1406 | Average Loss: 0.5688 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:69 | Iter:    0 | Time: 00:01:15 | Train Loss: 0.2774 | Average Loss: 0.5679 \n","Epoch:69 | Iter:   20 | Time: 00:01:16 | Train Loss: 0.2897 | Average Loss: 0.5665 \n","Epoch:69 | Iter:   40 | Time: 00:01:16 | Train Loss: 0.1774 | Average Loss: 0.5651 \n","Epoch:69 | Iter:   60 | Time: 00:01:16 | Train Loss: 0.3635 | Average Loss: 0.5639 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:70 | Iter:    0 | Time: 00:01:16 | Train Loss: 0.2631 | Average Loss: 0.5629 \n","Epoch:70 | Iter:   20 | Time: 00:01:17 | Train Loss: 0.2701 | Average Loss: 0.5616 \n","Epoch:70 | Iter:   40 | Time: 00:01:17 | Train Loss: 0.2317 | Average Loss: 0.5604 \n","Epoch:70 | Iter:   60 | Time: 00:01:17 | Train Loss: 0.3423 | Average Loss: 0.5592 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:71 | Iter:    0 | Time: 00:01:17 | Train Loss: 0.3987 | Average Loss: 0.5583 \n","Epoch:71 | Iter:   20 | Time: 00:01:18 | Train Loss: 0.2423 | Average Loss: 0.5570 \n","Epoch:71 | Iter:   40 | Time: 00:01:18 | Train Loss: 0.2173 | Average Loss: 0.5558 \n","Epoch:71 | Iter:   60 | Time: 00:01:18 | Train Loss: 0.1922 | Average Loss: 0.5545 \n","Accuracy: 0.718750 | Time: 00:00:00\n","Epoch:72 | Iter:    0 | Time: 00:01:19 | Train Loss: 0.1654 | Average Loss: 0.5537 \n","Epoch:72 | Iter:   20 | Time: 00:01:19 | Train Loss: 0.2393 | Average Loss: 0.5524 \n","Epoch:72 | Iter:   40 | Time: 00:01:19 | Train Loss: 0.2848 | Average Loss: 0.5512 \n","Epoch:72 | Iter:   60 | Time: 00:01:19 | Train Loss: 0.2154 | Average Loss: 0.5500 \n","Accuracy: 0.705729 | Time: 00:00:00\n","Epoch:73 | Iter:    0 | Time: 00:01:20 | Train Loss: 0.2731 | Average Loss: 0.5491 \n","Epoch:73 | Iter:   20 | Time: 00:01:20 | Train Loss: 0.1974 | Average Loss: 0.5479 \n","Epoch:73 | Iter:   40 | Time: 00:01:20 | Train Loss: 0.1754 | Average Loss: 0.5466 \n","Epoch:73 | Iter:   60 | Time: 00:01:21 | Train Loss: 0.1761 | Average Loss: 0.5455 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:74 | Iter:    0 | Time: 00:01:21 | Train Loss: 0.2982 | Average Loss: 0.5447 \n","Epoch:74 | Iter:   20 | Time: 00:01:21 | Train Loss: 0.2320 | Average Loss: 0.5435 \n","Epoch:74 | Iter:   40 | Time: 00:01:21 | Train Loss: 0.2142 | Average Loss: 0.5423 \n","Epoch:74 | Iter:   60 | Time: 00:01:22 | Train Loss: 0.3150 | Average Loss: 0.5411 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:75 | Iter:    0 | Time: 00:01:22 | Train Loss: 0.2482 | Average Loss: 0.5403 \n","Epoch:75 | Iter:   20 | Time: 00:01:22 | Train Loss: 0.1469 | Average Loss: 0.5390 \n","Epoch:75 | Iter:   40 | Time: 00:01:22 | Train Loss: 0.2129 | Average Loss: 0.5378 \n","Epoch:75 | Iter:   60 | Time: 00:01:23 | Train Loss: 0.2450 | Average Loss: 0.5368 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:76 | Iter:    0 | Time: 00:01:23 | Train Loss: 0.2618 | Average Loss: 0.5360 \n","Epoch:76 | Iter:   20 | Time: 00:01:23 | Train Loss: 0.1381 | Average Loss: 0.5350 \n","Epoch:76 | Iter:   40 | Time: 00:01:24 | Train Loss: 0.2257 | Average Loss: 0.5339 \n","Epoch:76 | Iter:   60 | Time: 00:01:24 | Train Loss: 0.2479 | Average Loss: 0.5328 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:77 | Iter:    0 | Time: 00:01:24 | Train Loss: 0.2285 | Average Loss: 0.5319 \n","Epoch:77 | Iter:   20 | Time: 00:01:24 | Train Loss: 0.1606 | Average Loss: 0.5308 \n","Epoch:77 | Iter:   40 | Time: 00:01:25 | Train Loss: 0.2702 | Average Loss: 0.5298 \n","Epoch:77 | Iter:   60 | Time: 00:01:25 | Train Loss: 0.1712 | Average Loss: 0.5287 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:78 | Iter:    0 | Time: 00:01:25 | Train Loss: 0.1567 | Average Loss: 0.5278 \n","Epoch:78 | Iter:   20 | Time: 00:01:26 | Train Loss: 0.1416 | Average Loss: 0.5266 \n","Epoch:78 | Iter:   40 | Time: 00:01:26 | Train Loss: 0.2418 | Average Loss: 0.5255 \n","Epoch:78 | Iter:   60 | Time: 00:01:26 | Train Loss: 0.3119 | Average Loss: 0.5244 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:79 | Iter:    0 | Time: 00:01:26 | Train Loss: 0.3138 | Average Loss: 0.5236 \n","Epoch:79 | Iter:   20 | Time: 00:01:27 | Train Loss: 0.1512 | Average Loss: 0.5224 \n","Epoch:79 | Iter:   40 | Time: 00:01:27 | Train Loss: 0.2733 | Average Loss: 0.5213 \n","Epoch:79 | Iter:   60 | Time: 00:01:27 | Train Loss: 0.2246 | Average Loss: 0.5202 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:80 | Iter:    0 | Time: 00:01:27 | Train Loss: 0.1618 | Average Loss: 0.5195 \n","Epoch:80 | Iter:   20 | Time: 00:01:28 | Train Loss: 0.1616 | Average Loss: 0.5185 \n","Epoch:80 | Iter:   40 | Time: 00:01:28 | Train Loss: 0.1660 | Average Loss: 0.5175 \n","Epoch:80 | Iter:   60 | Time: 00:01:28 | Train Loss: 0.3687 | Average Loss: 0.5164 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:81 | Iter:    0 | Time: 00:01:29 | Train Loss: 0.1287 | Average Loss: 0.5157 \n","Epoch:81 | Iter:   20 | Time: 00:01:29 | Train Loss: 0.1118 | Average Loss: 0.5146 \n","Epoch:81 | Iter:   40 | Time: 00:01:29 | Train Loss: 0.2329 | Average Loss: 0.5135 \n","Epoch:81 | Iter:   60 | Time: 00:01:29 | Train Loss: 0.1497 | Average Loss: 0.5125 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:82 | Iter:    0 | Time: 00:01:30 | Train Loss: 0.3676 | Average Loss: 0.5118 \n","Epoch:82 | Iter:   20 | Time: 00:01:30 | Train Loss: 0.1444 | Average Loss: 0.5107 \n","Epoch:82 | Iter:   40 | Time: 00:01:30 | Train Loss: 0.3404 | Average Loss: 0.5098 \n","Epoch:82 | Iter:   60 | Time: 00:01:31 | Train Loss: 0.1895 | Average Loss: 0.5088 \n","Accuracy: 0.710938 | Time: 00:00:00\n","Epoch:83 | Iter:    0 | Time: 00:01:31 | Train Loss: 0.4071 | Average Loss: 0.5081 \n","Epoch:83 | Iter:   20 | Time: 00:01:31 | Train Loss: 0.1714 | Average Loss: 0.5073 \n","Epoch:83 | Iter:   40 | Time: 00:01:31 | Train Loss: 0.2460 | Average Loss: 0.5063 \n","Epoch:83 | Iter:   60 | Time: 00:01:32 | Train Loss: 0.3686 | Average Loss: 0.5055 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:84 | Iter:    0 | Time: 00:01:32 | Train Loss: 0.1912 | Average Loss: 0.5048 \n","Epoch:84 | Iter:   20 | Time: 00:01:32 | Train Loss: 0.0883 | Average Loss: 0.5038 \n","Epoch:84 | Iter:   40 | Time: 00:01:33 | Train Loss: 0.2435 | Average Loss: 0.5030 \n","Epoch:84 | Iter:   60 | Time: 00:01:33 | Train Loss: 0.1170 | Average Loss: 0.5020 \n","Accuracy: 0.708333 | Time: 00:00:00\n","Epoch:85 | Iter:    0 | Time: 00:01:33 | Train Loss: 0.2200 | Average Loss: 0.5013 \n","Epoch:85 | Iter:   20 | Time: 00:01:33 | Train Loss: 0.1201 | Average Loss: 0.5003 \n","Epoch:85 | Iter:   40 | Time: 00:01:34 | Train Loss: 0.1295 | Average Loss: 0.4993 \n","Epoch:85 | Iter:   60 | Time: 00:01:34 | Train Loss: 0.1668 | Average Loss: 0.4983 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:86 | Iter:    0 | Time: 00:01:34 | Train Loss: 0.2559 | Average Loss: 0.4977 \n","Epoch:86 | Iter:   20 | Time: 00:01:34 | Train Loss: 0.1992 | Average Loss: 0.4967 \n","Epoch:86 | Iter:   40 | Time: 00:01:35 | Train Loss: 0.3083 | Average Loss: 0.4958 \n","Epoch:86 | Iter:   60 | Time: 00:01:35 | Train Loss: 0.3735 | Average Loss: 0.4950 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:87 | Iter:    0 | Time: 00:01:35 | Train Loss: 0.2342 | Average Loss: 0.4943 \n","Epoch:87 | Iter:   20 | Time: 00:01:36 | Train Loss: 0.1632 | Average Loss: 0.4934 \n","Epoch:87 | Iter:   40 | Time: 00:01:36 | Train Loss: 0.2194 | Average Loss: 0.4924 \n","Epoch:87 | Iter:   60 | Time: 00:01:36 | Train Loss: 0.1916 | Average Loss: 0.4915 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:88 | Iter:    0 | Time: 00:01:36 | Train Loss: 0.0966 | Average Loss: 0.4907 \n","Epoch:88 | Iter:   20 | Time: 00:01:37 | Train Loss: 0.2181 | Average Loss: 0.4898 \n","Epoch:88 | Iter:   40 | Time: 00:01:37 | Train Loss: 0.1879 | Average Loss: 0.4888 \n","Epoch:88 | Iter:   60 | Time: 00:01:37 | Train Loss: 0.1211 | Average Loss: 0.4879 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:89 | Iter:    0 | Time: 00:01:37 | Train Loss: 0.3388 | Average Loss: 0.4872 \n","Epoch:89 | Iter:   20 | Time: 00:01:38 | Train Loss: 0.1193 | Average Loss: 0.4863 \n","Epoch:89 | Iter:   40 | Time: 00:01:38 | Train Loss: 0.1765 | Average Loss: 0.4854 \n","Epoch:89 | Iter:   60 | Time: 00:01:38 | Train Loss: 0.2299 | Average Loss: 0.4846 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:90 | Iter:    0 | Time: 00:01:39 | Train Loss: 0.2183 | Average Loss: 0.4840 \n","Epoch:90 | Iter:   20 | Time: 00:01:39 | Train Loss: 0.2823 | Average Loss: 0.4831 \n","Epoch:90 | Iter:   40 | Time: 00:01:39 | Train Loss: 0.2181 | Average Loss: 0.4822 \n","Epoch:90 | Iter:   60 | Time: 00:01:39 | Train Loss: 0.1637 | Average Loss: 0.4814 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:91 | Iter:    0 | Time: 00:01:40 | Train Loss: 0.1292 | Average Loss: 0.4807 \n","Epoch:91 | Iter:   20 | Time: 00:01:40 | Train Loss: 0.0914 | Average Loss: 0.4798 \n","Epoch:91 | Iter:   40 | Time: 00:01:40 | Train Loss: 0.1473 | Average Loss: 0.4789 \n","Epoch:91 | Iter:   60 | Time: 00:01:41 | Train Loss: 0.1897 | Average Loss: 0.4782 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:92 | Iter:    0 | Time: 00:01:41 | Train Loss: 0.1544 | Average Loss: 0.4775 \n","Epoch:92 | Iter:   20 | Time: 00:01:41 | Train Loss: 0.1119 | Average Loss: 0.4768 \n","Epoch:92 | Iter:   40 | Time: 00:01:41 | Train Loss: 0.1911 | Average Loss: 0.4759 \n","Epoch:92 | Iter:   60 | Time: 00:01:42 | Train Loss: 0.2347 | Average Loss: 0.4750 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:93 | Iter:    0 | Time: 00:01:42 | Train Loss: 0.1763 | Average Loss: 0.4744 \n","Epoch:93 | Iter:   20 | Time: 00:01:42 | Train Loss: 0.1537 | Average Loss: 0.4735 \n","Epoch:93 | Iter:   40 | Time: 00:01:43 | Train Loss: 0.3261 | Average Loss: 0.4727 \n","Epoch:93 | Iter:   60 | Time: 00:01:43 | Train Loss: 0.1167 | Average Loss: 0.4718 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:94 | Iter:    0 | Time: 00:01:43 | Train Loss: 0.1864 | Average Loss: 0.4712 \n","Epoch:94 | Iter:   20 | Time: 00:01:43 | Train Loss: 0.1214 | Average Loss: 0.4703 \n","Epoch:94 | Iter:   40 | Time: 00:01:44 | Train Loss: 0.2057 | Average Loss: 0.4695 \n","Epoch:94 | Iter:   60 | Time: 00:01:44 | Train Loss: 0.1578 | Average Loss: 0.4687 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:95 | Iter:    0 | Time: 00:01:44 | Train Loss: 0.2998 | Average Loss: 0.4681 \n","Epoch:95 | Iter:   20 | Time: 00:01:44 | Train Loss: 0.1749 | Average Loss: 0.4674 \n","Epoch:95 | Iter:   40 | Time: 00:01:45 | Train Loss: 0.1102 | Average Loss: 0.4666 \n","Epoch:95 | Iter:   60 | Time: 00:01:45 | Train Loss: 0.0776 | Average Loss: 0.4658 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:96 | Iter:    0 | Time: 00:01:45 | Train Loss: 0.1665 | Average Loss: 0.4652 \n","Epoch:96 | Iter:   20 | Time: 00:01:46 | Train Loss: 0.1736 | Average Loss: 0.4644 \n","Epoch:96 | Iter:   40 | Time: 00:01:46 | Train Loss: 0.2079 | Average Loss: 0.4636 \n","Epoch:96 | Iter:   60 | Time: 00:01:46 | Train Loss: 0.2655 | Average Loss: 0.4628 \n","Accuracy: 0.705729 | Time: 00:00:00\n","Epoch:97 | Iter:    0 | Time: 00:01:46 | Train Loss: 0.0976 | Average Loss: 0.4622 \n","Epoch:97 | Iter:   20 | Time: 00:01:47 | Train Loss: 0.1569 | Average Loss: 0.4614 \n","Epoch:97 | Iter:   40 | Time: 00:01:47 | Train Loss: 0.1123 | Average Loss: 0.4606 \n","Epoch:97 | Iter:   60 | Time: 00:01:47 | Train Loss: 0.2636 | Average Loss: 0.4599 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:98 | Iter:    0 | Time: 00:01:48 | Train Loss: 0.0716 | Average Loss: 0.4592 \n","Epoch:98 | Iter:   20 | Time: 00:01:48 | Train Loss: 0.1349 | Average Loss: 0.4584 \n","Epoch:98 | Iter:   40 | Time: 00:01:48 | Train Loss: 0.2838 | Average Loss: 0.4576 \n","Epoch:98 | Iter:   60 | Time: 00:01:48 | Train Loss: 0.2502 | Average Loss: 0.4569 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:99 | Iter:    0 | Time: 00:01:49 | Train Loss: 0.0843 | Average Loss: 0.4564 \n","Epoch:99 | Iter:   20 | Time: 00:01:49 | Train Loss: 0.2158 | Average Loss: 0.4555 \n","Epoch:99 | Iter:   40 | Time: 00:01:49 | Train Loss: 0.1797 | Average Loss: 0.4548 \n","Epoch:99 | Iter:   60 | Time: 00:01:49 | Train Loss: 0.1680 | Average Loss: 0.4540 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:100 | Iter:    0 | Time: 00:01:50 | Train Loss: 0.1125 | Average Loss: 0.4534 \n","Epoch:100 | Iter:   20 | Time: 00:01:50 | Train Loss: 0.0718 | Average Loss: 0.4527 \n","Epoch:100 | Iter:   40 | Time: 00:01:50 | Train Loss: 0.1946 | Average Loss: 0.4519 \n","Epoch:100 | Iter:   60 | Time: 00:01:51 | Train Loss: 0.1976 | Average Loss: 0.4511 \n","Accuracy: 0.687500 | Time: 00:00:00\n"]}],"source":["#--------------------------------------------------\n","#       Start Training & Evaluation\n","#--------------------------------------------------\n","net = Improved_TNet()\n","train_option = {}\n","train_option['lr'] = 0.002\n","train_option['epoch'] = 100\n","train_option['device'] = 'gpu'\n","trainModel(net, trainloader_small, train_option, testloader_small)\n"]},{"cell_type":"markdown","metadata":{"id":"6RG-72WSzVEp"},"source":["Tried **three different techniques** to increase the accuracy of my model. \n","\n","### Technique 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rq6mIYHMfUcv","executionInfo":{"status":"ok","timestamp":1650857281231,"user_tz":240,"elapsed":13771,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"18ed4489-5951-4ded-8064-efa12fcc8bc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading images from class: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 150 minibatches (batch_size=32) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 12 minibatches (batch_size=32) of testing samples.\n"]}],"source":["#--------------------------------------------------\n","#    Load Training Data and Testing Data\n","#--------------------------------------------------\n","# load data into size (64, 64)\n","img_size = (64, 64)\n","batch_size = 32\n","\n","# load training dataset\n","trainloader_small = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True, augment=True, zero_centered=True))\n","train_num = len(trainloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n","\n","# load testing dataset\n","testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size, zero_centered=True))\n","test_num = len(testloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhdvKxQoSpKw"},"outputs":[],"source":["#--------------------------------------------------\n","#       Define Network Architecture\n","#--------------------------------------------------\n","class ModelArchitectureModification(nn.Module):\n","  def __init__(self):\n","    super(ModelArchitectureModification,self).__init__()\n","    self.features = torch.nn.Sequential(\n","        nn.Conv2d(1, 64, 3),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4),\n","        nn.Dropout(0.5),\n","        nn.Conv2d(64, 400, 3),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4),\n","        nn.Dropout(0.5)\n","    )\n","\n","    self.classifier = nn.Sequential(\n","        nn.Linear(3600, 16),\n","    )\n","    \n","  def forward(self, x):\n","    x = self.features(x)\n","    x = torch.flatten(x, 1)\n","    x = self.classifier(x)\n","    return x\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_T2UG90ATZ3K","executionInfo":{"status":"ok","timestamp":1650852925677,"user_tz":240,"elapsed":246741,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"4386a3f0-6e4e-4ba0-8d5a-642a3aed1a49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.8477 | Average Loss: 2.8477 \n","Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.5501 | Average Loss: 2.8028 \n","Epoch: 1 | Iter:   40 | Time: 00:00:01 | Train Loss: 2.4442 | Average Loss: 2.6408 \n","Epoch: 1 | Iter:   60 | Time: 00:00:02 | Train Loss: 2.1771 | Average Loss: 2.5380 \n","Accuracy: 0.286458 | Time: 00:00:00\n","Epoch: 2 | Iter:    0 | Time: 00:00:02 | Train Loss: 2.3020 | Average Loss: 2.4819 \n","Epoch: 2 | Iter:   20 | Time: 00:00:03 | Train Loss: 1.9120 | Average Loss: 2.4111 \n","Epoch: 2 | Iter:   40 | Time: 00:00:03 | Train Loss: 1.9274 | Average Loss: 2.3225 \n","Epoch: 2 | Iter:   60 | Time: 00:00:04 | Train Loss: 1.7224 | Average Loss: 2.2585 \n","Accuracy: 0.445312 | Time: 00:00:00\n","Epoch: 3 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.8909 | Average Loss: 2.2177 \n","Epoch: 3 | Iter:   20 | Time: 00:00:05 | Train Loss: 1.7082 | Average Loss: 2.1599 \n","Epoch: 3 | Iter:   40 | Time: 00:00:06 | Train Loss: 1.7456 | Average Loss: 2.1017 \n","Epoch: 3 | Iter:   60 | Time: 00:00:07 | Train Loss: 1.5361 | Average Loss: 2.0541 \n","Accuracy: 0.505208 | Time: 00:00:00\n","Epoch: 4 | Iter:    0 | Time: 00:00:07 | Train Loss: 1.6256 | Average Loss: 2.0204 \n","Epoch: 4 | Iter:   20 | Time: 00:00:08 | Train Loss: 1.5575 | Average Loss: 1.9778 \n","Epoch: 4 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.5765 | Average Loss: 1.9360 \n","Epoch: 4 | Iter:   60 | Time: 00:00:09 | Train Loss: 1.4647 | Average Loss: 1.9019 \n","Accuracy: 0.562500 | Time: 00:00:00\n","Epoch: 5 | Iter:    0 | Time: 00:00:10 | Train Loss: 1.5846 | Average Loss: 1.8790 \n","Epoch: 5 | Iter:   20 | Time: 00:00:10 | Train Loss: 1.5215 | Average Loss: 1.8491 \n","Epoch: 5 | Iter:   40 | Time: 00:00:11 | Train Loss: 1.3266 | Average Loss: 1.8153 \n","Epoch: 5 | Iter:   60 | Time: 00:00:11 | Train Loss: 1.2937 | Average Loss: 1.7867 \n","Accuracy: 0.604167 | Time: 00:00:00\n","Epoch: 6 | Iter:    0 | Time: 00:00:12 | Train Loss: 1.3776 | Average Loss: 1.7660 \n","Epoch: 6 | Iter:   20 | Time: 00:00:13 | Train Loss: 1.2680 | Average Loss: 1.7406 \n","Epoch: 6 | Iter:   40 | Time: 00:00:13 | Train Loss: 1.1389 | Average Loss: 1.7142 \n","Epoch: 6 | Iter:   60 | Time: 00:00:14 | Train Loss: 1.2318 | Average Loss: 1.6930 \n","Accuracy: 0.611979 | Time: 00:00:00\n","Epoch: 7 | Iter:    0 | Time: 00:00:14 | Train Loss: 1.2893 | Average Loss: 1.6775 \n","Epoch: 7 | Iter:   20 | Time: 00:00:15 | Train Loss: 1.2819 | Average Loss: 1.6564 \n","Epoch: 7 | Iter:   40 | Time: 00:00:16 | Train Loss: 1.0523 | Average Loss: 1.6347 \n","Epoch: 7 | Iter:   60 | Time: 00:00:16 | Train Loss: 1.0510 | Average Loss: 1.6164 \n","Accuracy: 0.627604 | Time: 00:00:00\n","Epoch: 8 | Iter:    0 | Time: 00:00:17 | Train Loss: 1.3602 | Average Loss: 1.6031 \n","Epoch: 8 | Iter:   20 | Time: 00:00:18 | Train Loss: 1.0747 | Average Loss: 1.5853 \n","Epoch: 8 | Iter:   40 | Time: 00:00:18 | Train Loss: 1.2485 | Average Loss: 1.5683 \n","Epoch: 8 | Iter:   60 | Time: 00:00:19 | Train Loss: 1.2130 | Average Loss: 1.5532 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch: 9 | Iter:    0 | Time: 00:00:19 | Train Loss: 1.3165 | Average Loss: 1.5423 \n","Epoch: 9 | Iter:   20 | Time: 00:00:20 | Train Loss: 1.0860 | Average Loss: 1.5263 \n","Epoch: 9 | Iter:   40 | Time: 00:00:21 | Train Loss: 1.0992 | Average Loss: 1.5110 \n","Epoch: 9 | Iter:   60 | Time: 00:00:21 | Train Loss: 1.1261 | Average Loss: 1.4977 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:10 | Iter:    0 | Time: 00:00:22 | Train Loss: 1.1179 | Average Loss: 1.4882 \n","Epoch:10 | Iter:   20 | Time: 00:00:22 | Train Loss: 1.1714 | Average Loss: 1.4752 \n","Epoch:10 | Iter:   40 | Time: 00:00:23 | Train Loss: 1.0443 | Average Loss: 1.4621 \n","Epoch:10 | Iter:   60 | Time: 00:00:24 | Train Loss: 0.9347 | Average Loss: 1.4503 \n","Accuracy: 0.645833 | Time: 00:00:00\n","Epoch:11 | Iter:    0 | Time: 00:00:24 | Train Loss: 1.1088 | Average Loss: 1.4425 \n","Epoch:11 | Iter:   20 | Time: 00:00:25 | Train Loss: 0.8498 | Average Loss: 1.4316 \n","Epoch:11 | Iter:   40 | Time: 00:00:26 | Train Loss: 1.0333 | Average Loss: 1.4193 \n","Epoch:11 | Iter:   60 | Time: 00:00:26 | Train Loss: 0.8475 | Average Loss: 1.4087 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:12 | Iter:    0 | Time: 00:00:27 | Train Loss: 1.1083 | Average Loss: 1.4019 \n","Epoch:12 | Iter:   20 | Time: 00:00:27 | Train Loss: 0.9738 | Average Loss: 1.3910 \n","Epoch:12 | Iter:   40 | Time: 00:00:28 | Train Loss: 1.1276 | Average Loss: 1.3808 \n","Epoch:12 | Iter:   60 | Time: 00:00:29 | Train Loss: 0.7104 | Average Loss: 1.3708 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:13 | Iter:    0 | Time: 00:00:29 | Train Loss: 1.1932 | Average Loss: 1.3644 \n","Epoch:13 | Iter:   20 | Time: 00:00:30 | Train Loss: 0.9697 | Average Loss: 1.3553 \n","Epoch:13 | Iter:   40 | Time: 00:00:30 | Train Loss: 0.9942 | Average Loss: 1.3458 \n","Epoch:13 | Iter:   60 | Time: 00:00:31 | Train Loss: 0.9152 | Average Loss: 1.3377 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:14 | Iter:    0 | Time: 00:00:32 | Train Loss: 1.0516 | Average Loss: 1.3319 \n","Epoch:14 | Iter:   20 | Time: 00:00:32 | Train Loss: 0.9773 | Average Loss: 1.3242 \n","Epoch:14 | Iter:   40 | Time: 00:00:33 | Train Loss: 0.9266 | Average Loss: 1.3163 \n","Epoch:14 | Iter:   60 | Time: 00:00:34 | Train Loss: 0.8754 | Average Loss: 1.3081 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:15 | Iter:    0 | Time: 00:00:34 | Train Loss: 0.9422 | Average Loss: 1.3024 \n","Epoch:15 | Iter:   20 | Time: 00:00:35 | Train Loss: 0.9170 | Average Loss: 1.2953 \n","Epoch:15 | Iter:   40 | Time: 00:00:35 | Train Loss: 0.8877 | Average Loss: 1.2874 \n","Epoch:15 | Iter:   60 | Time: 00:00:36 | Train Loss: 0.8287 | Average Loss: 1.2803 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:16 | Iter:    0 | Time: 00:00:37 | Train Loss: 1.0370 | Average Loss: 1.2750 \n","Epoch:16 | Iter:   20 | Time: 00:00:37 | Train Loss: 0.9174 | Average Loss: 1.2680 \n","Epoch:16 | Iter:   40 | Time: 00:00:38 | Train Loss: 0.7758 | Average Loss: 1.2610 \n","Epoch:16 | Iter:   60 | Time: 00:00:38 | Train Loss: 0.8465 | Average Loss: 1.2550 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:17 | Iter:    0 | Time: 00:00:39 | Train Loss: 0.8078 | Average Loss: 1.2499 \n","Epoch:17 | Iter:   20 | Time: 00:00:40 | Train Loss: 0.8408 | Average Loss: 1.2425 \n","Epoch:17 | Iter:   40 | Time: 00:00:40 | Train Loss: 0.9956 | Average Loss: 1.2357 \n","Epoch:17 | Iter:   60 | Time: 00:00:41 | Train Loss: 0.7282 | Average Loss: 1.2293 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:18 | Iter:    0 | Time: 00:00:41 | Train Loss: 0.9919 | Average Loss: 1.2247 \n","Epoch:18 | Iter:   20 | Time: 00:00:42 | Train Loss: 0.8013 | Average Loss: 1.2187 \n","Epoch:18 | Iter:   40 | Time: 00:00:43 | Train Loss: 0.9839 | Average Loss: 1.2126 \n","Epoch:18 | Iter:   60 | Time: 00:00:43 | Train Loss: 0.7484 | Average Loss: 1.2067 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:19 | Iter:    0 | Time: 00:00:44 | Train Loss: 0.9663 | Average Loss: 1.2025 \n","Epoch:19 | Iter:   20 | Time: 00:00:45 | Train Loss: 0.7691 | Average Loss: 1.1969 \n","Epoch:19 | Iter:   40 | Time: 00:00:45 | Train Loss: 0.7566 | Average Loss: 1.1911 \n","Epoch:19 | Iter:   60 | Time: 00:00:46 | Train Loss: 0.6745 | Average Loss: 1.1854 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:20 | Iter:    0 | Time: 00:00:46 | Train Loss: 1.0054 | Average Loss: 1.1816 \n","Epoch:20 | Iter:   20 | Time: 00:00:47 | Train Loss: 0.7107 | Average Loss: 1.1764 \n","Epoch:20 | Iter:   40 | Time: 00:00:48 | Train Loss: 0.7377 | Average Loss: 1.1707 \n","Epoch:20 | Iter:   60 | Time: 00:00:48 | Train Loss: 0.7743 | Average Loss: 1.1660 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:21 | Iter:    0 | Time: 00:00:49 | Train Loss: 0.9437 | Average Loss: 1.1626 \n","Epoch:21 | Iter:   20 | Time: 00:00:49 | Train Loss: 0.6259 | Average Loss: 1.1574 \n","Epoch:21 | Iter:   40 | Time: 00:00:50 | Train Loss: 0.7231 | Average Loss: 1.1521 \n","Epoch:21 | Iter:   60 | Time: 00:00:51 | Train Loss: 0.7501 | Average Loss: 1.1479 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:22 | Iter:    0 | Time: 00:00:51 | Train Loss: 0.8768 | Average Loss: 1.1447 \n","Epoch:22 | Iter:   20 | Time: 00:00:52 | Train Loss: 0.6574 | Average Loss: 1.1398 \n","Epoch:22 | Iter:   40 | Time: 00:00:53 | Train Loss: 0.9071 | Average Loss: 1.1347 \n","Epoch:22 | Iter:   60 | Time: 00:00:53 | Train Loss: 0.6773 | Average Loss: 1.1301 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:23 | Iter:    0 | Time: 00:00:54 | Train Loss: 0.7275 | Average Loss: 1.1266 \n","Epoch:23 | Iter:   20 | Time: 00:00:54 | Train Loss: 0.7621 | Average Loss: 1.1215 \n","Epoch:23 | Iter:   40 | Time: 00:00:55 | Train Loss: 0.8158 | Average Loss: 1.1169 \n","Epoch:23 | Iter:   60 | Time: 00:00:56 | Train Loss: 0.6006 | Average Loss: 1.1128 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:24 | Iter:    0 | Time: 00:00:56 | Train Loss: 1.0477 | Average Loss: 1.1098 \n","Epoch:24 | Iter:   20 | Time: 00:00:57 | Train Loss: 0.7066 | Average Loss: 1.1058 \n","Epoch:24 | Iter:   40 | Time: 00:00:57 | Train Loss: 0.7728 | Average Loss: 1.1011 \n","Epoch:24 | Iter:   60 | Time: 00:00:58 | Train Loss: 0.6220 | Average Loss: 1.0968 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:25 | Iter:    0 | Time: 00:00:59 | Train Loss: 0.7058 | Average Loss: 1.0940 \n","Epoch:25 | Iter:   20 | Time: 00:00:59 | Train Loss: 0.7126 | Average Loss: 1.0901 \n","Epoch:25 | Iter:   40 | Time: 00:01:00 | Train Loss: 0.8335 | Average Loss: 1.0860 \n","Epoch:25 | Iter:   60 | Time: 00:01:01 | Train Loss: 0.6111 | Average Loss: 1.0820 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:26 | Iter:    0 | Time: 00:01:01 | Train Loss: 0.8012 | Average Loss: 1.0793 \n","Epoch:26 | Iter:   20 | Time: 00:01:02 | Train Loss: 0.8755 | Average Loss: 1.0758 \n","Epoch:26 | Iter:   40 | Time: 00:01:02 | Train Loss: 0.7232 | Average Loss: 1.0713 \n","Epoch:26 | Iter:   60 | Time: 00:01:03 | Train Loss: 0.6354 | Average Loss: 1.0677 \n","Accuracy: 0.656250 | Time: 00:00:00\n","Epoch:27 | Iter:    0 | Time: 00:01:04 | Train Loss: 0.7124 | Average Loss: 1.0650 \n","Epoch:27 | Iter:   20 | Time: 00:01:04 | Train Loss: 0.6909 | Average Loss: 1.0610 \n","Epoch:27 | Iter:   40 | Time: 00:01:05 | Train Loss: 0.6799 | Average Loss: 1.0576 \n","Epoch:27 | Iter:   60 | Time: 00:01:05 | Train Loss: 0.5974 | Average Loss: 1.0541 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:28 | Iter:    0 | Time: 00:01:06 | Train Loss: 0.7860 | Average Loss: 1.0519 \n","Epoch:28 | Iter:   20 | Time: 00:01:07 | Train Loss: 0.6943 | Average Loss: 1.0482 \n","Epoch:28 | Iter:   40 | Time: 00:01:07 | Train Loss: 0.8136 | Average Loss: 1.0447 \n","Epoch:28 | Iter:   60 | Time: 00:01:08 | Train Loss: 0.6408 | Average Loss: 1.0413 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:29 | Iter:    0 | Time: 00:01:08 | Train Loss: 0.7488 | Average Loss: 1.0389 \n","Epoch:29 | Iter:   20 | Time: 00:01:09 | Train Loss: 0.9569 | Average Loss: 1.0359 \n","Epoch:29 | Iter:   40 | Time: 00:01:10 | Train Loss: 0.7646 | Average Loss: 1.0327 \n","Epoch:29 | Iter:   60 | Time: 00:01:10 | Train Loss: 0.6678 | Average Loss: 1.0293 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:30 | Iter:    0 | Time: 00:01:11 | Train Loss: 0.9093 | Average Loss: 1.0268 \n","Epoch:30 | Iter:   20 | Time: 00:01:12 | Train Loss: 0.6165 | Average Loss: 1.0234 \n","Epoch:30 | Iter:   40 | Time: 00:01:12 | Train Loss: 0.5531 | Average Loss: 1.0198 \n","Epoch:30 | Iter:   60 | Time: 00:01:13 | Train Loss: 0.6314 | Average Loss: 1.0168 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:31 | Iter:    0 | Time: 00:01:13 | Train Loss: 0.7899 | Average Loss: 1.0145 \n","Epoch:31 | Iter:   20 | Time: 00:01:14 | Train Loss: 0.8279 | Average Loss: 1.0114 \n","Epoch:31 | Iter:   40 | Time: 00:01:15 | Train Loss: 0.6365 | Average Loss: 1.0085 \n","Epoch:31 | Iter:   60 | Time: 00:01:15 | Train Loss: 0.4719 | Average Loss: 1.0053 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:32 | Iter:    0 | Time: 00:01:16 | Train Loss: 0.6930 | Average Loss: 1.0033 \n","Epoch:32 | Iter:   20 | Time: 00:01:17 | Train Loss: 0.8137 | Average Loss: 1.0002 \n","Epoch:32 | Iter:   40 | Time: 00:01:17 | Train Loss: 0.5883 | Average Loss: 0.9967 \n","Epoch:32 | Iter:   60 | Time: 00:01:18 | Train Loss: 0.5321 | Average Loss: 0.9936 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:33 | Iter:    0 | Time: 00:01:18 | Train Loss: 0.8714 | Average Loss: 0.9916 \n","Epoch:33 | Iter:   20 | Time: 00:01:19 | Train Loss: 0.6971 | Average Loss: 0.9890 \n","Epoch:33 | Iter:   40 | Time: 00:01:20 | Train Loss: 0.5815 | Average Loss: 0.9860 \n","Epoch:33 | Iter:   60 | Time: 00:01:20 | Train Loss: 0.4099 | Average Loss: 0.9832 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:34 | Iter:    0 | Time: 00:01:21 | Train Loss: 0.7995 | Average Loss: 0.9812 \n","Epoch:34 | Iter:   20 | Time: 00:01:21 | Train Loss: 0.6950 | Average Loss: 0.9781 \n","Epoch:34 | Iter:   40 | Time: 00:01:22 | Train Loss: 0.6715 | Average Loss: 0.9754 \n","Epoch:34 | Iter:   60 | Time: 00:01:23 | Train Loss: 0.5641 | Average Loss: 0.9726 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:35 | Iter:    0 | Time: 00:01:23 | Train Loss: 0.8529 | Average Loss: 0.9709 \n","Epoch:35 | Iter:   20 | Time: 00:01:24 | Train Loss: 0.6347 | Average Loss: 0.9678 \n","Epoch:35 | Iter:   40 | Time: 00:01:25 | Train Loss: 0.7456 | Average Loss: 0.9649 \n","Epoch:35 | Iter:   60 | Time: 00:01:25 | Train Loss: 0.4838 | Average Loss: 0.9620 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:36 | Iter:    0 | Time: 00:01:26 | Train Loss: 0.6827 | Average Loss: 0.9601 \n","Epoch:36 | Iter:   20 | Time: 00:01:26 | Train Loss: 0.6925 | Average Loss: 0.9574 \n","Epoch:36 | Iter:   40 | Time: 00:01:27 | Train Loss: 0.7311 | Average Loss: 0.9546 \n","Epoch:36 | Iter:   60 | Time: 00:01:28 | Train Loss: 0.6263 | Average Loss: 0.9519 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:37 | Iter:    0 | Time: 00:01:28 | Train Loss: 0.5865 | Average Loss: 0.9502 \n","Epoch:37 | Iter:   20 | Time: 00:01:29 | Train Loss: 0.5632 | Average Loss: 0.9472 \n","Epoch:37 | Iter:   40 | Time: 00:01:29 | Train Loss: 0.8084 | Average Loss: 0.9445 \n","Epoch:37 | Iter:   60 | Time: 00:01:30 | Train Loss: 0.5837 | Average Loss: 0.9420 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:38 | Iter:    0 | Time: 00:01:31 | Train Loss: 0.7395 | Average Loss: 0.9402 \n","Epoch:38 | Iter:   20 | Time: 00:01:31 | Train Loss: 0.6284 | Average Loss: 0.9377 \n","Epoch:38 | Iter:   40 | Time: 00:01:32 | Train Loss: 0.6810 | Average Loss: 0.9350 \n","Epoch:38 | Iter:   60 | Time: 00:01:33 | Train Loss: 0.6533 | Average Loss: 0.9328 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:39 | Iter:    0 | Time: 00:01:33 | Train Loss: 0.6179 | Average Loss: 0.9311 \n","Epoch:39 | Iter:   20 | Time: 00:01:34 | Train Loss: 0.5542 | Average Loss: 0.9289 \n","Epoch:39 | Iter:   40 | Time: 00:01:34 | Train Loss: 0.3820 | Average Loss: 0.9263 \n","Epoch:39 | Iter:   60 | Time: 00:01:35 | Train Loss: 0.4942 | Average Loss: 0.9241 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:40 | Iter:    0 | Time: 00:01:36 | Train Loss: 0.5200 | Average Loss: 0.9222 \n","Epoch:40 | Iter:   20 | Time: 00:01:36 | Train Loss: 0.5198 | Average Loss: 0.9199 \n","Epoch:40 | Iter:   40 | Time: 00:01:37 | Train Loss: 0.6904 | Average Loss: 0.9174 \n","Epoch:40 | Iter:   60 | Time: 00:01:38 | Train Loss: 0.5729 | Average Loss: 0.9155 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:41 | Iter:    0 | Time: 00:01:38 | Train Loss: 0.5705 | Average Loss: 0.9136 \n","Epoch:41 | Iter:   20 | Time: 00:01:39 | Train Loss: 0.6996 | Average Loss: 0.9112 \n","Epoch:41 | Iter:   40 | Time: 00:01:39 | Train Loss: 0.5734 | Average Loss: 0.9090 \n","Epoch:41 | Iter:   60 | Time: 00:01:40 | Train Loss: 0.6198 | Average Loss: 0.9068 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:42 | Iter:    0 | Time: 00:01:41 | Train Loss: 0.6647 | Average Loss: 0.9053 \n","Epoch:42 | Iter:   20 | Time: 00:01:41 | Train Loss: 0.6015 | Average Loss: 0.9031 \n","Epoch:42 | Iter:   40 | Time: 00:01:42 | Train Loss: 0.5452 | Average Loss: 0.9009 \n","Epoch:42 | Iter:   60 | Time: 00:01:42 | Train Loss: 0.5548 | Average Loss: 0.8987 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:43 | Iter:    0 | Time: 00:01:43 | Train Loss: 0.7346 | Average Loss: 0.8971 \n","Epoch:43 | Iter:   20 | Time: 00:01:44 | Train Loss: 0.5186 | Average Loss: 0.8949 \n","Epoch:43 | Iter:   40 | Time: 00:01:44 | Train Loss: 0.5020 | Average Loss: 0.8928 \n","Epoch:43 | Iter:   60 | Time: 00:01:45 | Train Loss: 0.7048 | Average Loss: 0.8907 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:44 | Iter:    0 | Time: 00:01:45 | Train Loss: 0.5619 | Average Loss: 0.8890 \n","Epoch:44 | Iter:   20 | Time: 00:01:46 | Train Loss: 0.6690 | Average Loss: 0.8868 \n","Epoch:44 | Iter:   40 | Time: 00:01:47 | Train Loss: 0.5955 | Average Loss: 0.8847 \n","Epoch:44 | Iter:   60 | Time: 00:01:47 | Train Loss: 0.5251 | Average Loss: 0.8827 \n","Accuracy: 0.651042 | Time: 00:00:00\n","Epoch:45 | Iter:    0 | Time: 00:01:48 | Train Loss: 0.4575 | Average Loss: 0.8813 \n","Epoch:45 | Iter:   20 | Time: 00:01:49 | Train Loss: 0.4596 | Average Loss: 0.8791 \n","Epoch:45 | Iter:   40 | Time: 00:01:49 | Train Loss: 0.5267 | Average Loss: 0.8770 \n","Epoch:45 | Iter:   60 | Time: 00:01:50 | Train Loss: 0.6142 | Average Loss: 0.8749 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:46 | Iter:    0 | Time: 00:01:50 | Train Loss: 0.3685 | Average Loss: 0.8735 \n","Epoch:46 | Iter:   20 | Time: 00:01:51 | Train Loss: 0.7320 | Average Loss: 0.8716 \n","Epoch:46 | Iter:   40 | Time: 00:01:52 | Train Loss: 0.5919 | Average Loss: 0.8696 \n","Epoch:46 | Iter:   60 | Time: 00:01:52 | Train Loss: 0.5622 | Average Loss: 0.8676 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:47 | Iter:    0 | Time: 00:01:53 | Train Loss: 0.5467 | Average Loss: 0.8661 \n","Epoch:47 | Iter:   20 | Time: 00:01:53 | Train Loss: 0.6567 | Average Loss: 0.8643 \n","Epoch:47 | Iter:   40 | Time: 00:01:54 | Train Loss: 0.4892 | Average Loss: 0.8623 \n","Epoch:47 | Iter:   60 | Time: 00:01:55 | Train Loss: 0.4203 | Average Loss: 0.8602 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:48 | Iter:    0 | Time: 00:01:55 | Train Loss: 0.6328 | Average Loss: 0.8587 \n","Epoch:48 | Iter:   20 | Time: 00:01:56 | Train Loss: 0.8758 | Average Loss: 0.8568 \n","Epoch:48 | Iter:   40 | Time: 00:01:57 | Train Loss: 0.5878 | Average Loss: 0.8547 \n","Epoch:48 | Iter:   60 | Time: 00:01:57 | Train Loss: 0.4938 | Average Loss: 0.8530 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:49 | Iter:    0 | Time: 00:01:58 | Train Loss: 0.7870 | Average Loss: 0.8517 \n","Epoch:49 | Iter:   20 | Time: 00:01:58 | Train Loss: 0.5775 | Average Loss: 0.8496 \n","Epoch:49 | Iter:   40 | Time: 00:01:59 | Train Loss: 0.4339 | Average Loss: 0.8477 \n","Epoch:49 | Iter:   60 | Time: 00:02:00 | Train Loss: 0.4046 | Average Loss: 0.8460 \n","Accuracy: 0.705729 | Time: 00:00:00\n","Epoch:50 | Iter:    0 | Time: 00:02:00 | Train Loss: 0.6441 | Average Loss: 0.8447 \n","Epoch:50 | Iter:   20 | Time: 00:02:01 | Train Loss: 0.4959 | Average Loss: 0.8430 \n","Epoch:50 | Iter:   40 | Time: 00:02:02 | Train Loss: 0.5224 | Average Loss: 0.8414 \n","Epoch:50 | Iter:   60 | Time: 00:02:02 | Train Loss: 0.5904 | Average Loss: 0.8396 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:51 | Iter:    0 | Time: 00:02:03 | Train Loss: 0.6963 | Average Loss: 0.8383 \n","Epoch:51 | Iter:   20 | Time: 00:02:03 | Train Loss: 0.6475 | Average Loss: 0.8365 \n","Epoch:51 | Iter:   40 | Time: 00:02:04 | Train Loss: 0.3504 | Average Loss: 0.8347 \n","Epoch:51 | Iter:   60 | Time: 00:02:05 | Train Loss: 0.5333 | Average Loss: 0.8330 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:52 | Iter:    0 | Time: 00:02:05 | Train Loss: 0.6073 | Average Loss: 0.8318 \n","Epoch:52 | Iter:   20 | Time: 00:02:06 | Train Loss: 0.5769 | Average Loss: 0.8299 \n","Epoch:52 | Iter:   40 | Time: 00:02:06 | Train Loss: 0.6952 | Average Loss: 0.8281 \n","Epoch:52 | Iter:   60 | Time: 00:02:07 | Train Loss: 0.5542 | Average Loss: 0.8264 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:53 | Iter:    0 | Time: 00:02:08 | Train Loss: 0.4346 | Average Loss: 0.8251 \n","Epoch:53 | Iter:   20 | Time: 00:02:08 | Train Loss: 0.4244 | Average Loss: 0.8233 \n","Epoch:53 | Iter:   40 | Time: 00:02:09 | Train Loss: 0.4960 | Average Loss: 0.8217 \n","Epoch:53 | Iter:   60 | Time: 00:02:10 | Train Loss: 0.4821 | Average Loss: 0.8202 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:54 | Iter:    0 | Time: 00:02:10 | Train Loss: 0.4371 | Average Loss: 0.8190 \n","Epoch:54 | Iter:   20 | Time: 00:02:11 | Train Loss: 0.5725 | Average Loss: 0.8174 \n","Epoch:54 | Iter:   40 | Time: 00:02:11 | Train Loss: 0.4535 | Average Loss: 0.8158 \n","Epoch:54 | Iter:   60 | Time: 00:02:12 | Train Loss: 0.3029 | Average Loss: 0.8141 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:55 | Iter:    0 | Time: 00:02:13 | Train Loss: 0.7826 | Average Loss: 0.8128 \n","Epoch:55 | Iter:   20 | Time: 00:02:13 | Train Loss: 0.5758 | Average Loss: 0.8113 \n","Epoch:55 | Iter:   40 | Time: 00:02:14 | Train Loss: 0.4570 | Average Loss: 0.8098 \n","Epoch:55 | Iter:   60 | Time: 00:02:14 | Train Loss: 0.4849 | Average Loss: 0.8083 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:56 | Iter:    0 | Time: 00:02:15 | Train Loss: 0.5443 | Average Loss: 0.8072 \n","Epoch:56 | Iter:   20 | Time: 00:02:16 | Train Loss: 0.6245 | Average Loss: 0.8057 \n","Epoch:56 | Iter:   40 | Time: 00:02:16 | Train Loss: 0.6664 | Average Loss: 0.8041 \n","Epoch:56 | Iter:   60 | Time: 00:02:17 | Train Loss: 0.5330 | Average Loss: 0.8027 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:57 | Iter:    0 | Time: 00:02:17 | Train Loss: 0.5679 | Average Loss: 0.8016 \n","Epoch:57 | Iter:   20 | Time: 00:02:18 | Train Loss: 0.4788 | Average Loss: 0.8000 \n","Epoch:57 | Iter:   40 | Time: 00:02:19 | Train Loss: 0.4787 | Average Loss: 0.7985 \n","Epoch:57 | Iter:   60 | Time: 00:02:19 | Train Loss: 0.5271 | Average Loss: 0.7971 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:58 | Iter:    0 | Time: 00:02:20 | Train Loss: 0.6952 | Average Loss: 0.7960 \n","Epoch:58 | Iter:   20 | Time: 00:02:21 | Train Loss: 0.5239 | Average Loss: 0.7945 \n","Epoch:58 | Iter:   40 | Time: 00:02:21 | Train Loss: 0.5669 | Average Loss: 0.7929 \n","Epoch:58 | Iter:   60 | Time: 00:02:22 | Train Loss: 0.4420 | Average Loss: 0.7914 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:59 | Iter:    0 | Time: 00:02:22 | Train Loss: 0.5809 | Average Loss: 0.7902 \n","Epoch:59 | Iter:   20 | Time: 00:02:23 | Train Loss: 0.4049 | Average Loss: 0.7886 \n","Epoch:59 | Iter:   40 | Time: 00:02:24 | Train Loss: 0.6720 | Average Loss: 0.7871 \n","Epoch:59 | Iter:   60 | Time: 00:02:24 | Train Loss: 0.4521 | Average Loss: 0.7857 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:60 | Iter:    0 | Time: 00:02:25 | Train Loss: 0.3889 | Average Loss: 0.7846 \n","Epoch:60 | Iter:   20 | Time: 00:02:26 | Train Loss: 0.6537 | Average Loss: 0.7829 \n","Epoch:60 | Iter:   40 | Time: 00:02:26 | Train Loss: 0.4686 | Average Loss: 0.7814 \n","Epoch:60 | Iter:   60 | Time: 00:02:27 | Train Loss: 0.2744 | Average Loss: 0.7799 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:61 | Iter:    0 | Time: 00:02:27 | Train Loss: 0.8213 | Average Loss: 0.7790 \n","Epoch:61 | Iter:   20 | Time: 00:02:28 | Train Loss: 0.5944 | Average Loss: 0.7773 \n","Epoch:61 | Iter:   40 | Time: 00:02:29 | Train Loss: 0.4633 | Average Loss: 0.7759 \n","Epoch:61 | Iter:   60 | Time: 00:02:29 | Train Loss: 0.4849 | Average Loss: 0.7744 \n","Accuracy: 0.708333 | Time: 00:00:00\n","Epoch:62 | Iter:    0 | Time: 00:02:30 | Train Loss: 0.6116 | Average Loss: 0.7734 \n","Epoch:62 | Iter:   20 | Time: 00:02:30 | Train Loss: 0.4572 | Average Loss: 0.7719 \n","Epoch:62 | Iter:   40 | Time: 00:02:31 | Train Loss: 0.4782 | Average Loss: 0.7706 \n","Epoch:62 | Iter:   60 | Time: 00:02:32 | Train Loss: 0.5032 | Average Loss: 0.7692 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:63 | Iter:    0 | Time: 00:02:32 | Train Loss: 0.6284 | Average Loss: 0.7683 \n","Epoch:63 | Iter:   20 | Time: 00:02:33 | Train Loss: 0.5201 | Average Loss: 0.7669 \n","Epoch:63 | Iter:   40 | Time: 00:02:34 | Train Loss: 0.3333 | Average Loss: 0.7654 \n","Epoch:63 | Iter:   60 | Time: 00:02:34 | Train Loss: 0.4810 | Average Loss: 0.7640 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:64 | Iter:    0 | Time: 00:02:35 | Train Loss: 0.2538 | Average Loss: 0.7630 \n","Epoch:64 | Iter:   20 | Time: 00:02:35 | Train Loss: 0.4061 | Average Loss: 0.7615 \n","Epoch:64 | Iter:   40 | Time: 00:02:36 | Train Loss: 0.3534 | Average Loss: 0.7600 \n","Epoch:64 | Iter:   60 | Time: 00:02:37 | Train Loss: 0.4028 | Average Loss: 0.7585 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:65 | Iter:    0 | Time: 00:02:37 | Train Loss: 0.3954 | Average Loss: 0.7576 \n","Epoch:65 | Iter:   20 | Time: 00:02:38 | Train Loss: 0.5415 | Average Loss: 0.7563 \n","Epoch:65 | Iter:   40 | Time: 00:02:38 | Train Loss: 0.6300 | Average Loss: 0.7549 \n","Epoch:65 | Iter:   60 | Time: 00:02:39 | Train Loss: 0.3981 | Average Loss: 0.7536 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:66 | Iter:    0 | Time: 00:02:40 | Train Loss: 0.4556 | Average Loss: 0.7526 \n","Epoch:66 | Iter:   20 | Time: 00:02:40 | Train Loss: 0.5249 | Average Loss: 0.7513 \n","Epoch:66 | Iter:   40 | Time: 00:02:41 | Train Loss: 0.3587 | Average Loss: 0.7499 \n","Epoch:66 | Iter:   60 | Time: 00:02:42 | Train Loss: 0.3235 | Average Loss: 0.7487 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:67 | Iter:    0 | Time: 00:02:42 | Train Loss: 0.5458 | Average Loss: 0.7477 \n","Epoch:67 | Iter:   20 | Time: 00:02:43 | Train Loss: 0.3530 | Average Loss: 0.7463 \n","Epoch:67 | Iter:   40 | Time: 00:02:43 | Train Loss: 0.3798 | Average Loss: 0.7449 \n","Epoch:67 | Iter:   60 | Time: 00:02:44 | Train Loss: 0.3446 | Average Loss: 0.7436 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:68 | Iter:    0 | Time: 00:02:45 | Train Loss: 0.4557 | Average Loss: 0.7425 \n","Epoch:68 | Iter:   20 | Time: 00:02:45 | Train Loss: 0.2817 | Average Loss: 0.7412 \n","Epoch:68 | Iter:   40 | Time: 00:02:46 | Train Loss: 0.3994 | Average Loss: 0.7398 \n","Epoch:68 | Iter:   60 | Time: 00:02:46 | Train Loss: 0.3543 | Average Loss: 0.7385 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:69 | Iter:    0 | Time: 00:02:47 | Train Loss: 0.5856 | Average Loss: 0.7376 \n","Epoch:69 | Iter:   20 | Time: 00:02:48 | Train Loss: 0.4071 | Average Loss: 0.7363 \n","Epoch:69 | Iter:   40 | Time: 00:02:48 | Train Loss: 0.7111 | Average Loss: 0.7351 \n","Epoch:69 | Iter:   60 | Time: 00:02:49 | Train Loss: 0.2877 | Average Loss: 0.7339 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:70 | Iter:    0 | Time: 00:02:49 | Train Loss: 0.3573 | Average Loss: 0.7329 \n","Epoch:70 | Iter:   20 | Time: 00:02:50 | Train Loss: 0.4156 | Average Loss: 0.7317 \n","Epoch:70 | Iter:   40 | Time: 00:02:51 | Train Loss: 0.5044 | Average Loss: 0.7305 \n","Epoch:70 | Iter:   60 | Time: 00:02:51 | Train Loss: 0.5372 | Average Loss: 0.7295 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:71 | Iter:    0 | Time: 00:02:52 | Train Loss: 0.3016 | Average Loss: 0.7287 \n","Epoch:71 | Iter:   20 | Time: 00:02:53 | Train Loss: 0.5017 | Average Loss: 0.7272 \n","Epoch:71 | Iter:   40 | Time: 00:02:53 | Train Loss: 0.4205 | Average Loss: 0.7260 \n","Epoch:71 | Iter:   60 | Time: 00:02:54 | Train Loss: 0.3514 | Average Loss: 0.7248 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:72 | Iter:    0 | Time: 00:02:54 | Train Loss: 0.4622 | Average Loss: 0.7240 \n","Epoch:72 | Iter:   20 | Time: 00:02:55 | Train Loss: 0.4216 | Average Loss: 0.7227 \n","Epoch:72 | Iter:   40 | Time: 00:02:56 | Train Loss: 0.3701 | Average Loss: 0.7215 \n","Epoch:72 | Iter:   60 | Time: 00:02:56 | Train Loss: 0.3486 | Average Loss: 0.7203 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:73 | Iter:    0 | Time: 00:02:57 | Train Loss: 0.5554 | Average Loss: 0.7194 \n","Epoch:73 | Iter:   20 | Time: 00:02:58 | Train Loss: 0.4998 | Average Loss: 0.7182 \n","Epoch:73 | Iter:   40 | Time: 00:02:58 | Train Loss: 0.5653 | Average Loss: 0.7172 \n","Epoch:73 | Iter:   60 | Time: 00:02:59 | Train Loss: 0.4740 | Average Loss: 0.7161 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:74 | Iter:    0 | Time: 00:02:59 | Train Loss: 0.3378 | Average Loss: 0.7154 \n","Epoch:74 | Iter:   20 | Time: 00:03:00 | Train Loss: 0.3721 | Average Loss: 0.7142 \n","Epoch:74 | Iter:   40 | Time: 00:03:01 | Train Loss: 0.4281 | Average Loss: 0.7130 \n","Epoch:74 | Iter:   60 | Time: 00:03:01 | Train Loss: 0.5745 | Average Loss: 0.7119 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:75 | Iter:    0 | Time: 00:03:02 | Train Loss: 0.8170 | Average Loss: 0.7112 \n","Epoch:75 | Iter:   20 | Time: 00:03:02 | Train Loss: 0.3343 | Average Loss: 0.7100 \n","Epoch:75 | Iter:   40 | Time: 00:03:03 | Train Loss: 0.4276 | Average Loss: 0.7089 \n","Epoch:75 | Iter:   60 | Time: 00:03:04 | Train Loss: 0.3635 | Average Loss: 0.7078 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:76 | Iter:    0 | Time: 00:03:04 | Train Loss: 1.0529 | Average Loss: 0.7072 \n","Epoch:76 | Iter:   20 | Time: 00:03:05 | Train Loss: 0.2675 | Average Loss: 0.7061 \n","Epoch:76 | Iter:   40 | Time: 00:03:06 | Train Loss: 0.4573 | Average Loss: 0.7050 \n","Epoch:76 | Iter:   60 | Time: 00:03:06 | Train Loss: 0.3566 | Average Loss: 0.7039 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:77 | Iter:    0 | Time: 00:03:07 | Train Loss: 0.6213 | Average Loss: 0.7031 \n","Epoch:77 | Iter:   20 | Time: 00:03:07 | Train Loss: 0.3694 | Average Loss: 0.7020 \n","Epoch:77 | Iter:   40 | Time: 00:03:08 | Train Loss: 0.4939 | Average Loss: 0.7009 \n","Epoch:77 | Iter:   60 | Time: 00:03:09 | Train Loss: 0.3870 | Average Loss: 0.6997 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:78 | Iter:    0 | Time: 00:03:09 | Train Loss: 0.3586 | Average Loss: 0.6990 \n","Epoch:78 | Iter:   20 | Time: 00:03:10 | Train Loss: 0.3932 | Average Loss: 0.6977 \n","Epoch:78 | Iter:   40 | Time: 00:03:10 | Train Loss: 0.3095 | Average Loss: 0.6966 \n","Epoch:78 | Iter:   60 | Time: 00:03:11 | Train Loss: 0.3171 | Average Loss: 0.6955 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:79 | Iter:    0 | Time: 00:03:12 | Train Loss: 0.4534 | Average Loss: 0.6947 \n","Epoch:79 | Iter:   20 | Time: 00:03:12 | Train Loss: 0.2405 | Average Loss: 0.6936 \n","Epoch:79 | Iter:   40 | Time: 00:03:13 | Train Loss: 0.3501 | Average Loss: 0.6926 \n","Epoch:79 | Iter:   60 | Time: 00:03:14 | Train Loss: 0.4712 | Average Loss: 0.6916 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:80 | Iter:    0 | Time: 00:03:14 | Train Loss: 0.4936 | Average Loss: 0.6910 \n","Epoch:80 | Iter:   20 | Time: 00:03:15 | Train Loss: 0.4865 | Average Loss: 0.6900 \n","Epoch:80 | Iter:   40 | Time: 00:03:15 | Train Loss: 0.2438 | Average Loss: 0.6890 \n","Epoch:80 | Iter:   60 | Time: 00:03:16 | Train Loss: 0.3436 | Average Loss: 0.6879 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:81 | Iter:    0 | Time: 00:03:17 | Train Loss: 0.3530 | Average Loss: 0.6871 \n","Epoch:81 | Iter:   20 | Time: 00:03:17 | Train Loss: 0.2928 | Average Loss: 0.6860 \n","Epoch:81 | Iter:   40 | Time: 00:03:18 | Train Loss: 0.4551 | Average Loss: 0.6849 \n","Epoch:81 | Iter:   60 | Time: 00:03:19 | Train Loss: 0.4020 | Average Loss: 0.6840 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:82 | Iter:    0 | Time: 00:03:19 | Train Loss: 0.2244 | Average Loss: 0.6832 \n","Epoch:82 | Iter:   20 | Time: 00:03:20 | Train Loss: 0.3801 | Average Loss: 0.6822 \n","Epoch:82 | Iter:   40 | Time: 00:03:20 | Train Loss: 0.4845 | Average Loss: 0.6812 \n","Epoch:82 | Iter:   60 | Time: 00:03:21 | Train Loss: 0.3693 | Average Loss: 0.6804 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:83 | Iter:    0 | Time: 00:03:22 | Train Loss: 0.4487 | Average Loss: 0.6797 \n","Epoch:83 | Iter:   20 | Time: 00:03:22 | Train Loss: 0.2994 | Average Loss: 0.6787 \n","Epoch:83 | Iter:   40 | Time: 00:03:23 | Train Loss: 0.4267 | Average Loss: 0.6777 \n","Epoch:83 | Iter:   60 | Time: 00:03:23 | Train Loss: 0.3421 | Average Loss: 0.6767 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:84 | Iter:    0 | Time: 00:03:24 | Train Loss: 0.5737 | Average Loss: 0.6759 \n","Epoch:84 | Iter:   20 | Time: 00:03:25 | Train Loss: 0.2956 | Average Loss: 0.6750 \n","Epoch:84 | Iter:   40 | Time: 00:03:25 | Train Loss: 0.4768 | Average Loss: 0.6740 \n","Epoch:84 | Iter:   60 | Time: 00:03:26 | Train Loss: 0.4098 | Average Loss: 0.6731 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:85 | Iter:    0 | Time: 00:03:26 | Train Loss: 0.5291 | Average Loss: 0.6724 \n","Epoch:85 | Iter:   20 | Time: 00:03:27 | Train Loss: 0.3737 | Average Loss: 0.6714 \n","Epoch:85 | Iter:   40 | Time: 00:03:28 | Train Loss: 0.3651 | Average Loss: 0.6704 \n","Epoch:85 | Iter:   60 | Time: 00:03:28 | Train Loss: 0.2739 | Average Loss: 0.6695 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:86 | Iter:    0 | Time: 00:03:29 | Train Loss: 0.6312 | Average Loss: 0.6689 \n","Epoch:86 | Iter:   20 | Time: 00:03:30 | Train Loss: 0.2863 | Average Loss: 0.6679 \n","Epoch:86 | Iter:   40 | Time: 00:03:30 | Train Loss: 0.3366 | Average Loss: 0.6669 \n","Epoch:86 | Iter:   60 | Time: 00:03:31 | Train Loss: 0.3258 | Average Loss: 0.6659 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:87 | Iter:    0 | Time: 00:03:31 | Train Loss: 0.2915 | Average Loss: 0.6652 \n","Epoch:87 | Iter:   20 | Time: 00:03:32 | Train Loss: 0.2197 | Average Loss: 0.6642 \n","Epoch:87 | Iter:   40 | Time: 00:03:33 | Train Loss: 0.3579 | Average Loss: 0.6632 \n","Epoch:87 | Iter:   60 | Time: 00:03:33 | Train Loss: 0.3073 | Average Loss: 0.6622 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:88 | Iter:    0 | Time: 00:03:34 | Train Loss: 0.5520 | Average Loss: 0.6616 \n","Epoch:88 | Iter:   20 | Time: 00:03:34 | Train Loss: 0.4094 | Average Loss: 0.6606 \n","Epoch:88 | Iter:   40 | Time: 00:03:35 | Train Loss: 0.3666 | Average Loss: 0.6598 \n","Epoch:88 | Iter:   60 | Time: 00:03:36 | Train Loss: 0.3453 | Average Loss: 0.6589 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:89 | Iter:    0 | Time: 00:03:36 | Train Loss: 0.2962 | Average Loss: 0.6582 \n","Epoch:89 | Iter:   20 | Time: 00:03:37 | Train Loss: 0.3916 | Average Loss: 0.6572 \n","Epoch:89 | Iter:   40 | Time: 00:03:38 | Train Loss: 0.4553 | Average Loss: 0.6562 \n","Epoch:89 | Iter:   60 | Time: 00:03:38 | Train Loss: 0.2573 | Average Loss: 0.6553 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:90 | Iter:    0 | Time: 00:03:39 | Train Loss: 0.4614 | Average Loss: 0.6546 \n","Epoch:90 | Iter:   20 | Time: 00:03:39 | Train Loss: 0.2841 | Average Loss: 0.6536 \n","Epoch:90 | Iter:   40 | Time: 00:03:40 | Train Loss: 0.4164 | Average Loss: 0.6528 \n","Epoch:90 | Iter:   60 | Time: 00:03:41 | Train Loss: 0.4312 | Average Loss: 0.6519 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:91 | Iter:    0 | Time: 00:03:41 | Train Loss: 0.4798 | Average Loss: 0.6512 \n","Epoch:91 | Iter:   20 | Time: 00:03:42 | Train Loss: 0.3054 | Average Loss: 0.6504 \n","Epoch:91 | Iter:   40 | Time: 00:03:42 | Train Loss: 0.4870 | Average Loss: 0.6495 \n","Epoch:91 | Iter:   60 | Time: 00:03:43 | Train Loss: 0.4291 | Average Loss: 0.6486 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:92 | Iter:    0 | Time: 00:03:44 | Train Loss: 0.2106 | Average Loss: 0.6480 \n","Epoch:92 | Iter:   20 | Time: 00:03:44 | Train Loss: 0.4232 | Average Loss: 0.6470 \n","Epoch:92 | Iter:   40 | Time: 00:03:45 | Train Loss: 0.5797 | Average Loss: 0.6461 \n","Epoch:92 | Iter:   60 | Time: 00:03:46 | Train Loss: 0.3765 | Average Loss: 0.6454 \n","Accuracy: 0.705729 | Time: 00:00:00\n","Epoch:93 | Iter:    0 | Time: 00:03:46 | Train Loss: 0.2411 | Average Loss: 0.6447 \n","Epoch:93 | Iter:   20 | Time: 00:03:47 | Train Loss: 0.2827 | Average Loss: 0.6438 \n","Epoch:93 | Iter:   40 | Time: 00:03:47 | Train Loss: 0.3880 | Average Loss: 0.6429 \n","Epoch:93 | Iter:   60 | Time: 00:03:48 | Train Loss: 0.3929 | Average Loss: 0.6421 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:94 | Iter:    0 | Time: 00:03:49 | Train Loss: 0.3189 | Average Loss: 0.6415 \n","Epoch:94 | Iter:   20 | Time: 00:03:49 | Train Loss: 0.3587 | Average Loss: 0.6405 \n","Epoch:94 | Iter:   40 | Time: 00:03:50 | Train Loss: 0.5006 | Average Loss: 0.6397 \n","Epoch:94 | Iter:   60 | Time: 00:03:51 | Train Loss: 0.1973 | Average Loss: 0.6389 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:95 | Iter:    0 | Time: 00:03:51 | Train Loss: 0.6445 | Average Loss: 0.6383 \n","Epoch:95 | Iter:   20 | Time: 00:03:52 | Train Loss: 0.3132 | Average Loss: 0.6374 \n","Epoch:95 | Iter:   40 | Time: 00:03:52 | Train Loss: 0.2504 | Average Loss: 0.6365 \n","Epoch:95 | Iter:   60 | Time: 00:03:53 | Train Loss: 0.4158 | Average Loss: 0.6357 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:96 | Iter:    0 | Time: 00:03:54 | Train Loss: 0.3316 | Average Loss: 0.6351 \n","Epoch:96 | Iter:   20 | Time: 00:03:54 | Train Loss: 0.3524 | Average Loss: 0.6343 \n","Epoch:96 | Iter:   40 | Time: 00:03:55 | Train Loss: 0.3864 | Average Loss: 0.6334 \n","Epoch:96 | Iter:   60 | Time: 00:03:55 | Train Loss: 0.3949 | Average Loss: 0.6327 \n","Accuracy: 0.708333 | Time: 00:00:00\n","Epoch:97 | Iter:    0 | Time: 00:03:56 | Train Loss: 0.3247 | Average Loss: 0.6320 \n","Epoch:97 | Iter:   20 | Time: 00:03:57 | Train Loss: 0.3802 | Average Loss: 0.6311 \n","Epoch:97 | Iter:   40 | Time: 00:03:57 | Train Loss: 0.3559 | Average Loss: 0.6303 \n","Epoch:97 | Iter:   60 | Time: 00:03:58 | Train Loss: 0.2596 | Average Loss: 0.6295 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:98 | Iter:    0 | Time: 00:03:58 | Train Loss: 0.2132 | Average Loss: 0.6288 \n","Epoch:98 | Iter:   20 | Time: 00:03:59 | Train Loss: 0.3011 | Average Loss: 0.6280 \n","Epoch:98 | Iter:   40 | Time: 00:04:00 | Train Loss: 0.3420 | Average Loss: 0.6272 \n","Epoch:98 | Iter:   60 | Time: 00:04:00 | Train Loss: 0.5333 | Average Loss: 0.6264 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:99 | Iter:    0 | Time: 00:04:01 | Train Loss: 0.7552 | Average Loss: 0.6259 \n","Epoch:99 | Iter:   20 | Time: 00:04:02 | Train Loss: 0.5502 | Average Loss: 0.6252 \n","Epoch:99 | Iter:   40 | Time: 00:04:02 | Train Loss: 0.3696 | Average Loss: 0.6244 \n","Epoch:99 | Iter:   60 | Time: 00:04:03 | Train Loss: 0.3281 | Average Loss: 0.6235 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:100 | Iter:    0 | Time: 00:04:03 | Train Loss: 0.3336 | Average Loss: 0.6229 \n","Epoch:100 | Iter:   20 | Time: 00:04:04 | Train Loss: 0.4547 | Average Loss: 0.6222 \n","Epoch:100 | Iter:   40 | Time: 00:04:05 | Train Loss: 0.3195 | Average Loss: 0.6213 \n","Epoch:100 | Iter:   60 | Time: 00:04:05 | Train Loss: 0.2737 | Average Loss: 0.6204 \n","Accuracy: 0.700521 | Time: 00:00:00\n"]}],"source":["#--------------------------------------------------\n","#       Start Training & Evaluation\n","#--------------------------------------------------\n","net = ModelArchitectureModification()\n","train_option = {}\n","train_option['lr'] = 0.002\n","train_option['epoch'] = 100\n","train_option['device'] = 'gpu'\n","trainModel(net, trainloader_small, train_option, testloader_small)\n"]},{"cell_type":"markdown","metadata":{"id":"IkePS4VnHnZO"},"source":["### Technique 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgzhiz12fXjj","executionInfo":{"status":"ok","timestamp":1650857301855,"user_tz":240,"elapsed":14254,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"dee7bd6f-1ed3-429e-ab4f-957da5a9a6a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading images from class: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 150 minibatches (batch_size=32) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 12 minibatches (batch_size=32) of testing samples.\n"]}],"source":["#--------------------------------------------------\n","#    Load Training Data and Testing Data\n","#--------------------------------------------------\n","img_size = (64, 64)\n","batch_size = 32\n","\n","# load training dataset\n","trainloader_small = list(load_dataset('./data/train/', img_size, batch_size=batch_size, shuffle=True, augment=True, zero_centered=True))\n","train_num = len(trainloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n","\n","# load testing dataset\n","testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size, zero_centered=True))\n","test_num = len(testloader_small)\n","print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5ozpyI0T8aE"},"outputs":[],"source":["#--------------------------------------------------\n","#       Define Network Architecture\n","#--------------------------------------------------\n","class BatchNormalization(nn.Module):\n","  def __init__(self):\n","    super(BatchNormalization,self).__init__()\n","    self.features = torch.nn.Sequential(\n","        nn.Conv2d(1, 32, 3),\n","        nn.BatchNorm2d(32),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4),\n","        nn.Dropout(0.5),\n","        nn.Conv2d(32, 256, 3),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(),\n","        nn.MaxPool2d(4, stride=4),\n","        nn.Dropout(0.5)\n","    )\n","\n","    self.classifier = nn.Sequential(\n","        nn.Linear(2304, 16),\n","    )\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = torch.flatten(x, 1)\n","    x = self.classifier(x)\n","    return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdy9SRdFsyWZ","executionInfo":{"status":"ok","timestamp":1650853193494,"user_tz":240,"elapsed":133238,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"3f4b1b25-b099-402c-ce0a-9b8e389f4526"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 3.5393 | Average Loss: 3.5393 \n","Epoch: 1 | Iter:   20 | Time: 00:00:00 | Train Loss: 2.6974 | Average Loss: 4.3868 \n","Epoch: 1 | Iter:   40 | Time: 00:00:00 | Train Loss: 2.8273 | Average Loss: 3.6317 \n","Epoch: 1 | Iter:   60 | Time: 00:00:01 | Train Loss: 2.2673 | Average Loss: 3.2826 \n","Accuracy: 0.325521 | Time: 00:00:00\n","Epoch: 2 | Iter:    0 | Time: 00:00:01 | Train Loss: 2.5276 | Average Loss: 3.1100 \n","Epoch: 2 | Iter:   20 | Time: 00:00:01 | Train Loss: 2.0352 | Average Loss: 2.9312 \n","Epoch: 2 | Iter:   40 | Time: 00:00:02 | Train Loss: 2.1227 | Average Loss: 2.7855 \n","Epoch: 2 | Iter:   60 | Time: 00:00:02 | Train Loss: 1.7911 | Average Loss: 2.6948 \n","Accuracy: 0.414062 | Time: 00:00:00\n","Epoch: 3 | Iter:    0 | Time: 00:00:02 | Train Loss: 1.9217 | Average Loss: 2.6222 \n","Epoch: 3 | Iter:   20 | Time: 00:00:03 | Train Loss: 1.8461 | Average Loss: 2.5256 \n","Epoch: 3 | Iter:   40 | Time: 00:00:03 | Train Loss: 2.0004 | Average Loss: 2.4458 \n","Epoch: 3 | Iter:   60 | Time: 00:00:03 | Train Loss: 1.3089 | Average Loss: 2.3851 \n","Accuracy: 0.526042 | Time: 00:00:00\n","Epoch: 4 | Iter:    0 | Time: 00:00:04 | Train Loss: 1.7016 | Average Loss: 2.3387 \n","Epoch: 4 | Iter:   20 | Time: 00:00:04 | Train Loss: 1.4557 | Average Loss: 2.2730 \n","Epoch: 4 | Iter:   40 | Time: 00:00:04 | Train Loss: 1.5096 | Average Loss: 2.2182 \n","Epoch: 4 | Iter:   60 | Time: 00:00:05 | Train Loss: 1.5063 | Average Loss: 2.1738 \n","Accuracy: 0.526042 | Time: 00:00:00\n","Epoch: 5 | Iter:    0 | Time: 00:00:05 | Train Loss: 1.6523 | Average Loss: 2.1438 \n","Epoch: 5 | Iter:   20 | Time: 00:00:05 | Train Loss: 1.4401 | Average Loss: 2.1000 \n","Epoch: 5 | Iter:   40 | Time: 00:00:06 | Train Loss: 1.5868 | Average Loss: 2.0627 \n","Epoch: 5 | Iter:   60 | Time: 00:00:06 | Train Loss: 1.3907 | Average Loss: 2.0300 \n","Accuracy: 0.562500 | Time: 00:00:00\n","Epoch: 6 | Iter:    0 | Time: 00:00:06 | Train Loss: 1.4642 | Average Loss: 2.0062 \n","Epoch: 6 | Iter:   20 | Time: 00:00:07 | Train Loss: 1.4781 | Average Loss: 1.9744 \n","Epoch: 6 | Iter:   40 | Time: 00:00:07 | Train Loss: 1.4142 | Average Loss: 1.9429 \n","Epoch: 6 | Iter:   60 | Time: 00:00:07 | Train Loss: 1.1252 | Average Loss: 1.9182 \n","Accuracy: 0.557292 | Time: 00:00:00\n","Epoch: 7 | Iter:    0 | Time: 00:00:08 | Train Loss: 1.1637 | Average Loss: 1.8987 \n","Epoch: 7 | Iter:   20 | Time: 00:00:08 | Train Loss: 1.4296 | Average Loss: 1.8740 \n","Epoch: 7 | Iter:   40 | Time: 00:00:08 | Train Loss: 1.3297 | Average Loss: 1.8513 \n","Epoch: 7 | Iter:   60 | Time: 00:00:09 | Train Loss: 1.1639 | Average Loss: 1.8306 \n","Accuracy: 0.598958 | Time: 00:00:00\n","Epoch: 8 | Iter:    0 | Time: 00:00:09 | Train Loss: 1.2423 | Average Loss: 1.8141 \n","Epoch: 8 | Iter:   20 | Time: 00:00:09 | Train Loss: 1.0401 | Average Loss: 1.7942 \n","Epoch: 8 | Iter:   40 | Time: 00:00:10 | Train Loss: 1.4422 | Average Loss: 1.7744 \n","Epoch: 8 | Iter:   60 | Time: 00:00:10 | Train Loss: 1.1041 | Average Loss: 1.7565 \n","Accuracy: 0.598958 | Time: 00:00:00\n","Epoch: 9 | Iter:    0 | Time: 00:00:10 | Train Loss: 1.2609 | Average Loss: 1.7435 \n","Epoch: 9 | Iter:   20 | Time: 00:00:11 | Train Loss: 1.2238 | Average Loss: 1.7259 \n","Epoch: 9 | Iter:   40 | Time: 00:00:11 | Train Loss: 1.2024 | Average Loss: 1.7097 \n","Epoch: 9 | Iter:   60 | Time: 00:00:11 | Train Loss: 1.0834 | Average Loss: 1.6948 \n","Accuracy: 0.598958 | Time: 00:00:00\n","Epoch:10 | Iter:    0 | Time: 00:00:12 | Train Loss: 1.5094 | Average Loss: 1.6842 \n","Epoch:10 | Iter:   20 | Time: 00:00:12 | Train Loss: 1.2680 | Average Loss: 1.6694 \n","Epoch:10 | Iter:   40 | Time: 00:00:12 | Train Loss: 1.0029 | Average Loss: 1.6547 \n","Epoch:10 | Iter:   60 | Time: 00:00:13 | Train Loss: 1.0341 | Average Loss: 1.6413 \n","Accuracy: 0.611979 | Time: 00:00:00\n","Epoch:11 | Iter:    0 | Time: 00:00:13 | Train Loss: 1.3410 | Average Loss: 1.6314 \n","Epoch:11 | Iter:   20 | Time: 00:00:13 | Train Loss: 1.0860 | Average Loss: 1.6187 \n","Epoch:11 | Iter:   40 | Time: 00:00:14 | Train Loss: 1.2033 | Average Loss: 1.6052 \n","Epoch:11 | Iter:   60 | Time: 00:00:14 | Train Loss: 1.0032 | Average Loss: 1.5927 \n","Accuracy: 0.651042 | Time: 00:00:00\n","Epoch:12 | Iter:    0 | Time: 00:00:14 | Train Loss: 1.2782 | Average Loss: 1.5840 \n","Epoch:12 | Iter:   20 | Time: 00:00:15 | Train Loss: 1.0353 | Average Loss: 1.5726 \n","Epoch:12 | Iter:   40 | Time: 00:00:15 | Train Loss: 1.0521 | Average Loss: 1.5598 \n","Epoch:12 | Iter:   60 | Time: 00:00:15 | Train Loss: 0.8216 | Average Loss: 1.5483 \n","Accuracy: 0.632812 | Time: 00:00:00\n","Epoch:13 | Iter:    0 | Time: 00:00:16 | Train Loss: 1.1330 | Average Loss: 1.5395 \n","Epoch:13 | Iter:   20 | Time: 00:00:16 | Train Loss: 0.9962 | Average Loss: 1.5281 \n","Epoch:13 | Iter:   40 | Time: 00:00:16 | Train Loss: 1.2009 | Average Loss: 1.5173 \n","Epoch:13 | Iter:   60 | Time: 00:00:17 | Train Loss: 0.9699 | Average Loss: 1.5064 \n","Accuracy: 0.640625 | Time: 00:00:00\n","Epoch:14 | Iter:    0 | Time: 00:00:17 | Train Loss: 1.0042 | Average Loss: 1.4993 \n","Epoch:14 | Iter:   20 | Time: 00:00:17 | Train Loss: 0.9649 | Average Loss: 1.4895 \n","Epoch:14 | Iter:   40 | Time: 00:00:18 | Train Loss: 1.0862 | Average Loss: 1.4795 \n","Epoch:14 | Iter:   60 | Time: 00:00:18 | Train Loss: 0.8701 | Average Loss: 1.4703 \n","Accuracy: 0.627604 | Time: 00:00:00\n","Epoch:15 | Iter:    0 | Time: 00:00:18 | Train Loss: 1.1252 | Average Loss: 1.4630 \n","Epoch:15 | Iter:   20 | Time: 00:00:19 | Train Loss: 1.0059 | Average Loss: 1.4541 \n","Epoch:15 | Iter:   40 | Time: 00:00:19 | Train Loss: 1.0787 | Average Loss: 1.4448 \n","Epoch:15 | Iter:   60 | Time: 00:00:19 | Train Loss: 1.0406 | Average Loss: 1.4358 \n","Accuracy: 0.643229 | Time: 00:00:00\n","Epoch:16 | Iter:    0 | Time: 00:00:20 | Train Loss: 1.2224 | Average Loss: 1.4295 \n","Epoch:16 | Iter:   20 | Time: 00:00:20 | Train Loss: 1.0302 | Average Loss: 1.4208 \n","Epoch:16 | Iter:   40 | Time: 00:00:20 | Train Loss: 1.0725 | Average Loss: 1.4126 \n","Epoch:16 | Iter:   60 | Time: 00:00:21 | Train Loss: 0.8270 | Average Loss: 1.4050 \n","Accuracy: 0.625000 | Time: 00:00:00\n","Epoch:17 | Iter:    0 | Time: 00:00:21 | Train Loss: 1.1166 | Average Loss: 1.3999 \n","Epoch:17 | Iter:   20 | Time: 00:00:21 | Train Loss: 1.0163 | Average Loss: 1.3926 \n","Epoch:17 | Iter:   40 | Time: 00:00:22 | Train Loss: 0.9380 | Average Loss: 1.3851 \n","Epoch:17 | Iter:   60 | Time: 00:00:22 | Train Loss: 0.9024 | Average Loss: 1.3781 \n","Accuracy: 0.640625 | Time: 00:00:00\n","Epoch:18 | Iter:    0 | Time: 00:00:22 | Train Loss: 1.0326 | Average Loss: 1.3735 \n","Epoch:18 | Iter:   20 | Time: 00:00:23 | Train Loss: 0.9516 | Average Loss: 1.3656 \n","Epoch:18 | Iter:   40 | Time: 00:00:23 | Train Loss: 0.9264 | Average Loss: 1.3585 \n","Epoch:18 | Iter:   60 | Time: 00:00:23 | Train Loss: 0.7615 | Average Loss: 1.3522 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:19 | Iter:    0 | Time: 00:00:24 | Train Loss: 0.8412 | Average Loss: 1.3471 \n","Epoch:19 | Iter:   20 | Time: 00:00:24 | Train Loss: 0.8666 | Average Loss: 1.3402 \n","Epoch:19 | Iter:   40 | Time: 00:00:24 | Train Loss: 0.9053 | Average Loss: 1.3336 \n","Epoch:19 | Iter:   60 | Time: 00:00:25 | Train Loss: 0.8583 | Average Loss: 1.3274 \n","Accuracy: 0.651042 | Time: 00:00:00\n","Epoch:20 | Iter:    0 | Time: 00:00:25 | Train Loss: 1.0553 | Average Loss: 1.3232 \n","Epoch:20 | Iter:   20 | Time: 00:00:25 | Train Loss: 0.8204 | Average Loss: 1.3169 \n","Epoch:20 | Iter:   40 | Time: 00:00:26 | Train Loss: 0.9581 | Average Loss: 1.3109 \n","Epoch:20 | Iter:   60 | Time: 00:00:26 | Train Loss: 0.6837 | Average Loss: 1.3048 \n","Accuracy: 0.643229 | Time: 00:00:00\n","Epoch:21 | Iter:    0 | Time: 00:00:26 | Train Loss: 1.0518 | Average Loss: 1.3007 \n","Epoch:21 | Iter:   20 | Time: 00:00:27 | Train Loss: 0.8018 | Average Loss: 1.2946 \n","Epoch:21 | Iter:   40 | Time: 00:00:27 | Train Loss: 1.1045 | Average Loss: 1.2885 \n","Epoch:21 | Iter:   60 | Time: 00:00:27 | Train Loss: 0.8326 | Average Loss: 1.2826 \n","Accuracy: 0.627604 | Time: 00:00:00\n","Epoch:22 | Iter:    0 | Time: 00:00:28 | Train Loss: 0.9530 | Average Loss: 1.2784 \n","Epoch:22 | Iter:   20 | Time: 00:00:28 | Train Loss: 0.8972 | Average Loss: 1.2726 \n","Epoch:22 | Iter:   40 | Time: 00:00:28 | Train Loss: 0.9253 | Average Loss: 1.2665 \n","Epoch:22 | Iter:   60 | Time: 00:00:29 | Train Loss: 0.6777 | Average Loss: 1.2617 \n","Accuracy: 0.645833 | Time: 00:00:00\n","Epoch:23 | Iter:    0 | Time: 00:00:29 | Train Loss: 0.9117 | Average Loss: 1.2581 \n","Epoch:23 | Iter:   20 | Time: 00:00:29 | Train Loss: 0.8594 | Average Loss: 1.2527 \n","Epoch:23 | Iter:   40 | Time: 00:00:30 | Train Loss: 0.8932 | Average Loss: 1.2472 \n","Epoch:23 | Iter:   60 | Time: 00:00:30 | Train Loss: 0.7778 | Average Loss: 1.2424 \n","Accuracy: 0.640625 | Time: 00:00:00\n","Epoch:24 | Iter:    0 | Time: 00:00:30 | Train Loss: 0.8634 | Average Loss: 1.2387 \n","Epoch:24 | Iter:   20 | Time: 00:00:31 | Train Loss: 0.7745 | Average Loss: 1.2337 \n","Epoch:24 | Iter:   40 | Time: 00:00:31 | Train Loss: 0.9788 | Average Loss: 1.2288 \n","Epoch:24 | Iter:   60 | Time: 00:00:31 | Train Loss: 0.6985 | Average Loss: 1.2245 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:25 | Iter:    0 | Time: 00:00:32 | Train Loss: 0.9480 | Average Loss: 1.2211 \n","Epoch:25 | Iter:   20 | Time: 00:00:32 | Train Loss: 0.8380 | Average Loss: 1.2159 \n","Epoch:25 | Iter:   40 | Time: 00:00:32 | Train Loss: 0.8375 | Average Loss: 1.2113 \n","Epoch:25 | Iter:   60 | Time: 00:00:33 | Train Loss: 0.7125 | Average Loss: 1.2068 \n","Accuracy: 0.638021 | Time: 00:00:00\n","Epoch:26 | Iter:    0 | Time: 00:00:33 | Train Loss: 0.7856 | Average Loss: 1.2038 \n","Epoch:26 | Iter:   20 | Time: 00:00:33 | Train Loss: 0.7714 | Average Loss: 1.1992 \n","Epoch:26 | Iter:   40 | Time: 00:00:34 | Train Loss: 0.9302 | Average Loss: 1.1948 \n","Epoch:26 | Iter:   60 | Time: 00:00:34 | Train Loss: 0.8007 | Average Loss: 1.1911 \n","Accuracy: 0.635417 | Time: 00:00:00\n","Epoch:27 | Iter:    0 | Time: 00:00:34 | Train Loss: 0.7733 | Average Loss: 1.1884 \n","Epoch:27 | Iter:   20 | Time: 00:00:35 | Train Loss: 0.8566 | Average Loss: 1.1838 \n","Epoch:27 | Iter:   40 | Time: 00:00:35 | Train Loss: 0.8067 | Average Loss: 1.1796 \n","Epoch:27 | Iter:   60 | Time: 00:00:35 | Train Loss: 0.7142 | Average Loss: 1.1754 \n","Accuracy: 0.658854 | Time: 00:00:00\n","Epoch:28 | Iter:    0 | Time: 00:00:35 | Train Loss: 0.8914 | Average Loss: 1.1726 \n","Epoch:28 | Iter:   20 | Time: 00:00:36 | Train Loss: 0.7320 | Average Loss: 1.1687 \n","Epoch:28 | Iter:   40 | Time: 00:00:36 | Train Loss: 0.9420 | Average Loss: 1.1644 \n","Epoch:28 | Iter:   60 | Time: 00:00:37 | Train Loss: 0.6685 | Average Loss: 1.1605 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:29 | Iter:    0 | Time: 00:00:37 | Train Loss: 0.6837 | Average Loss: 1.1578 \n","Epoch:29 | Iter:   20 | Time: 00:00:37 | Train Loss: 0.6815 | Average Loss: 1.1536 \n","Epoch:29 | Iter:   40 | Time: 00:00:38 | Train Loss: 0.9568 | Average Loss: 1.1494 \n","Epoch:29 | Iter:   60 | Time: 00:00:38 | Train Loss: 0.6435 | Average Loss: 1.1457 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:30 | Iter:    0 | Time: 00:00:38 | Train Loss: 0.6620 | Average Loss: 1.1429 \n","Epoch:30 | Iter:   20 | Time: 00:00:38 | Train Loss: 0.9175 | Average Loss: 1.1389 \n","Epoch:30 | Iter:   40 | Time: 00:00:39 | Train Loss: 0.8417 | Average Loss: 1.1352 \n","Epoch:30 | Iter:   60 | Time: 00:00:39 | Train Loss: 0.6434 | Average Loss: 1.1317 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:31 | Iter:    0 | Time: 00:00:39 | Train Loss: 0.7927 | Average Loss: 1.1290 \n","Epoch:31 | Iter:   20 | Time: 00:00:40 | Train Loss: 0.7169 | Average Loss: 1.1254 \n","Epoch:31 | Iter:   40 | Time: 00:00:40 | Train Loss: 0.8068 | Average Loss: 1.1216 \n","Epoch:31 | Iter:   60 | Time: 00:00:41 | Train Loss: 0.6402 | Average Loss: 1.1180 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:32 | Iter:    0 | Time: 00:00:41 | Train Loss: 0.8350 | Average Loss: 1.1158 \n","Epoch:32 | Iter:   20 | Time: 00:00:41 | Train Loss: 0.7589 | Average Loss: 1.1122 \n","Epoch:32 | Iter:   40 | Time: 00:00:41 | Train Loss: 0.7350 | Average Loss: 1.1086 \n","Epoch:32 | Iter:   60 | Time: 00:00:42 | Train Loss: 0.6956 | Average Loss: 1.1053 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:33 | Iter:    0 | Time: 00:00:42 | Train Loss: 0.8633 | Average Loss: 1.1030 \n","Epoch:33 | Iter:   20 | Time: 00:00:42 | Train Loss: 0.8051 | Average Loss: 1.0997 \n","Epoch:33 | Iter:   40 | Time: 00:00:43 | Train Loss: 0.7763 | Average Loss: 1.0964 \n","Epoch:33 | Iter:   60 | Time: 00:00:43 | Train Loss: 0.5427 | Average Loss: 1.0930 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:34 | Iter:    0 | Time: 00:00:43 | Train Loss: 0.7774 | Average Loss: 1.0907 \n","Epoch:34 | Iter:   20 | Time: 00:00:44 | Train Loss: 0.7664 | Average Loss: 1.0873 \n","Epoch:34 | Iter:   40 | Time: 00:00:44 | Train Loss: 0.8395 | Average Loss: 1.0840 \n","Epoch:34 | Iter:   60 | Time: 00:00:44 | Train Loss: 0.5279 | Average Loss: 1.0809 \n","Accuracy: 0.656250 | Time: 00:00:00\n","Epoch:35 | Iter:    0 | Time: 00:00:45 | Train Loss: 0.8244 | Average Loss: 1.0790 \n","Epoch:35 | Iter:   20 | Time: 00:00:45 | Train Loss: 0.7293 | Average Loss: 1.0756 \n","Epoch:35 | Iter:   40 | Time: 00:00:45 | Train Loss: 0.7430 | Average Loss: 1.0726 \n","Epoch:35 | Iter:   60 | Time: 00:00:46 | Train Loss: 0.6859 | Average Loss: 1.0698 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:36 | Iter:    0 | Time: 00:00:46 | Train Loss: 0.7386 | Average Loss: 1.0676 \n","Epoch:36 | Iter:   20 | Time: 00:00:46 | Train Loss: 0.6001 | Average Loss: 1.0642 \n","Epoch:36 | Iter:   40 | Time: 00:00:47 | Train Loss: 0.8003 | Average Loss: 1.0613 \n","Epoch:36 | Iter:   60 | Time: 00:00:47 | Train Loss: 0.5269 | Average Loss: 1.0585 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:37 | Iter:    0 | Time: 00:00:47 | Train Loss: 0.8985 | Average Loss: 1.0565 \n","Epoch:37 | Iter:   20 | Time: 00:00:48 | Train Loss: 0.6439 | Average Loss: 1.0533 \n","Epoch:37 | Iter:   40 | Time: 00:00:48 | Train Loss: 0.7765 | Average Loss: 1.0503 \n","Epoch:37 | Iter:   60 | Time: 00:00:48 | Train Loss: 0.5285 | Average Loss: 1.0476 \n","Accuracy: 0.664062 | Time: 00:00:00\n","Epoch:38 | Iter:    0 | Time: 00:00:49 | Train Loss: 0.5588 | Average Loss: 1.0455 \n","Epoch:38 | Iter:   20 | Time: 00:00:49 | Train Loss: 0.6802 | Average Loss: 1.0425 \n","Epoch:38 | Iter:   40 | Time: 00:00:49 | Train Loss: 0.7111 | Average Loss: 1.0400 \n","Epoch:38 | Iter:   60 | Time: 00:00:50 | Train Loss: 0.4372 | Average Loss: 1.0372 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:39 | Iter:    0 | Time: 00:00:50 | Train Loss: 0.6998 | Average Loss: 1.0353 \n","Epoch:39 | Iter:   20 | Time: 00:00:50 | Train Loss: 0.7581 | Average Loss: 1.0323 \n","Epoch:39 | Iter:   40 | Time: 00:00:51 | Train Loss: 0.5692 | Average Loss: 1.0296 \n","Epoch:39 | Iter:   60 | Time: 00:00:51 | Train Loss: 0.6244 | Average Loss: 1.0271 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:40 | Iter:    0 | Time: 00:00:51 | Train Loss: 0.6724 | Average Loss: 1.0253 \n","Epoch:40 | Iter:   20 | Time: 00:00:52 | Train Loss: 0.6723 | Average Loss: 1.0225 \n","Epoch:40 | Iter:   40 | Time: 00:00:52 | Train Loss: 0.7781 | Average Loss: 1.0198 \n","Epoch:40 | Iter:   60 | Time: 00:00:52 | Train Loss: 0.5756 | Average Loss: 1.0175 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:41 | Iter:    0 | Time: 00:00:53 | Train Loss: 0.6181 | Average Loss: 1.0156 \n","Epoch:41 | Iter:   20 | Time: 00:00:53 | Train Loss: 0.7195 | Average Loss: 1.0130 \n","Epoch:41 | Iter:   40 | Time: 00:00:53 | Train Loss: 0.7680 | Average Loss: 1.0105 \n","Epoch:41 | Iter:   60 | Time: 00:00:54 | Train Loss: 0.6302 | Average Loss: 1.0081 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:42 | Iter:    0 | Time: 00:00:54 | Train Loss: 0.7913 | Average Loss: 1.0063 \n","Epoch:42 | Iter:   20 | Time: 00:00:54 | Train Loss: 0.7037 | Average Loss: 1.0039 \n","Epoch:42 | Iter:   40 | Time: 00:00:55 | Train Loss: 0.6631 | Average Loss: 1.0013 \n","Epoch:42 | Iter:   60 | Time: 00:00:55 | Train Loss: 0.4700 | Average Loss: 0.9990 \n","Accuracy: 0.677083 | Time: 00:00:00\n","Epoch:43 | Iter:    0 | Time: 00:00:55 | Train Loss: 0.6795 | Average Loss: 0.9972 \n","Epoch:43 | Iter:   20 | Time: 00:00:56 | Train Loss: 0.6653 | Average Loss: 0.9944 \n","Epoch:43 | Iter:   40 | Time: 00:00:56 | Train Loss: 0.8554 | Average Loss: 0.9919 \n","Epoch:43 | Iter:   60 | Time: 00:00:56 | Train Loss: 0.6125 | Average Loss: 0.9897 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:44 | Iter:    0 | Time: 00:00:57 | Train Loss: 0.6483 | Average Loss: 0.9882 \n","Epoch:44 | Iter:   20 | Time: 00:00:57 | Train Loss: 0.6775 | Average Loss: 0.9857 \n","Epoch:44 | Iter:   40 | Time: 00:00:57 | Train Loss: 0.6301 | Average Loss: 0.9832 \n","Epoch:44 | Iter:   60 | Time: 00:00:58 | Train Loss: 0.6483 | Average Loss: 0.9809 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:45 | Iter:    0 | Time: 00:00:58 | Train Loss: 0.7249 | Average Loss: 0.9794 \n","Epoch:45 | Iter:   20 | Time: 00:00:58 | Train Loss: 0.6918 | Average Loss: 0.9772 \n","Epoch:45 | Iter:   40 | Time: 00:00:59 | Train Loss: 0.6931 | Average Loss: 0.9750 \n","Epoch:45 | Iter:   60 | Time: 00:00:59 | Train Loss: 0.5216 | Average Loss: 0.9730 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:46 | Iter:    0 | Time: 00:00:59 | Train Loss: 0.7378 | Average Loss: 0.9714 \n","Epoch:46 | Iter:   20 | Time: 00:01:00 | Train Loss: 0.8063 | Average Loss: 0.9692 \n","Epoch:46 | Iter:   40 | Time: 00:01:00 | Train Loss: 0.8510 | Average Loss: 0.9667 \n","Epoch:46 | Iter:   60 | Time: 00:01:00 | Train Loss: 0.6129 | Average Loss: 0.9647 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:47 | Iter:    0 | Time: 00:01:01 | Train Loss: 0.5060 | Average Loss: 0.9631 \n","Epoch:47 | Iter:   20 | Time: 00:01:01 | Train Loss: 0.6089 | Average Loss: 0.9608 \n","Epoch:47 | Iter:   40 | Time: 00:01:01 | Train Loss: 0.6361 | Average Loss: 0.9587 \n","Epoch:47 | Iter:   60 | Time: 00:01:02 | Train Loss: 0.5733 | Average Loss: 0.9568 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:48 | Iter:    0 | Time: 00:01:02 | Train Loss: 0.7106 | Average Loss: 0.9553 \n","Epoch:48 | Iter:   20 | Time: 00:01:02 | Train Loss: 0.5808 | Average Loss: 0.9531 \n","Epoch:48 | Iter:   40 | Time: 00:01:03 | Train Loss: 0.6563 | Average Loss: 0.9510 \n","Epoch:48 | Iter:   60 | Time: 00:01:03 | Train Loss: 0.6072 | Average Loss: 0.9490 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:49 | Iter:    0 | Time: 00:01:03 | Train Loss: 0.5658 | Average Loss: 0.9475 \n","Epoch:49 | Iter:   20 | Time: 00:01:04 | Train Loss: 0.6284 | Average Loss: 0.9453 \n","Epoch:49 | Iter:   40 | Time: 00:01:04 | Train Loss: 0.5635 | Average Loss: 0.9432 \n","Epoch:49 | Iter:   60 | Time: 00:01:04 | Train Loss: 0.5612 | Average Loss: 0.9413 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:50 | Iter:    0 | Time: 00:01:05 | Train Loss: 0.6616 | Average Loss: 0.9400 \n","Epoch:50 | Iter:   20 | Time: 00:01:05 | Train Loss: 0.5314 | Average Loss: 0.9379 \n","Epoch:50 | Iter:   40 | Time: 00:01:05 | Train Loss: 0.6710 | Average Loss: 0.9358 \n","Epoch:50 | Iter:   60 | Time: 00:01:06 | Train Loss: 0.5782 | Average Loss: 0.9339 \n","Accuracy: 0.666667 | Time: 00:00:00\n","Epoch:51 | Iter:    0 | Time: 00:01:06 | Train Loss: 0.6218 | Average Loss: 0.9324 \n","Epoch:51 | Iter:   20 | Time: 00:01:06 | Train Loss: 0.5583 | Average Loss: 0.9304 \n","Epoch:51 | Iter:   40 | Time: 00:01:07 | Train Loss: 0.6332 | Average Loss: 0.9287 \n","Epoch:51 | Iter:   60 | Time: 00:01:07 | Train Loss: 0.3739 | Average Loss: 0.9270 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:52 | Iter:    0 | Time: 00:01:07 | Train Loss: 0.4962 | Average Loss: 0.9257 \n","Epoch:52 | Iter:   20 | Time: 00:01:08 | Train Loss: 0.5077 | Average Loss: 0.9237 \n","Epoch:52 | Iter:   40 | Time: 00:01:08 | Train Loss: 0.5975 | Average Loss: 0.9218 \n","Epoch:52 | Iter:   60 | Time: 00:01:08 | Train Loss: 0.6900 | Average Loss: 0.9200 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:53 | Iter:    0 | Time: 00:01:09 | Train Loss: 0.8841 | Average Loss: 0.9186 \n","Epoch:53 | Iter:   20 | Time: 00:01:09 | Train Loss: 0.4679 | Average Loss: 0.9164 \n","Epoch:53 | Iter:   40 | Time: 00:01:09 | Train Loss: 0.6905 | Average Loss: 0.9146 \n","Epoch:53 | Iter:   60 | Time: 00:01:10 | Train Loss: 0.5689 | Average Loss: 0.9130 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:54 | Iter:    0 | Time: 00:01:10 | Train Loss: 0.5454 | Average Loss: 0.9119 \n","Epoch:54 | Iter:   20 | Time: 00:01:10 | Train Loss: 0.4758 | Average Loss: 0.9100 \n","Epoch:54 | Iter:   40 | Time: 00:01:11 | Train Loss: 0.7840 | Average Loss: 0.9081 \n","Epoch:54 | Iter:   60 | Time: 00:01:11 | Train Loss: 0.6151 | Average Loss: 0.9066 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:55 | Iter:    0 | Time: 00:01:11 | Train Loss: 0.6216 | Average Loss: 0.9053 \n","Epoch:55 | Iter:   20 | Time: 00:01:12 | Train Loss: 0.5617 | Average Loss: 0.9035 \n","Epoch:55 | Iter:   40 | Time: 00:01:12 | Train Loss: 0.6040 | Average Loss: 0.9017 \n","Epoch:55 | Iter:   60 | Time: 00:01:12 | Train Loss: 0.4889 | Average Loss: 0.9000 \n","Accuracy: 0.669271 | Time: 00:00:00\n","Epoch:56 | Iter:    0 | Time: 00:01:13 | Train Loss: 0.6646 | Average Loss: 0.8987 \n","Epoch:56 | Iter:   20 | Time: 00:01:13 | Train Loss: 0.6194 | Average Loss: 0.8971 \n","Epoch:56 | Iter:   40 | Time: 00:01:13 | Train Loss: 0.6540 | Average Loss: 0.8954 \n","Epoch:56 | Iter:   60 | Time: 00:01:14 | Train Loss: 0.4668 | Average Loss: 0.8939 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:57 | Iter:    0 | Time: 00:01:14 | Train Loss: 0.6140 | Average Loss: 0.8927 \n","Epoch:57 | Iter:   20 | Time: 00:01:14 | Train Loss: 0.5637 | Average Loss: 0.8910 \n","Epoch:57 | Iter:   40 | Time: 00:01:15 | Train Loss: 0.6669 | Average Loss: 0.8896 \n","Epoch:57 | Iter:   60 | Time: 00:01:15 | Train Loss: 0.4490 | Average Loss: 0.8880 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:58 | Iter:    0 | Time: 00:01:15 | Train Loss: 0.6177 | Average Loss: 0.8868 \n","Epoch:58 | Iter:   20 | Time: 00:01:16 | Train Loss: 0.8161 | Average Loss: 0.8851 \n","Epoch:58 | Iter:   40 | Time: 00:01:16 | Train Loss: 0.6512 | Average Loss: 0.8835 \n","Epoch:58 | Iter:   60 | Time: 00:01:16 | Train Loss: 0.4276 | Average Loss: 0.8820 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:59 | Iter:    0 | Time: 00:01:17 | Train Loss: 0.6002 | Average Loss: 0.8809 \n","Epoch:59 | Iter:   20 | Time: 00:01:17 | Train Loss: 0.7931 | Average Loss: 0.8792 \n","Epoch:59 | Iter:   40 | Time: 00:01:17 | Train Loss: 0.7058 | Average Loss: 0.8775 \n","Epoch:59 | Iter:   60 | Time: 00:01:18 | Train Loss: 0.3784 | Average Loss: 0.8759 \n","Accuracy: 0.674479 | Time: 00:00:00\n","Epoch:60 | Iter:    0 | Time: 00:01:18 | Train Loss: 0.5403 | Average Loss: 0.8748 \n","Epoch:60 | Iter:   20 | Time: 00:01:18 | Train Loss: 0.4627 | Average Loss: 0.8731 \n","Epoch:60 | Iter:   40 | Time: 00:01:19 | Train Loss: 0.5840 | Average Loss: 0.8715 \n","Epoch:60 | Iter:   60 | Time: 00:01:19 | Train Loss: 0.4253 | Average Loss: 0.8700 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:61 | Iter:    0 | Time: 00:01:19 | Train Loss: 0.4438 | Average Loss: 0.8688 \n","Epoch:61 | Iter:   20 | Time: 00:01:20 | Train Loss: 0.5818 | Average Loss: 0.8674 \n","Epoch:61 | Iter:   40 | Time: 00:01:20 | Train Loss: 0.4899 | Average Loss: 0.8658 \n","Epoch:61 | Iter:   60 | Time: 00:01:20 | Train Loss: 0.4243 | Average Loss: 0.8643 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:62 | Iter:    0 | Time: 00:01:21 | Train Loss: 0.5532 | Average Loss: 0.8632 \n","Epoch:62 | Iter:   20 | Time: 00:01:21 | Train Loss: 0.5992 | Average Loss: 0.8615 \n","Epoch:62 | Iter:   40 | Time: 00:01:21 | Train Loss: 0.5894 | Average Loss: 0.8599 \n","Epoch:62 | Iter:   60 | Time: 00:01:22 | Train Loss: 0.5520 | Average Loss: 0.8585 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:63 | Iter:    0 | Time: 00:01:22 | Train Loss: 0.5890 | Average Loss: 0.8575 \n","Epoch:63 | Iter:   20 | Time: 00:01:22 | Train Loss: 0.5490 | Average Loss: 0.8558 \n","Epoch:63 | Iter:   40 | Time: 00:01:23 | Train Loss: 0.5385 | Average Loss: 0.8541 \n","Epoch:63 | Iter:   60 | Time: 00:01:23 | Train Loss: 0.5247 | Average Loss: 0.8528 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:64 | Iter:    0 | Time: 00:01:23 | Train Loss: 0.5868 | Average Loss: 0.8516 \n","Epoch:64 | Iter:   20 | Time: 00:01:24 | Train Loss: 0.5560 | Average Loss: 0.8501 \n","Epoch:64 | Iter:   40 | Time: 00:01:24 | Train Loss: 0.5810 | Average Loss: 0.8485 \n","Epoch:64 | Iter:   60 | Time: 00:01:24 | Train Loss: 0.4191 | Average Loss: 0.8471 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:65 | Iter:    0 | Time: 00:01:25 | Train Loss: 0.5307 | Average Loss: 0.8460 \n","Epoch:65 | Iter:   20 | Time: 00:01:25 | Train Loss: 0.5525 | Average Loss: 0.8446 \n","Epoch:65 | Iter:   40 | Time: 00:01:25 | Train Loss: 0.6640 | Average Loss: 0.8432 \n","Epoch:65 | Iter:   60 | Time: 00:01:26 | Train Loss: 0.2430 | Average Loss: 0.8417 \n","Accuracy: 0.671875 | Time: 00:00:00\n","Epoch:66 | Iter:    0 | Time: 00:01:26 | Train Loss: 0.4948 | Average Loss: 0.8407 \n","Epoch:66 | Iter:   20 | Time: 00:01:26 | Train Loss: 0.4617 | Average Loss: 0.8393 \n","Epoch:66 | Iter:   40 | Time: 00:01:27 | Train Loss: 0.5130 | Average Loss: 0.8379 \n","Epoch:66 | Iter:   60 | Time: 00:01:27 | Train Loss: 0.4443 | Average Loss: 0.8366 \n","Accuracy: 0.679688 | Time: 00:00:00\n","Epoch:67 | Iter:    0 | Time: 00:01:27 | Train Loss: 0.4059 | Average Loss: 0.8356 \n","Epoch:67 | Iter:   20 | Time: 00:01:28 | Train Loss: 0.5991 | Average Loss: 0.8342 \n","Epoch:67 | Iter:   40 | Time: 00:01:28 | Train Loss: 0.5315 | Average Loss: 0.8328 \n","Epoch:67 | Iter:   60 | Time: 00:01:28 | Train Loss: 0.4672 | Average Loss: 0.8315 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:68 | Iter:    0 | Time: 00:01:29 | Train Loss: 0.4459 | Average Loss: 0.8304 \n","Epoch:68 | Iter:   20 | Time: 00:01:29 | Train Loss: 0.5580 | Average Loss: 0.8291 \n","Epoch:68 | Iter:   40 | Time: 00:01:29 | Train Loss: 0.4921 | Average Loss: 0.8276 \n","Epoch:68 | Iter:   60 | Time: 00:01:30 | Train Loss: 0.5136 | Average Loss: 0.8264 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:69 | Iter:    0 | Time: 00:01:30 | Train Loss: 0.3618 | Average Loss: 0.8254 \n","Epoch:69 | Iter:   20 | Time: 00:01:30 | Train Loss: 0.8045 | Average Loss: 0.8239 \n","Epoch:69 | Iter:   40 | Time: 00:01:31 | Train Loss: 0.5343 | Average Loss: 0.8227 \n","Epoch:69 | Iter:   60 | Time: 00:01:31 | Train Loss: 0.3293 | Average Loss: 0.8214 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:70 | Iter:    0 | Time: 00:01:31 | Train Loss: 0.4080 | Average Loss: 0.8205 \n","Epoch:70 | Iter:   20 | Time: 00:01:32 | Train Loss: 0.4368 | Average Loss: 0.8192 \n","Epoch:70 | Iter:   40 | Time: 00:01:32 | Train Loss: 0.6529 | Average Loss: 0.8178 \n","Epoch:70 | Iter:   60 | Time: 00:01:32 | Train Loss: 0.3730 | Average Loss: 0.8167 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:71 | Iter:    0 | Time: 00:01:33 | Train Loss: 0.3780 | Average Loss: 0.8158 \n","Epoch:71 | Iter:   20 | Time: 00:01:33 | Train Loss: 0.5205 | Average Loss: 0.8145 \n","Epoch:71 | Iter:   40 | Time: 00:01:33 | Train Loss: 0.6393 | Average Loss: 0.8133 \n","Epoch:71 | Iter:   60 | Time: 00:01:34 | Train Loss: 0.5787 | Average Loss: 0.8121 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:72 | Iter:    0 | Time: 00:01:34 | Train Loss: 0.5952 | Average Loss: 0.8113 \n","Epoch:72 | Iter:   20 | Time: 00:01:34 | Train Loss: 0.5343 | Average Loss: 0.8101 \n","Epoch:72 | Iter:   40 | Time: 00:01:35 | Train Loss: 0.5235 | Average Loss: 0.8088 \n","Epoch:72 | Iter:   60 | Time: 00:01:35 | Train Loss: 0.4803 | Average Loss: 0.8078 \n","Accuracy: 0.682292 | Time: 00:00:00\n","Epoch:73 | Iter:    0 | Time: 00:01:35 | Train Loss: 0.6858 | Average Loss: 0.8070 \n","Epoch:73 | Iter:   20 | Time: 00:01:36 | Train Loss: 0.4842 | Average Loss: 0.8059 \n","Epoch:73 | Iter:   40 | Time: 00:01:36 | Train Loss: 0.6761 | Average Loss: 0.8047 \n","Epoch:73 | Iter:   60 | Time: 00:01:36 | Train Loss: 0.3878 | Average Loss: 0.8035 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:74 | Iter:    0 | Time: 00:01:37 | Train Loss: 0.5139 | Average Loss: 0.8026 \n","Epoch:74 | Iter:   20 | Time: 00:01:37 | Train Loss: 0.5291 | Average Loss: 0.8013 \n","Epoch:74 | Iter:   40 | Time: 00:01:37 | Train Loss: 0.6897 | Average Loss: 0.8001 \n","Epoch:74 | Iter:   60 | Time: 00:01:38 | Train Loss: 0.4353 | Average Loss: 0.7990 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:75 | Iter:    0 | Time: 00:01:38 | Train Loss: 0.3919 | Average Loss: 0.7981 \n","Epoch:75 | Iter:   20 | Time: 00:01:38 | Train Loss: 0.4879 | Average Loss: 0.7970 \n","Epoch:75 | Iter:   40 | Time: 00:01:39 | Train Loss: 0.5293 | Average Loss: 0.7957 \n","Epoch:75 | Iter:   60 | Time: 00:01:39 | Train Loss: 0.4435 | Average Loss: 0.7946 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:76 | Iter:    0 | Time: 00:01:39 | Train Loss: 0.4273 | Average Loss: 0.7938 \n","Epoch:76 | Iter:   20 | Time: 00:01:40 | Train Loss: 0.4969 | Average Loss: 0.7926 \n","Epoch:76 | Iter:   40 | Time: 00:01:40 | Train Loss: 0.6010 | Average Loss: 0.7915 \n","Epoch:76 | Iter:   60 | Time: 00:01:40 | Train Loss: 0.4910 | Average Loss: 0.7904 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:77 | Iter:    0 | Time: 00:01:41 | Train Loss: 0.4282 | Average Loss: 0.7895 \n","Epoch:77 | Iter:   20 | Time: 00:01:41 | Train Loss: 0.6218 | Average Loss: 0.7885 \n","Epoch:77 | Iter:   40 | Time: 00:01:41 | Train Loss: 0.6200 | Average Loss: 0.7872 \n","Epoch:77 | Iter:   60 | Time: 00:01:42 | Train Loss: 0.5242 | Average Loss: 0.7862 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:78 | Iter:    0 | Time: 00:01:42 | Train Loss: 0.5557 | Average Loss: 0.7854 \n","Epoch:78 | Iter:   20 | Time: 00:01:42 | Train Loss: 0.5411 | Average Loss: 0.7842 \n","Epoch:78 | Iter:   40 | Time: 00:01:43 | Train Loss: 0.5811 | Average Loss: 0.7830 \n","Epoch:78 | Iter:   60 | Time: 00:01:43 | Train Loss: 0.3863 | Average Loss: 0.7819 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:79 | Iter:    0 | Time: 00:01:43 | Train Loss: 0.3655 | Average Loss: 0.7811 \n","Epoch:79 | Iter:   20 | Time: 00:01:44 | Train Loss: 0.4264 | Average Loss: 0.7798 \n","Epoch:79 | Iter:   40 | Time: 00:01:44 | Train Loss: 0.4976 | Average Loss: 0.7786 \n","Epoch:79 | Iter:   60 | Time: 00:01:44 | Train Loss: 0.3383 | Average Loss: 0.7775 \n","Accuracy: 0.708333 | Time: 00:00:00\n","Epoch:80 | Iter:    0 | Time: 00:01:45 | Train Loss: 0.3603 | Average Loss: 0.7767 \n","Epoch:80 | Iter:   20 | Time: 00:01:45 | Train Loss: 0.4093 | Average Loss: 0.7755 \n","Epoch:80 | Iter:   40 | Time: 00:01:45 | Train Loss: 0.6036 | Average Loss: 0.7745 \n","Epoch:80 | Iter:   60 | Time: 00:01:46 | Train Loss: 0.2901 | Average Loss: 0.7733 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:81 | Iter:    0 | Time: 00:01:46 | Train Loss: 0.4466 | Average Loss: 0.7725 \n","Epoch:81 | Iter:   20 | Time: 00:01:46 | Train Loss: 0.6171 | Average Loss: 0.7714 \n","Epoch:81 | Iter:   40 | Time: 00:01:47 | Train Loss: 0.5314 | Average Loss: 0.7704 \n","Epoch:81 | Iter:   60 | Time: 00:01:47 | Train Loss: 0.3593 | Average Loss: 0.7693 \n","Accuracy: 0.697917 | Time: 00:00:00\n","Epoch:82 | Iter:    0 | Time: 00:01:47 | Train Loss: 0.4969 | Average Loss: 0.7685 \n","Epoch:82 | Iter:   20 | Time: 00:01:48 | Train Loss: 0.3927 | Average Loss: 0.7673 \n","Epoch:82 | Iter:   40 | Time: 00:01:48 | Train Loss: 0.6434 | Average Loss: 0.7662 \n","Epoch:82 | Iter:   60 | Time: 00:01:48 | Train Loss: 0.3845 | Average Loss: 0.7653 \n","Accuracy: 0.708333 | Time: 00:00:00\n","Epoch:83 | Iter:    0 | Time: 00:01:49 | Train Loss: 0.6188 | Average Loss: 0.7646 \n","Epoch:83 | Iter:   20 | Time: 00:01:49 | Train Loss: 0.4217 | Average Loss: 0.7635 \n","Epoch:83 | Iter:   40 | Time: 00:01:49 | Train Loss: 0.4724 | Average Loss: 0.7625 \n","Epoch:83 | Iter:   60 | Time: 00:01:50 | Train Loss: 0.5697 | Average Loss: 0.7615 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:84 | Iter:    0 | Time: 00:01:50 | Train Loss: 0.5090 | Average Loss: 0.7607 \n","Epoch:84 | Iter:   20 | Time: 00:01:50 | Train Loss: 0.6263 | Average Loss: 0.7596 \n","Epoch:84 | Iter:   40 | Time: 00:01:51 | Train Loss: 0.5678 | Average Loss: 0.7585 \n","Epoch:84 | Iter:   60 | Time: 00:01:51 | Train Loss: 0.2810 | Average Loss: 0.7574 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:85 | Iter:    0 | Time: 00:01:51 | Train Loss: 0.5125 | Average Loss: 0.7566 \n","Epoch:85 | Iter:   20 | Time: 00:01:52 | Train Loss: 0.5055 | Average Loss: 0.7556 \n","Epoch:85 | Iter:   40 | Time: 00:01:52 | Train Loss: 0.6840 | Average Loss: 0.7545 \n","Epoch:85 | Iter:   60 | Time: 00:01:52 | Train Loss: 0.4291 | Average Loss: 0.7536 \n","Accuracy: 0.684896 | Time: 00:00:00\n","Epoch:86 | Iter:    0 | Time: 00:01:53 | Train Loss: 0.3776 | Average Loss: 0.7529 \n","Epoch:86 | Iter:   20 | Time: 00:01:53 | Train Loss: 0.6803 | Average Loss: 0.7519 \n","Epoch:86 | Iter:   40 | Time: 00:01:53 | Train Loss: 0.5609 | Average Loss: 0.7508 \n","Epoch:86 | Iter:   60 | Time: 00:01:54 | Train Loss: 0.4648 | Average Loss: 0.7498 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:87 | Iter:    0 | Time: 00:01:54 | Train Loss: 0.5457 | Average Loss: 0.7492 \n","Epoch:87 | Iter:   20 | Time: 00:01:54 | Train Loss: 0.5359 | Average Loss: 0.7482 \n","Epoch:87 | Iter:   40 | Time: 00:01:55 | Train Loss: 0.5240 | Average Loss: 0.7471 \n","Epoch:87 | Iter:   60 | Time: 00:01:55 | Train Loss: 0.3771 | Average Loss: 0.7462 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:88 | Iter:    0 | Time: 00:01:55 | Train Loss: 0.5112 | Average Loss: 0.7455 \n","Epoch:88 | Iter:   20 | Time: 00:01:56 | Train Loss: 0.4855 | Average Loss: 0.7446 \n","Epoch:88 | Iter:   40 | Time: 00:01:56 | Train Loss: 0.6159 | Average Loss: 0.7437 \n","Epoch:88 | Iter:   60 | Time: 00:01:56 | Train Loss: 0.4079 | Average Loss: 0.7429 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:89 | Iter:    0 | Time: 00:01:57 | Train Loss: 0.4498 | Average Loss: 0.7423 \n","Epoch:89 | Iter:   20 | Time: 00:01:57 | Train Loss: 0.7165 | Average Loss: 0.7413 \n","Epoch:89 | Iter:   40 | Time: 00:01:57 | Train Loss: 0.4729 | Average Loss: 0.7403 \n","Epoch:89 | Iter:   60 | Time: 00:01:58 | Train Loss: 0.5045 | Average Loss: 0.7394 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:90 | Iter:    0 | Time: 00:01:58 | Train Loss: 0.4159 | Average Loss: 0.7388 \n","Epoch:90 | Iter:   20 | Time: 00:01:58 | Train Loss: 0.5629 | Average Loss: 0.7378 \n","Epoch:90 | Iter:   40 | Time: 00:01:59 | Train Loss: 0.7298 | Average Loss: 0.7368 \n","Epoch:90 | Iter:   60 | Time: 00:01:59 | Train Loss: 0.5526 | Average Loss: 0.7359 \n","Accuracy: 0.705729 | Time: 00:00:00\n","Epoch:91 | Iter:    0 | Time: 00:01:59 | Train Loss: 0.4869 | Average Loss: 0.7352 \n","Epoch:91 | Iter:   20 | Time: 00:02:00 | Train Loss: 0.5247 | Average Loss: 0.7342 \n","Epoch:91 | Iter:   40 | Time: 00:02:00 | Train Loss: 0.4501 | Average Loss: 0.7332 \n","Epoch:91 | Iter:   60 | Time: 00:02:00 | Train Loss: 0.4438 | Average Loss: 0.7323 \n","Accuracy: 0.692708 | Time: 00:00:00\n","Epoch:92 | Iter:    0 | Time: 00:02:01 | Train Loss: 0.5666 | Average Loss: 0.7316 \n","Epoch:92 | Iter:   20 | Time: 00:02:01 | Train Loss: 0.4460 | Average Loss: 0.7307 \n","Epoch:92 | Iter:   40 | Time: 00:02:01 | Train Loss: 0.5839 | Average Loss: 0.7298 \n","Epoch:92 | Iter:   60 | Time: 00:02:02 | Train Loss: 0.3486 | Average Loss: 0.7289 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:93 | Iter:    0 | Time: 00:02:02 | Train Loss: 0.3862 | Average Loss: 0.7283 \n","Epoch:93 | Iter:   20 | Time: 00:02:02 | Train Loss: 0.4094 | Average Loss: 0.7274 \n","Epoch:93 | Iter:   40 | Time: 00:02:03 | Train Loss: 0.4970 | Average Loss: 0.7265 \n","Epoch:93 | Iter:   60 | Time: 00:02:03 | Train Loss: 0.4640 | Average Loss: 0.7257 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:94 | Iter:    0 | Time: 00:02:03 | Train Loss: 0.7067 | Average Loss: 0.7251 \n","Epoch:94 | Iter:   20 | Time: 00:02:04 | Train Loss: 0.5090 | Average Loss: 0.7241 \n","Epoch:94 | Iter:   40 | Time: 00:02:04 | Train Loss: 0.5815 | Average Loss: 0.7233 \n","Epoch:94 | Iter:   60 | Time: 00:02:04 | Train Loss: 0.2231 | Average Loss: 0.7224 \n","Accuracy: 0.690104 | Time: 00:00:00\n","Epoch:95 | Iter:    0 | Time: 00:02:05 | Train Loss: 0.5676 | Average Loss: 0.7219 \n","Epoch:95 | Iter:   20 | Time: 00:02:05 | Train Loss: 0.6324 | Average Loss: 0.7210 \n","Epoch:95 | Iter:   40 | Time: 00:02:05 | Train Loss: 0.5207 | Average Loss: 0.7202 \n","Epoch:95 | Iter:   60 | Time: 00:02:06 | Train Loss: 0.4740 | Average Loss: 0.7194 \n","Accuracy: 0.687500 | Time: 00:00:00\n","Epoch:96 | Iter:    0 | Time: 00:02:06 | Train Loss: 0.4270 | Average Loss: 0.7188 \n","Epoch:96 | Iter:   20 | Time: 00:02:06 | Train Loss: 0.4882 | Average Loss: 0.7179 \n","Epoch:96 | Iter:   40 | Time: 00:02:07 | Train Loss: 0.3495 | Average Loss: 0.7170 \n","Epoch:96 | Iter:   60 | Time: 00:02:07 | Train Loss: 0.4679 | Average Loss: 0.7162 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:97 | Iter:    0 | Time: 00:02:07 | Train Loss: 0.3505 | Average Loss: 0.7156 \n","Epoch:97 | Iter:   20 | Time: 00:02:08 | Train Loss: 0.5096 | Average Loss: 0.7147 \n","Epoch:97 | Iter:   40 | Time: 00:02:08 | Train Loss: 0.4712 | Average Loss: 0.7138 \n","Epoch:97 | Iter:   60 | Time: 00:02:08 | Train Loss: 0.3547 | Average Loss: 0.7129 \n","Accuracy: 0.695312 | Time: 00:00:00\n","Epoch:98 | Iter:    0 | Time: 00:02:09 | Train Loss: 0.5892 | Average Loss: 0.7124 \n","Epoch:98 | Iter:   20 | Time: 00:02:09 | Train Loss: 0.4381 | Average Loss: 0.7114 \n","Epoch:98 | Iter:   40 | Time: 00:02:09 | Train Loss: 0.5169 | Average Loss: 0.7105 \n","Epoch:98 | Iter:   60 | Time: 00:02:10 | Train Loss: 0.3241 | Average Loss: 0.7097 \n","Accuracy: 0.700521 | Time: 00:00:00\n","Epoch:99 | Iter:    0 | Time: 00:02:10 | Train Loss: 0.3012 | Average Loss: 0.7090 \n","Epoch:99 | Iter:   20 | Time: 00:02:10 | Train Loss: 0.4989 | Average Loss: 0.7082 \n","Epoch:99 | Iter:   40 | Time: 00:02:11 | Train Loss: 0.5344 | Average Loss: 0.7074 \n","Epoch:99 | Iter:   60 | Time: 00:02:11 | Train Loss: 0.2422 | Average Loss: 0.7066 \n","Accuracy: 0.703125 | Time: 00:00:00\n","Epoch:100 | Iter:    0 | Time: 00:02:11 | Train Loss: 0.4230 | Average Loss: 0.7060 \n","Epoch:100 | Iter:   20 | Time: 00:02:12 | Train Loss: 0.4880 | Average Loss: 0.7051 \n","Epoch:100 | Iter:   40 | Time: 00:02:12 | Train Loss: 0.7859 | Average Loss: 0.7044 \n","Epoch:100 | Iter:   60 | Time: 00:02:12 | Train Loss: 0.3498 | Average Loss: 0.7035 \n","Accuracy: 0.708333 | Time: 00:00:00\n"]}],"source":["#--------------------------------------------------\n","#       Start Training & Evaluation\n","#--------------------------------------------------\n","net = BatchNormalization()\n","train_option = {}\n","train_option['lr'] = 0.002\n","train_option['epoch'] = 100\n","train_option['device'] = 'gpu'\n","trainModel(net, trainloader_small, train_option, testloader_small)\n"]},{"cell_type":"markdown","metadata":{"id":"ZGgDlKIVH_wK"},"source":["### Technique 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKXeIjYIq65w"},"outputs":[],"source":["#--------------------------------------------------\n","#    Load Training Data and Testing Data\n","#--------------------------------------------------\n","\n","import torchvision.transforms as transforms\n","import torchsample as ts\n","\n","train = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((64,64)),\n","    torchvision.transforms.RandomHorizontalFlip(),\n","    torchvision.transforms.RandomRotation(20),\n","    torchvision.transforms.Grayscale(num_output_channels=1),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize([0], [1])\n","    ])\n","\n","test = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((64,64)),\n","    torchvision.transforms.Grayscale(num_output_channels=1),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize([0], [1])\n","    ])\n","# Train data augmented\n","augmentedTrainData = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder('./data/train/', transform=train),\n","batch_size=64, shuffle=True)\n","\n","# Test data augmented\n","augmentedTestData = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder('./data/test/', transform=test),\n","batch_size=64, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gekIIIh9fZ8j"},"outputs":[],"source":["#--------------------------------------------------\n","#       Define Network Architecture\n","#--------------------------------------------------\n","class AugmentationTechnique(nn.Module):\n","  def __init__(self):\n","    super(AugmentationTechnique,self).__init__()\n","    self.features = torch.nn.Sequential(\n","      nn.Conv2d(1, 32, 3),\n","      nn.ReLU(),\n","      nn.MaxPool2d(4, stride=4),\n","      nn.Dropout(0.5),\n","      nn.Conv2d(32, 256, 3),\n","      nn.ReLU(),\n","      nn.MaxPool2d(4, stride=4),\n","      nn.Dropout(0.5)\n","    )\n","\n","    self.classifier = nn.Sequential(\n","      nn.Linear(2304, 16),\n","    )\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = torch.flatten(x, 1)\n","    x = self.classifier(x)\n","    return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QonkDqbUWG0l","executionInfo":{"status":"ok","timestamp":1650855083161,"user_tz":240,"elapsed":1034806,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"31957ae6-51e8-490b-d1e4-82693cdc1241"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.8605 | Average Loss: 2.8605 \n","Epoch: 1 | Iter:   20 | Time: 00:00:05 | Train Loss: 2.7623 | Average Loss: 2.8060 \n","Accuracy: 0.140000 | Time: 00:00:01\n","Epoch: 2 | Iter:    0 | Time: 00:00:11 | Train Loss: 2.6115 | Average Loss: 2.7626 \n","Epoch: 2 | Iter:   20 | Time: 00:00:16 | Train Loss: 2.4780 | Average Loss: 2.6980 \n","Accuracy: 0.272500 | Time: 00:00:01\n","Epoch: 3 | Iter:    0 | Time: 00:00:21 | Train Loss: 2.2424 | Average Loss: 2.6427 \n","Epoch: 3 | Iter:   20 | Time: 00:00:26 | Train Loss: 2.2665 | Average Loss: 2.5890 \n","Accuracy: 0.265000 | Time: 00:00:01\n","Epoch: 4 | Iter:    0 | Time: 00:00:32 | Train Loss: 1.9537 | Average Loss: 2.5354 \n","Epoch: 4 | Iter:   20 | Time: 00:00:36 | Train Loss: 2.0844 | Average Loss: 2.4869 \n","Accuracy: 0.317500 | Time: 00:00:01\n","Epoch: 5 | Iter:    0 | Time: 00:00:42 | Train Loss: 2.0252 | Average Loss: 2.4520 \n","Epoch: 5 | Iter:   20 | Time: 00:00:47 | Train Loss: 2.1618 | Average Loss: 2.4097 \n","Accuracy: 0.360000 | Time: 00:00:01\n","Epoch: 6 | Iter:    0 | Time: 00:00:52 | Train Loss: 2.3957 | Average Loss: 2.3819 \n","Epoch: 6 | Iter:   20 | Time: 00:00:57 | Train Loss: 2.1748 | Average Loss: 2.3505 \n","Accuracy: 0.410000 | Time: 00:00:01\n","Epoch: 7 | Iter:    0 | Time: 00:01:03 | Train Loss: 1.8398 | Average Loss: 2.3168 \n","Epoch: 7 | Iter:   20 | Time: 00:01:08 | Train Loss: 1.9714 | Average Loss: 2.2822 \n","Accuracy: 0.415000 | Time: 00:00:01\n","Epoch: 8 | Iter:    0 | Time: 00:01:13 | Train Loss: 1.8428 | Average Loss: 2.2583 \n","Epoch: 8 | Iter:   20 | Time: 00:01:18 | Train Loss: 1.8281 | Average Loss: 2.2299 \n","Accuracy: 0.412500 | Time: 00:00:01\n","Epoch: 9 | Iter:    0 | Time: 00:01:23 | Train Loss: 1.5546 | Average Loss: 2.2067 \n","Epoch: 9 | Iter:   20 | Time: 00:01:28 | Train Loss: 1.9076 | Average Loss: 2.1826 \n","Accuracy: 0.410000 | Time: 00:00:01\n","Epoch:10 | Iter:    0 | Time: 00:01:34 | Train Loss: 1.7243 | Average Loss: 2.1614 \n","Epoch:10 | Iter:   20 | Time: 00:01:38 | Train Loss: 1.7887 | Average Loss: 2.1392 \n","Accuracy: 0.420000 | Time: 00:00:01\n","Epoch:11 | Iter:    0 | Time: 00:01:44 | Train Loss: 1.6806 | Average Loss: 2.1195 \n","Epoch:11 | Iter:   20 | Time: 00:01:49 | Train Loss: 1.8560 | Average Loss: 2.0985 \n","Accuracy: 0.445000 | Time: 00:00:01\n","Epoch:12 | Iter:    0 | Time: 00:01:54 | Train Loss: 1.7233 | Average Loss: 2.0809 \n","Epoch:12 | Iter:   20 | Time: 00:01:59 | Train Loss: 1.9098 | Average Loss: 2.0605 \n","Accuracy: 0.485000 | Time: 00:00:01\n","Epoch:13 | Iter:    0 | Time: 00:02:05 | Train Loss: 1.6768 | Average Loss: 2.0486 \n","Epoch:13 | Iter:   20 | Time: 00:02:10 | Train Loss: 1.6338 | Average Loss: 2.0306 \n","Accuracy: 0.467500 | Time: 00:00:01\n","Epoch:14 | Iter:    0 | Time: 00:02:15 | Train Loss: 1.7108 | Average Loss: 2.0173 \n","Epoch:14 | Iter:   20 | Time: 00:02:20 | Train Loss: 1.4513 | Average Loss: 1.9997 \n","Accuracy: 0.510000 | Time: 00:00:01\n","Epoch:15 | Iter:    0 | Time: 00:02:26 | Train Loss: 1.6359 | Average Loss: 1.9874 \n","Epoch:15 | Iter:   20 | Time: 00:02:31 | Train Loss: 1.5567 | Average Loss: 1.9729 \n","Accuracy: 0.497500 | Time: 00:00:01\n","Epoch:16 | Iter:    0 | Time: 00:02:36 | Train Loss: 1.5082 | Average Loss: 1.9606 \n","Epoch:16 | Iter:   20 | Time: 00:02:41 | Train Loss: 1.2210 | Average Loss: 1.9465 \n","Accuracy: 0.520000 | Time: 00:00:01\n","Epoch:17 | Iter:    0 | Time: 00:02:47 | Train Loss: 1.4882 | Average Loss: 1.9339 \n","Epoch:17 | Iter:   20 | Time: 00:02:51 | Train Loss: 1.4508 | Average Loss: 1.9206 \n","Accuracy: 0.545000 | Time: 00:00:01\n","Epoch:18 | Iter:    0 | Time: 00:02:57 | Train Loss: 1.5972 | Average Loss: 1.9100 \n","Epoch:18 | Iter:   20 | Time: 00:03:02 | Train Loss: 1.3401 | Average Loss: 1.8980 \n","Accuracy: 0.525000 | Time: 00:00:01\n","Epoch:19 | Iter:    0 | Time: 00:03:08 | Train Loss: 1.5658 | Average Loss: 1.8881 \n","Epoch:19 | Iter:   20 | Time: 00:03:12 | Train Loss: 1.3082 | Average Loss: 1.8774 \n","Accuracy: 0.512500 | Time: 00:00:01\n","Epoch:20 | Iter:    0 | Time: 00:03:18 | Train Loss: 1.3692 | Average Loss: 1.8682 \n","Epoch:20 | Iter:   20 | Time: 00:03:23 | Train Loss: 1.5093 | Average Loss: 1.8579 \n","Accuracy: 0.517500 | Time: 00:00:01\n","Epoch:21 | Iter:    0 | Time: 00:03:29 | Train Loss: 1.2518 | Average Loss: 1.8495 \n","Epoch:21 | Iter:   20 | Time: 00:03:34 | Train Loss: 1.2145 | Average Loss: 1.8406 \n","Accuracy: 0.547500 | Time: 00:00:01\n","Epoch:22 | Iter:    0 | Time: 00:03:39 | Train Loss: 1.6792 | Average Loss: 1.8332 \n","Epoch:22 | Iter:   20 | Time: 00:03:44 | Train Loss: 1.6205 | Average Loss: 1.8240 \n","Accuracy: 0.512500 | Time: 00:00:01\n","Epoch:23 | Iter:    0 | Time: 00:03:50 | Train Loss: 1.3752 | Average Loss: 1.8154 \n","Epoch:23 | Iter:   20 | Time: 00:03:54 | Train Loss: 1.3336 | Average Loss: 1.8065 \n","Accuracy: 0.505000 | Time: 00:00:01\n","Epoch:24 | Iter:    0 | Time: 00:04:00 | Train Loss: 1.3085 | Average Loss: 1.7990 \n","Epoch:24 | Iter:   20 | Time: 00:04:05 | Train Loss: 1.4584 | Average Loss: 1.7902 \n","Accuracy: 0.552500 | Time: 00:00:01\n","Epoch:25 | Iter:    0 | Time: 00:04:11 | Train Loss: 1.1930 | Average Loss: 1.7828 \n","Epoch:25 | Iter:   20 | Time: 00:04:15 | Train Loss: 1.5786 | Average Loss: 1.7748 \n","Accuracy: 0.532500 | Time: 00:00:01\n","Epoch:26 | Iter:    0 | Time: 00:04:21 | Train Loss: 1.4275 | Average Loss: 1.7685 \n","Epoch:26 | Iter:   20 | Time: 00:04:26 | Train Loss: 1.7269 | Average Loss: 1.7609 \n","Accuracy: 0.502500 | Time: 00:00:01\n","Epoch:27 | Iter:    0 | Time: 00:04:31 | Train Loss: 1.3695 | Average Loss: 1.7550 \n","Epoch:27 | Iter:   20 | Time: 00:04:36 | Train Loss: 1.3995 | Average Loss: 1.7478 \n","Accuracy: 0.542500 | Time: 00:00:01\n","Epoch:28 | Iter:    0 | Time: 00:04:42 | Train Loss: 1.3369 | Average Loss: 1.7426 \n","Epoch:28 | Iter:   20 | Time: 00:04:47 | Train Loss: 1.3052 | Average Loss: 1.7354 \n","Accuracy: 0.537500 | Time: 00:00:01\n","Epoch:29 | Iter:    0 | Time: 00:04:52 | Train Loss: 1.3889 | Average Loss: 1.7307 \n","Epoch:29 | Iter:   20 | Time: 00:04:57 | Train Loss: 1.1901 | Average Loss: 1.7239 \n","Accuracy: 0.565000 | Time: 00:00:01\n","Epoch:30 | Iter:    0 | Time: 00:05:03 | Train Loss: 1.3148 | Average Loss: 1.7184 \n","Epoch:30 | Iter:   20 | Time: 00:05:08 | Train Loss: 1.2071 | Average Loss: 1.7115 \n","Accuracy: 0.560000 | Time: 00:00:01\n","Epoch:31 | Iter:    0 | Time: 00:05:13 | Train Loss: 1.3954 | Average Loss: 1.7064 \n","Epoch:31 | Iter:   20 | Time: 00:05:18 | Train Loss: 1.7236 | Average Loss: 1.7001 \n","Accuracy: 0.547500 | Time: 00:00:01\n","Epoch:32 | Iter:    0 | Time: 00:05:24 | Train Loss: 1.5316 | Average Loss: 1.6945 \n","Epoch:32 | Iter:   20 | Time: 00:05:29 | Train Loss: 1.2897 | Average Loss: 1.6887 \n","Accuracy: 0.540000 | Time: 00:00:01\n","Epoch:33 | Iter:    0 | Time: 00:05:35 | Train Loss: 1.2596 | Average Loss: 1.6835 \n","Epoch:33 | Iter:   20 | Time: 00:05:39 | Train Loss: 1.5224 | Average Loss: 1.6786 \n","Accuracy: 0.550000 | Time: 00:00:01\n","Epoch:34 | Iter:    0 | Time: 00:05:45 | Train Loss: 1.3413 | Average Loss: 1.6737 \n","Epoch:34 | Iter:   20 | Time: 00:05:50 | Train Loss: 1.1651 | Average Loss: 1.6673 \n","Accuracy: 0.560000 | Time: 00:00:01\n","Epoch:35 | Iter:    0 | Time: 00:05:55 | Train Loss: 1.1326 | Average Loss: 1.6626 \n","Epoch:35 | Iter:   20 | Time: 00:06:00 | Train Loss: 1.4563 | Average Loss: 1.6577 \n","Accuracy: 0.565000 | Time: 00:00:01\n","Epoch:36 | Iter:    0 | Time: 00:06:05 | Train Loss: 1.1004 | Average Loss: 1.6529 \n","Epoch:36 | Iter:   20 | Time: 00:06:10 | Train Loss: 1.1732 | Average Loss: 1.6479 \n","Accuracy: 0.612500 | Time: 00:00:01\n","Epoch:37 | Iter:    0 | Time: 00:06:16 | Train Loss: 1.2360 | Average Loss: 1.6436 \n","Epoch:37 | Iter:   20 | Time: 00:06:21 | Train Loss: 1.1117 | Average Loss: 1.6388 \n","Accuracy: 0.565000 | Time: 00:00:01\n","Epoch:38 | Iter:    0 | Time: 00:06:26 | Train Loss: 1.3945 | Average Loss: 1.6350 \n","Epoch:38 | Iter:   20 | Time: 00:06:31 | Train Loss: 1.3493 | Average Loss: 1.6301 \n","Accuracy: 0.582500 | Time: 00:00:01\n","Epoch:39 | Iter:    0 | Time: 00:06:36 | Train Loss: 1.2775 | Average Loss: 1.6260 \n","Epoch:39 | Iter:   20 | Time: 00:06:41 | Train Loss: 1.3308 | Average Loss: 1.6208 \n","Accuracy: 0.557500 | Time: 00:00:01\n","Epoch:40 | Iter:    0 | Time: 00:06:47 | Train Loss: 1.3427 | Average Loss: 1.6171 \n","Epoch:40 | Iter:   20 | Time: 00:06:52 | Train Loss: 1.2732 | Average Loss: 1.6128 \n","Accuracy: 0.592500 | Time: 00:00:01\n","Epoch:41 | Iter:    0 | Time: 00:06:57 | Train Loss: 1.2591 | Average Loss: 1.6092 \n","Epoch:41 | Iter:   20 | Time: 00:07:02 | Train Loss: 1.1694 | Average Loss: 1.6043 \n","Accuracy: 0.530000 | Time: 00:00:01\n","Epoch:42 | Iter:    0 | Time: 00:07:07 | Train Loss: 1.2248 | Average Loss: 1.6007 \n","Epoch:42 | Iter:   20 | Time: 00:07:12 | Train Loss: 1.2943 | Average Loss: 1.5963 \n","Accuracy: 0.580000 | Time: 00:00:01\n","Epoch:43 | Iter:    0 | Time: 00:07:18 | Train Loss: 1.1714 | Average Loss: 1.5925 \n","Epoch:43 | Iter:   20 | Time: 00:07:23 | Train Loss: 1.2355 | Average Loss: 1.5882 \n","Accuracy: 0.597500 | Time: 00:00:01\n","Epoch:44 | Iter:    0 | Time: 00:07:28 | Train Loss: 1.0630 | Average Loss: 1.5843 \n","Epoch:44 | Iter:   20 | Time: 00:07:33 | Train Loss: 1.1924 | Average Loss: 1.5804 \n","Accuracy: 0.580000 | Time: 00:00:01\n","Epoch:45 | Iter:    0 | Time: 00:07:38 | Train Loss: 1.0923 | Average Loss: 1.5772 \n","Epoch:45 | Iter:   20 | Time: 00:07:43 | Train Loss: 1.1740 | Average Loss: 1.5735 \n","Accuracy: 0.550000 | Time: 00:00:01\n","Epoch:46 | Iter:    0 | Time: 00:07:49 | Train Loss: 1.1654 | Average Loss: 1.5694 \n","Epoch:46 | Iter:   20 | Time: 00:07:53 | Train Loss: 1.1290 | Average Loss: 1.5655 \n","Accuracy: 0.587500 | Time: 00:00:01\n","Epoch:47 | Iter:    0 | Time: 00:07:59 | Train Loss: 1.1249 | Average Loss: 1.5622 \n","Epoch:47 | Iter:   20 | Time: 00:08:04 | Train Loss: 1.2882 | Average Loss: 1.5591 \n","Accuracy: 0.607500 | Time: 00:00:01\n","Epoch:48 | Iter:    0 | Time: 00:08:09 | Train Loss: 1.2708 | Average Loss: 1.5561 \n","Epoch:48 | Iter:   20 | Time: 00:08:14 | Train Loss: 1.0827 | Average Loss: 1.5526 \n","Accuracy: 0.585000 | Time: 00:00:01\n","Epoch:49 | Iter:    0 | Time: 00:08:20 | Train Loss: 1.0436 | Average Loss: 1.5491 \n","Epoch:49 | Iter:   20 | Time: 00:08:24 | Train Loss: 1.0873 | Average Loss: 1.5458 \n","Accuracy: 0.585000 | Time: 00:00:01\n","Epoch:50 | Iter:    0 | Time: 00:08:30 | Train Loss: 1.1454 | Average Loss: 1.5426 \n","Epoch:50 | Iter:   20 | Time: 00:08:35 | Train Loss: 1.3695 | Average Loss: 1.5391 \n","Accuracy: 0.575000 | Time: 00:00:01\n","Epoch:51 | Iter:    0 | Time: 00:08:40 | Train Loss: 1.0073 | Average Loss: 1.5362 \n","Epoch:51 | Iter:   20 | Time: 00:08:45 | Train Loss: 1.0021 | Average Loss: 1.5330 \n","Accuracy: 0.552500 | Time: 00:00:01\n","Epoch:52 | Iter:    0 | Time: 00:08:51 | Train Loss: 1.2854 | Average Loss: 1.5297 \n","Epoch:52 | Iter:   20 | Time: 00:08:55 | Train Loss: 1.0799 | Average Loss: 1.5264 \n","Accuracy: 0.582500 | Time: 00:00:01\n","Epoch:53 | Iter:    0 | Time: 00:09:01 | Train Loss: 1.4123 | Average Loss: 1.5237 \n","Epoch:53 | Iter:   20 | Time: 00:09:06 | Train Loss: 1.1882 | Average Loss: 1.5205 \n","Accuracy: 0.607500 | Time: 00:00:01\n","Epoch:54 | Iter:    0 | Time: 00:09:11 | Train Loss: 0.9860 | Average Loss: 1.5176 \n","Epoch:54 | Iter:   20 | Time: 00:09:16 | Train Loss: 1.0109 | Average Loss: 1.5146 \n","Accuracy: 0.597500 | Time: 00:00:01\n","Epoch:55 | Iter:    0 | Time: 00:09:21 | Train Loss: 1.2267 | Average Loss: 1.5119 \n","Epoch:55 | Iter:   20 | Time: 00:09:26 | Train Loss: 1.0001 | Average Loss: 1.5090 \n","Accuracy: 0.572500 | Time: 00:00:01\n","Epoch:56 | Iter:    0 | Time: 00:09:32 | Train Loss: 1.1254 | Average Loss: 1.5068 \n","Epoch:56 | Iter:   20 | Time: 00:09:37 | Train Loss: 1.0488 | Average Loss: 1.5032 \n","Accuracy: 0.602500 | Time: 00:00:01\n","Epoch:57 | Iter:    0 | Time: 00:09:42 | Train Loss: 1.2933 | Average Loss: 1.5011 \n","Epoch:57 | Iter:   20 | Time: 00:09:47 | Train Loss: 1.1692 | Average Loss: 1.4979 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:58 | Iter:    0 | Time: 00:09:52 | Train Loss: 1.3787 | Average Loss: 1.4957 \n","Epoch:58 | Iter:   20 | Time: 00:09:57 | Train Loss: 1.0303 | Average Loss: 1.4928 \n","Accuracy: 0.585000 | Time: 00:00:01\n","Epoch:59 | Iter:    0 | Time: 00:10:03 | Train Loss: 1.1697 | Average Loss: 1.4900 \n","Epoch:59 | Iter:   20 | Time: 00:10:07 | Train Loss: 1.4139 | Average Loss: 1.4876 \n","Accuracy: 0.602500 | Time: 00:00:01\n","Epoch:60 | Iter:    0 | Time: 00:10:13 | Train Loss: 0.8469 | Average Loss: 1.4848 \n","Epoch:60 | Iter:   20 | Time: 00:10:18 | Train Loss: 1.1918 | Average Loss: 1.4825 \n","Accuracy: 0.587500 | Time: 00:00:01\n","Epoch:61 | Iter:    0 | Time: 00:10:23 | Train Loss: 1.2230 | Average Loss: 1.4801 \n","Epoch:61 | Iter:   20 | Time: 00:10:28 | Train Loss: 1.1087 | Average Loss: 1.4774 \n","Accuracy: 0.595000 | Time: 00:00:01\n","Epoch:62 | Iter:    0 | Time: 00:10:34 | Train Loss: 1.2900 | Average Loss: 1.4752 \n","Epoch:62 | Iter:   20 | Time: 00:10:39 | Train Loss: 1.3622 | Average Loss: 1.4725 \n","Accuracy: 0.610000 | Time: 00:00:01\n","Epoch:63 | Iter:    0 | Time: 00:10:44 | Train Loss: 1.1069 | Average Loss: 1.4705 \n","Epoch:63 | Iter:   20 | Time: 00:10:49 | Train Loss: 0.9746 | Average Loss: 1.4677 \n","Accuracy: 0.597500 | Time: 00:00:01\n","Epoch:64 | Iter:    0 | Time: 00:10:55 | Train Loss: 1.1369 | Average Loss: 1.4656 \n","Epoch:64 | Iter:   20 | Time: 00:11:00 | Train Loss: 1.1721 | Average Loss: 1.4630 \n","Accuracy: 0.620000 | Time: 00:00:01\n","Epoch:65 | Iter:    0 | Time: 00:11:05 | Train Loss: 1.2571 | Average Loss: 1.4607 \n","Epoch:65 | Iter:   20 | Time: 00:11:10 | Train Loss: 0.9466 | Average Loss: 1.4586 \n","Accuracy: 0.600000 | Time: 00:00:01\n","Epoch:66 | Iter:    0 | Time: 00:11:16 | Train Loss: 1.5158 | Average Loss: 1.4567 \n","Epoch:66 | Iter:   20 | Time: 00:11:21 | Train Loss: 0.9957 | Average Loss: 1.4542 \n","Accuracy: 0.580000 | Time: 00:00:01\n","Epoch:67 | Iter:    0 | Time: 00:11:26 | Train Loss: 1.1197 | Average Loss: 1.4519 \n","Epoch:67 | Iter:   20 | Time: 00:11:31 | Train Loss: 1.2166 | Average Loss: 1.4491 \n","Accuracy: 0.610000 | Time: 00:00:01\n","Epoch:68 | Iter:    0 | Time: 00:11:36 | Train Loss: 1.2190 | Average Loss: 1.4472 \n","Epoch:68 | Iter:   20 | Time: 00:11:41 | Train Loss: 1.3908 | Average Loss: 1.4450 \n","Accuracy: 0.590000 | Time: 00:00:01\n","Epoch:69 | Iter:    0 | Time: 00:11:46 | Train Loss: 1.2348 | Average Loss: 1.4432 \n","Epoch:69 | Iter:   20 | Time: 00:11:51 | Train Loss: 1.0692 | Average Loss: 1.4411 \n","Accuracy: 0.610000 | Time: 00:00:01\n","Epoch:70 | Iter:    0 | Time: 00:11:57 | Train Loss: 1.0826 | Average Loss: 1.4390 \n","Epoch:70 | Iter:   20 | Time: 00:12:01 | Train Loss: 0.8475 | Average Loss: 1.4362 \n","Accuracy: 0.610000 | Time: 00:00:01\n","Epoch:71 | Iter:    0 | Time: 00:12:07 | Train Loss: 1.0039 | Average Loss: 1.4340 \n","Epoch:71 | Iter:   20 | Time: 00:12:12 | Train Loss: 1.1622 | Average Loss: 1.4323 \n","Accuracy: 0.602500 | Time: 00:00:01\n","Epoch:72 | Iter:    0 | Time: 00:12:17 | Train Loss: 1.0529 | Average Loss: 1.4302 \n","Epoch:72 | Iter:   20 | Time: 00:12:22 | Train Loss: 1.1629 | Average Loss: 1.4280 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:73 | Iter:    0 | Time: 00:12:27 | Train Loss: 1.1788 | Average Loss: 1.4260 \n","Epoch:73 | Iter:   20 | Time: 00:12:32 | Train Loss: 1.0532 | Average Loss: 1.4237 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:74 | Iter:    0 | Time: 00:12:38 | Train Loss: 0.8931 | Average Loss: 1.4215 \n","Epoch:74 | Iter:   20 | Time: 00:12:43 | Train Loss: 1.0202 | Average Loss: 1.4191 \n","Accuracy: 0.617500 | Time: 00:00:01\n","Epoch:75 | Iter:    0 | Time: 00:12:48 | Train Loss: 1.0820 | Average Loss: 1.4177 \n","Epoch:75 | Iter:   20 | Time: 00:12:53 | Train Loss: 0.9202 | Average Loss: 1.4158 \n","Accuracy: 0.637500 | Time: 00:00:01\n","Epoch:76 | Iter:    0 | Time: 00:12:58 | Train Loss: 1.2352 | Average Loss: 1.4138 \n","Epoch:76 | Iter:   20 | Time: 00:13:03 | Train Loss: 0.9688 | Average Loss: 1.4115 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:77 | Iter:    0 | Time: 00:13:08 | Train Loss: 1.2095 | Average Loss: 1.4094 \n","Epoch:77 | Iter:   20 | Time: 00:13:13 | Train Loss: 1.0782 | Average Loss: 1.4072 \n","Accuracy: 0.630000 | Time: 00:00:01\n","Epoch:78 | Iter:    0 | Time: 00:13:19 | Train Loss: 1.0751 | Average Loss: 1.4054 \n","Epoch:78 | Iter:   20 | Time: 00:13:23 | Train Loss: 1.2324 | Average Loss: 1.4030 \n","Accuracy: 0.605000 | Time: 00:00:01\n","Epoch:79 | Iter:    0 | Time: 00:13:29 | Train Loss: 1.0136 | Average Loss: 1.4016 \n","Epoch:79 | Iter:   20 | Time: 00:13:34 | Train Loss: 1.3622 | Average Loss: 1.3998 \n","Accuracy: 0.605000 | Time: 00:00:01\n","Epoch:80 | Iter:    0 | Time: 00:13:39 | Train Loss: 1.0282 | Average Loss: 1.3982 \n","Epoch:80 | Iter:   20 | Time: 00:13:44 | Train Loss: 1.0371 | Average Loss: 1.3960 \n","Accuracy: 0.620000 | Time: 00:00:01\n","Epoch:81 | Iter:    0 | Time: 00:13:50 | Train Loss: 0.9629 | Average Loss: 1.3942 \n","Epoch:81 | Iter:   20 | Time: 00:13:54 | Train Loss: 1.1497 | Average Loss: 1.3922 \n","Accuracy: 0.585000 | Time: 00:00:01\n","Epoch:82 | Iter:    0 | Time: 00:14:00 | Train Loss: 1.2729 | Average Loss: 1.3909 \n","Accuracy: 0.625000 | Time: 00:00:01\n","Epoch:83 | Iter:    0 | Time: 00:14:10 | Train Loss: 0.8902 | Average Loss: 1.3877 \n","Epoch:83 | Iter:   20 | Time: 00:14:15 | Train Loss: 1.1178 | Average Loss: 1.3859 \n","Accuracy: 0.617500 | Time: 00:00:01\n","Epoch:84 | Iter:    0 | Time: 00:14:20 | Train Loss: 1.1503 | Average Loss: 1.3843 \n","Epoch:84 | Iter:   20 | Time: 00:14:25 | Train Loss: 1.2732 | Average Loss: 1.3823 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:85 | Iter:    0 | Time: 00:14:30 | Train Loss: 1.0638 | Average Loss: 1.3807 \n","Epoch:85 | Iter:   20 | Time: 00:14:35 | Train Loss: 1.0058 | Average Loss: 1.3786 \n","Accuracy: 0.635000 | Time: 00:00:01\n","Epoch:86 | Iter:    0 | Time: 00:14:40 | Train Loss: 1.4066 | Average Loss: 1.3771 \n","Epoch:86 | Iter:   20 | Time: 00:14:45 | Train Loss: 1.2567 | Average Loss: 1.3753 \n","Accuracy: 0.602500 | Time: 00:00:01\n","Epoch:87 | Iter:    0 | Time: 00:14:51 | Train Loss: 1.1408 | Average Loss: 1.3737 \n","Epoch:87 | Iter:   20 | Time: 00:14:55 | Train Loss: 1.2612 | Average Loss: 1.3720 \n","Accuracy: 0.610000 | Time: 00:00:01\n","Epoch:88 | Iter:    0 | Time: 00:15:01 | Train Loss: 1.0188 | Average Loss: 1.3703 \n","Epoch:88 | Iter:   20 | Time: 00:15:06 | Train Loss: 0.9188 | Average Loss: 1.3686 \n","Accuracy: 0.595000 | Time: 00:00:01\n","Epoch:89 | Iter:    0 | Time: 00:15:11 | Train Loss: 0.8135 | Average Loss: 1.3668 \n","Epoch:89 | Iter:   20 | Time: 00:15:16 | Train Loss: 1.1783 | Average Loss: 1.3651 \n","Accuracy: 0.637500 | Time: 00:00:01\n","Epoch:90 | Iter:    0 | Time: 00:15:21 | Train Loss: 0.9616 | Average Loss: 1.3633 \n","Epoch:90 | Iter:   20 | Time: 00:15:26 | Train Loss: 0.8053 | Average Loss: 1.3616 \n","Accuracy: 0.612500 | Time: 00:00:01\n","Epoch:91 | Iter:    0 | Time: 00:15:32 | Train Loss: 0.9685 | Average Loss: 1.3599 \n","Epoch:91 | Iter:   20 | Time: 00:15:37 | Train Loss: 1.0639 | Average Loss: 1.3584 \n","Accuracy: 0.612500 | Time: 00:00:01\n","Epoch:92 | Iter:    0 | Time: 00:15:42 | Train Loss: 0.9512 | Average Loss: 1.3569 \n","Epoch:92 | Iter:   20 | Time: 00:15:47 | Train Loss: 1.0492 | Average Loss: 1.3549 \n","Accuracy: 0.610000 | Time: 00:00:01\n","Epoch:93 | Iter:    0 | Time: 00:15:53 | Train Loss: 1.0686 | Average Loss: 1.3537 \n","Epoch:93 | Iter:   20 | Time: 00:15:58 | Train Loss: 0.9197 | Average Loss: 1.3521 \n","Accuracy: 0.627500 | Time: 00:00:01\n","Epoch:94 | Iter:    0 | Time: 00:16:03 | Train Loss: 1.2786 | Average Loss: 1.3507 \n","Epoch:94 | Iter:   20 | Time: 00:16:08 | Train Loss: 1.0567 | Average Loss: 1.3489 \n","Accuracy: 0.615000 | Time: 00:00:01\n","Epoch:95 | Iter:    0 | Time: 00:16:13 | Train Loss: 0.9864 | Average Loss: 1.3475 \n","Epoch:95 | Iter:   20 | Time: 00:16:18 | Train Loss: 0.9660 | Average Loss: 1.3459 \n","Accuracy: 0.632500 | Time: 00:00:01\n","Epoch:96 | Iter:    0 | Time: 00:16:23 | Train Loss: 0.9471 | Average Loss: 1.3444 \n","Epoch:96 | Iter:   20 | Time: 00:16:28 | Train Loss: 1.0749 | Average Loss: 1.3428 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:97 | Iter:    0 | Time: 00:16:34 | Train Loss: 0.8735 | Average Loss: 1.3413 \n","Epoch:97 | Iter:   20 | Time: 00:16:38 | Train Loss: 1.2997 | Average Loss: 1.3398 \n","Accuracy: 0.622500 | Time: 00:00:01\n","Epoch:98 | Iter:    0 | Time: 00:16:44 | Train Loss: 1.2443 | Average Loss: 1.3384 \n","Epoch:98 | Iter:   20 | Time: 00:16:49 | Train Loss: 1.1889 | Average Loss: 1.3367 \n","Accuracy: 0.645000 | Time: 00:00:01\n","Epoch:99 | Iter:    0 | Time: 00:16:54 | Train Loss: 0.9598 | Average Loss: 1.3352 \n","Epoch:99 | Iter:   20 | Time: 00:16:59 | Train Loss: 1.1343 | Average Loss: 1.3336 \n","Accuracy: 0.635000 | Time: 00:00:01\n","Epoch:100 | Iter:    0 | Time: 00:17:04 | Train Loss: 1.1067 | Average Loss: 1.3325 \n","Epoch:100 | Iter:   20 | Time: 00:17:09 | Train Loss: 1.0537 | Average Loss: 1.3312 \n","Accuracy: 0.617500 | Time: 00:00:01\n"]}],"source":["#--------------------------------------------------\n","#       Start Training & Evaluation\n","#--------------------------------------------------\n","net = AugmentationTechnique()\n","train_option = {}\n","train_option['lr'] = 0.002\n","train_option['epoch'] = 100\n","train_option['device'] = 'gpu'\n","trainModel(net, augmentedTrainData, train_option, augmentedTestData)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"nBHKIxzAYYM2"},"source":["Fine Tuning a Pre-Trained Deep Network\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VLG3WtEmYYM3","executionInfo":{"status":"ok","timestamp":1650857965359,"user_tz":240,"elapsed":47435,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"12f6bd77-d34d-4443-f8f2-e5fc06b2344b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading images from class: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]},{"output_type":"stream","name":"stdout","text":["Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 100 minibatches (batch_size=32) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Finish loading 12 minibatches (batch_size=32) of testing samples.\n"]}],"source":["# reload data with a larger size\n","img_size = (224, 224)\n","batch_size = 32 # training sample number per batch \n","\n","# load training dataset\n","trainloader_large = list(load_dataset('./data/train/', img_size, num_per_class=100, batch_size=batch_size, shuffle=True, \n","                                      augment=True, is_color=True, zero_centered=True))\n","train_num = len(trainloader_large)\n","print(\"Finish loading %d minibatches (batch_size=%d) of training samples.\" % (train_num, batch_size))\n","\n","# load testing dataset\n","testloader_large = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_size=batch_size, is_color=True, zero_centered=True))\n","test_num = len(testloader_large)\n","print(\"Finish loading %d minibatches (batch_size=%d) of testing samples.\" % (test_num, batch_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btOal_ampEnm","executionInfo":{"status":"ok","timestamp":1650855922733,"user_tz":240,"elapsed":220385,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"6f8183b8-4e24-4750-fde3-5e4893b09dae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:00 | Train Loss: 2.7856 | Average Loss: 2.7856 \n","Epoch: 1 | Iter:   20 | Time: 00:00:06 | Train Loss: 0.9991 | Average Loss: 1.4080 \n","Epoch: 1 | Iter:   40 | Time: 00:00:11 | Train Loss: 0.4687 | Average Loss: 1.0368 \n","Accuracy: 0.781250 | Time: 00:00:00\n","Epoch: 2 | Iter:    0 | Time: 00:00:14 | Train Loss: 0.4090 | Average Loss: 0.9404 \n","Epoch: 2 | Iter:   20 | Time: 00:00:20 | Train Loss: 0.4477 | Average Loss: 0.8102 \n","Epoch: 2 | Iter:   40 | Time: 00:00:26 | Train Loss: 0.4870 | Average Loss: 0.7185 \n","Accuracy: 0.789062 | Time: 00:00:00\n","Epoch: 3 | Iter:    0 | Time: 00:00:29 | Train Loss: 0.2621 | Average Loss: 0.6790 \n","Epoch: 3 | Iter:   20 | Time: 00:00:35 | Train Loss: 0.3824 | Average Loss: 0.6085 \n","Epoch: 3 | Iter:   40 | Time: 00:00:40 | Train Loss: 0.1313 | Average Loss: 0.5524 \n","Accuracy: 0.802083 | Time: 00:00:00\n","Epoch: 4 | Iter:    0 | Time: 00:00:43 | Train Loss: 0.0790 | Average Loss: 0.5298 \n","Epoch: 4 | Iter:   20 | Time: 00:00:49 | Train Loss: 0.1582 | Average Loss: 0.4865 \n","Epoch: 4 | Iter:   40 | Time: 00:00:55 | Train Loss: 0.2724 | Average Loss: 0.4476 \n","Accuracy: 0.781250 | Time: 00:00:00\n","Epoch: 5 | Iter:    0 | Time: 00:00:58 | Train Loss: 0.1040 | Average Loss: 0.4315 \n","Epoch: 5 | Iter:   20 | Time: 00:01:04 | Train Loss: 0.1727 | Average Loss: 0.4036 \n","Epoch: 5 | Iter:   40 | Time: 00:01:10 | Train Loss: 0.0312 | Average Loss: 0.3798 \n","Accuracy: 0.804688 | Time: 00:00:00\n","Epoch: 6 | Iter:    0 | Time: 00:01:13 | Train Loss: 0.0223 | Average Loss: 0.3678 \n","Epoch: 6 | Iter:   20 | Time: 00:01:19 | Train Loss: 0.0361 | Average Loss: 0.3471 \n","Epoch: 6 | Iter:   40 | Time: 00:01:24 | Train Loss: 0.0943 | Average Loss: 0.3288 \n","Accuracy: 0.812500 | Time: 00:00:00\n","Epoch: 7 | Iter:    0 | Time: 00:01:28 | Train Loss: 0.0344 | Average Loss: 0.3197 \n","Epoch: 7 | Iter:   20 | Time: 00:01:33 | Train Loss: 0.1207 | Average Loss: 0.3035 \n","Epoch: 7 | Iter:   40 | Time: 00:01:39 | Train Loss: 0.0239 | Average Loss: 0.2881 \n","Accuracy: 0.794271 | Time: 00:00:00\n","Epoch: 8 | Iter:    0 | Time: 00:01:42 | Train Loss: 0.0925 | Average Loss: 0.2817 \n","Epoch: 8 | Iter:   20 | Time: 00:01:48 | Train Loss: 0.0231 | Average Loss: 0.2691 \n","Epoch: 8 | Iter:   40 | Time: 00:01:53 | Train Loss: 0.0048 | Average Loss: 0.2566 \n","Accuracy: 0.820312 | Time: 00:00:00\n","Epoch: 9 | Iter:    0 | Time: 00:01:57 | Train Loss: 0.0124 | Average Loss: 0.2512 \n","Epoch: 9 | Iter:   20 | Time: 00:02:02 | Train Loss: 0.0580 | Average Loss: 0.2407 \n","Epoch: 9 | Iter:   40 | Time: 00:02:08 | Train Loss: 0.0270 | Average Loss: 0.2306 \n","Accuracy: 0.820312 | Time: 00:00:00\n","Epoch:10 | Iter:    0 | Time: 00:02:11 | Train Loss: 0.0112 | Average Loss: 0.2258 \n","Epoch:10 | Iter:   20 | Time: 00:02:17 | Train Loss: 0.0213 | Average Loss: 0.2172 \n","Epoch:10 | Iter:   40 | Time: 00:02:23 | Train Loss: 0.0048 | Average Loss: 0.2088 \n","Accuracy: 0.804688 | Time: 00:00:00\n","Epoch:11 | Iter:    0 | Time: 00:02:26 | Train Loss: 0.0170 | Average Loss: 0.2050 \n","Epoch:11 | Iter:   20 | Time: 00:02:32 | Train Loss: 0.0268 | Average Loss: 0.1977 \n","Epoch:11 | Iter:   40 | Time: 00:02:37 | Train Loss: 0.0339 | Average Loss: 0.1912 \n","Accuracy: 0.809896 | Time: 00:00:00\n","Epoch:12 | Iter:    0 | Time: 00:02:41 | Train Loss: 0.0200 | Average Loss: 0.1885 \n","Epoch:12 | Iter:   20 | Time: 00:02:46 | Train Loss: 0.0359 | Average Loss: 0.1829 \n","Epoch:12 | Iter:   40 | Time: 00:02:52 | Train Loss: 0.0096 | Average Loss: 0.1779 \n","Accuracy: 0.804688 | Time: 00:00:00\n","Epoch:13 | Iter:    0 | Time: 00:02:55 | Train Loss: 0.0687 | Average Loss: 0.1752 \n","Epoch:13 | Iter:   20 | Time: 00:03:01 | Train Loss: 0.0547 | Average Loss: 0.1702 \n","Epoch:13 | Iter:   40 | Time: 00:03:07 | Train Loss: 0.0019 | Average Loss: 0.1660 \n","Accuracy: 0.778646 | Time: 00:00:00\n","Epoch:14 | Iter:    0 | Time: 00:03:10 | Train Loss: 0.0990 | Average Loss: 0.1645 \n","Epoch:14 | Iter:   20 | Time: 00:03:15 | Train Loss: 0.0445 | Average Loss: 0.1604 \n","Epoch:14 | Iter:   40 | Time: 00:03:21 | Train Loss: 0.0352 | Average Loss: 0.1575 \n","Accuracy: 0.789062 | Time: 00:00:00\n","Epoch:15 | Iter:    0 | Time: 00:03:24 | Train Loss: 0.0238 | Average Loss: 0.1560 \n","Epoch:15 | Iter:   20 | Time: 00:03:30 | Train Loss: 0.0037 | Average Loss: 0.1526 \n","Epoch:15 | Iter:   40 | Time: 00:03:36 | Train Loss: 0.0018 | Average Loss: 0.1489 \n","Accuracy: 0.809896 | Time: 00:00:00\n"]},{"output_type":"execute_result","data":{"text/plain":["702"]},"metadata":{},"execution_count":40}],"source":["#--------------------------------------------------\n","#       Fine-Tune Pretrained Network\n","#--------------------------------------------------\n","finetune_alexnet = models.alexnet(pretrained=True)\n","finetune_alexnet.classifier[6] = nn.Linear(4096,16)\n","\n","train_option = {}\n","train_option['lr'] = 0.00015\n","train_option['epoch'] = 15\n","train_option['device'] = 'gpu'\n","trainModel(finetune_alexnet, trainloader_large, train_option, testloader_large)\n","gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9pmTLWH7KIs"},"outputs":[],"source":["#--------------------------------------------------\n","#       Get Features from AlexNet\n","#--------------------------------------------------\n","def getFeatures(model, features_count, dataset):\n","\n","  if torch.cuda.is_available():\n","    model = model.cuda()\n","        \n","  features = []\n","  labels = []\n","    \n","  for img, label in dataset:\n","    if torch.cuda.is_available():\n","      img  = img.float().cuda()\n","      label = label.long().cuda()\n","        \n","    img_out = model(img)\n","    \n","    for index2, img_it in enumerate(img_out):\n","      features.append(np.array(img_it[:features_count].cpu().detach().numpy()))\n","      labels.append(label[index2].cpu().detach().numpy())\n","            \n","  return np.array(features), np.array(labels)\n","    \n","\n","train_feat, train_label = getFeatures(finetune_alexnet, 400, trainloader_large)\n","test_feat, test_label = getFeatures(finetune_alexnet, 400, testloader_large)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghPvfDAUT-Xs","executionInfo":{"status":"ok","timestamp":1650855988351,"user_tz":240,"elapsed":1261,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"d400b89d-b745-48c0-aa3b-2524744c6a75"},"outputs":[{"output_type":"stream","name":"stdout","text":["The accuracy is 82.81%\n"]}],"source":["#--------------------------------------------------\n","#       Train and Evaluate SVM\n","#--------------------------------------------------\n","from sklearn import svm\n","from sklearn import preprocessing\n","\n","#Calculating accuracy using techniques used in assignment 3\n","\n","def train_SVM(X, Y):\n","  return svm.SVC(C=0.95).fit(X, Y)\n","\n","def predict_SVM(clf, X):\n","  return clf.predict(X)\n","\n","\n","clf = train_SVM(train_feat, train_label)\n","test_label_pred = predict_SVM(clf, test_feat)\n","\n","accuracy = sum(np.array(test_label_pred) == test_label) / float(len(test_label))\n","print(\"The accuracy is {:.2f}%\".format(accuracy*100))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wC0I4MWBk7pr","executionInfo":{"status":"ok","timestamp":1650858902840,"user_tz":240,"elapsed":927802,"user":{"displayName":"Yuhao Wang","userId":"16030694374399777638"}},"outputId":"e701839c-1ff9-4c09-da43-effda4dec960"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Iter:    0 | Time: 00:00:11 | Train Loss: 2.8156 | Average Loss: 2.8156 \n","Epoch: 1 | Iter:   20 | Time: 00:00:28 | Train Loss: 0.9621 | Average Loss: 1.3177 \n","Epoch: 1 | Iter:   40 | Time: 00:00:45 | Train Loss: 0.5792 | Average Loss: 0.9688 \n","Epoch: 1 | Iter:   60 | Time: 00:01:03 | Train Loss: 0.3483 | Average Loss: 0.8134 \n","Epoch: 1 | Iter:   80 | Time: 00:01:21 | Train Loss: 0.2174 | Average Loss: 0.7084 \n","Accuracy: 0.838542 | Time: 00:00:02\n","Epoch: 2 | Iter:    0 | Time: 00:01:41 | Train Loss: 0.2519 | Average Loss: 0.6327 \n","Epoch: 2 | Iter:   20 | Time: 00:01:59 | Train Loss: 0.1438 | Average Loss: 0.5548 \n","Epoch: 2 | Iter:   40 | Time: 00:02:17 | Train Loss: 0.1669 | Average Loss: 0.4901 \n","Epoch: 2 | Iter:   60 | Time: 00:02:34 | Train Loss: 0.0772 | Average Loss: 0.4412 \n","Epoch: 2 | Iter:   80 | Time: 00:02:52 | Train Loss: 0.0123 | Average Loss: 0.4007 \n","Accuracy: 0.898438 | Time: 00:00:02\n","Epoch: 3 | Iter:    0 | Time: 00:03:13 | Train Loss: 0.0277 | Average Loss: 0.3650 \n","Epoch: 3 | Iter:   20 | Time: 00:03:31 | Train Loss: 0.0190 | Average Loss: 0.3359 \n","Epoch: 3 | Iter:   40 | Time: 00:03:48 | Train Loss: 0.0353 | Average Loss: 0.3102 \n","Epoch: 3 | Iter:   60 | Time: 00:04:06 | Train Loss: 0.0169 | Average Loss: 0.2886 \n","Epoch: 3 | Iter:   80 | Time: 00:04:24 | Train Loss: 0.0056 | Average Loss: 0.2702 \n","Accuracy: 0.911458 | Time: 00:00:02\n","Epoch: 4 | Iter:    0 | Time: 00:04:45 | Train Loss: 0.0177 | Average Loss: 0.2529 \n","Epoch: 4 | Iter:   20 | Time: 00:05:02 | Train Loss: 0.0495 | Average Loss: 0.2381 \n","Epoch: 4 | Iter:   40 | Time: 00:05:20 | Train Loss: 0.0068 | Average Loss: 0.2250 \n","Epoch: 4 | Iter:   60 | Time: 00:05:38 | Train Loss: 0.0066 | Average Loss: 0.2131 \n","Epoch: 4 | Iter:   80 | Time: 00:05:56 | Train Loss: 0.0320 | Average Loss: 0.2036 \n","Accuracy: 0.919271 | Time: 00:00:02\n","Epoch: 5 | Iter:    0 | Time: 00:06:16 | Train Loss: 0.0082 | Average Loss: 0.1938 \n","Epoch: 5 | Iter:   20 | Time: 00:06:34 | Train Loss: 0.0024 | Average Loss: 0.1850 \n","Epoch: 5 | Iter:   40 | Time: 00:06:52 | Train Loss: 0.0040 | Average Loss: 0.1768 \n","Epoch: 5 | Iter:   60 | Time: 00:07:10 | Train Loss: 0.0047 | Average Loss: 0.1693 \n","Epoch: 5 | Iter:   80 | Time: 00:07:27 | Train Loss: 0.0268 | Average Loss: 0.1663 \n","Accuracy: 0.692708 | Time: 00:00:02\n","Epoch: 6 | Iter:    0 | Time: 00:07:48 | Train Loss: 0.1051 | Average Loss: 0.1671 \n","Epoch: 6 | Iter:   20 | Time: 00:08:06 | Train Loss: 0.1042 | Average Loss: 0.1650 \n","Epoch: 6 | Iter:   40 | Time: 00:08:24 | Train Loss: 0.0604 | Average Loss: 0.1613 \n","Epoch: 6 | Iter:   60 | Time: 00:08:41 | Train Loss: 0.0228 | Average Loss: 0.1571 \n","Epoch: 6 | Iter:   80 | Time: 00:08:59 | Train Loss: 0.0063 | Average Loss: 0.1530 \n","Accuracy: 0.875000 | Time: 00:00:02\n","Epoch: 7 | Iter:    0 | Time: 00:09:20 | Train Loss: 0.0167 | Average Loss: 0.1491 \n","Epoch: 7 | Iter:   20 | Time: 00:09:38 | Train Loss: 0.0049 | Average Loss: 0.1447 \n","Epoch: 7 | Iter:   40 | Time: 00:09:55 | Train Loss: 0.0045 | Average Loss: 0.1403 \n","Epoch: 7 | Iter:   60 | Time: 00:10:13 | Train Loss: 0.0049 | Average Loss: 0.1363 \n","Epoch: 7 | Iter:   80 | Time: 00:10:31 | Train Loss: 0.0013 | Average Loss: 0.1325 \n","Accuracy: 0.911458 | Time: 00:00:02\n","Epoch: 8 | Iter:    0 | Time: 00:10:51 | Train Loss: 0.0012 | Average Loss: 0.1288 \n","Epoch: 8 | Iter:   20 | Time: 00:11:09 | Train Loss: 0.0009 | Average Loss: 0.1253 \n","Epoch: 8 | Iter:   40 | Time: 00:11:27 | Train Loss: 0.0021 | Average Loss: 0.1219 \n","Epoch: 8 | Iter:   60 | Time: 00:11:45 | Train Loss: 0.0019 | Average Loss: 0.1188 \n","Epoch: 8 | Iter:   80 | Time: 00:12:03 | Train Loss: 0.0006 | Average Loss: 0.1158 \n","Accuracy: 0.911458 | Time: 00:00:02\n","Epoch: 9 | Iter:    0 | Time: 00:12:23 | Train Loss: 0.0007 | Average Loss: 0.1129 \n","Epoch: 9 | Iter:   20 | Time: 00:12:41 | Train Loss: 0.0005 | Average Loss: 0.1102 \n","Epoch: 9 | Iter:   40 | Time: 00:12:59 | Train Loss: 0.0011 | Average Loss: 0.1076 \n","Epoch: 9 | Iter:   60 | Time: 00:13:16 | Train Loss: 0.0010 | Average Loss: 0.1051 \n","Epoch: 9 | Iter:   80 | Time: 00:13:34 | Train Loss: 0.0003 | Average Loss: 0.1027 \n","Accuracy: 0.911458 | Time: 00:00:02\n","Epoch:10 | Iter:    0 | Time: 00:13:55 | Train Loss: 0.0004 | Average Loss: 0.1004 \n","Epoch:10 | Iter:   40 | Time: 00:14:30 | Train Loss: 0.0007 | Average Loss: 0.0962 \n","Epoch:10 | Iter:   60 | Time: 00:14:48 | Train Loss: 0.0007 | Average Loss: 0.0942 \n","Epoch:10 | Iter:   80 | Time: 00:15:06 | Train Loss: 0.0002 | Average Loss: 0.0923 \n","Accuracy: 0.914062 | Time: 00:00:02\n"]}],"source":["#--------------------------------------------------\n","#       Fine-Tune Pretrained Network\n","#--------------------------------------------------\n","#Fine Tune resnet 50\n","import torch.optim as optim\n","import time\n","resnet_model = models.resnet50(pretrained=True)\n","resnet_model.fc = nn.Linear(2048, 16)\n","\n","train_option = {'optimizer':optim.RMSprop(resnet_model.parameters(), lr=0.0001)}\n","train_option['lr'] = 0.0001\n","train_option['epoch'] = 10\n","train_option['device'] = 'gpu'\n","trainModel(resnet_model, trainloader_large, train_option, testloader_large)\n"]},{"cell_type":"markdown","metadata":{"id":"AYJOi8QYYYNG"},"source":["<!--Write your report here in markdown or html-->\n"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"1wWuNKtuA6cbnsBeVwEnBpWT1gu8aSKI7","timestamp":1650858961264}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}